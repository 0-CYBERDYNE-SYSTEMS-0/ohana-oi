This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-25T22:29:07.996Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
code-execution/
  computer-api.mdx
  custom-languages.mdx
  settings.mdx
  usage.mdx
computer/
  introduction.mdx
  language-model-usage.mdx
  user-usage.mdx
getting-started/
  introduction.mdx
  setup.mdx
guides/
  advanced-terminal-usage.mdx
  basic-usage.mdx
  demos.mdx
  multiple-instances.mdx
  os-mode.mdx
  profiles.mdx
  running-locally.mdx
  streaming-response.mdx
integrations/
  docker.mdx
  e2b.mdx
language-models/
  hosted-models/
    ai21.mdx
    anthropic.mdx
    anyscale.mdx
    aws-sagemaker.mdx
    azure.mdx
    baseten.mdx
    cloudflare.mdx
    cohere.mdx
    deepinfra.mdx
    gpt-4-setup.mdx
    huggingface.mdx
    mistral-api.mdx
    nlp-cloud.mdx
    openai.mdx
    openrouter.mdx
    palm.mdx
    perplexity.mdx
    petals.mdx
    replicate.mdx
    togetherai.mdx
    vertex-ai.mdx
    vllm.mdx
  local-models/
    best-practices.mdx
    custom-endpoint.mdx
    janai.mdx
    llamafile.mdx
    lm-studio.mdx
    ollama.mdx
  custom-models.mdx
  introduction.mdx
  settings.mdx
legal/
  license.mdx
protocols/
  lmc-messages.mdx
safety/
  best-practices.mdx
  introduction.mdx
  isolation.mdx
  safe-mode.mdx
server/
  usage.mdx
settings/
  all-settings.mdx
  example-profiles.mdx
  profiles.mdx
telemetry/
  telemetry.mdx
troubleshooting/
  faq.mdx
usage/
  desktop/
    help.md
    install.mdx
  python/
    arguments.mdx
    budget-manager.mdx
    conversation-history.mdx
    magic-commands.mdx
    multiple-instances.mdx
    settings.mdx
  terminal/
    arguments.mdx
    budget-manager.mdx
    magic-commands.mdx
    settings.mdx
    vision.mdx
  examples.mdx
CONTRIBUTING.md
mint.json
NCU_MIGRATION_GUIDE.md
README_DE.md
README_ES.md
README_IN.md
README_JA.md
README_UK.md
README_VN.md
README_ZH.md
ROADMAP.md
SAFE_MODE.md
SECURITY.md
style.css

================================================================
Repository Files
================================================================

================
File: code-execution/computer-api.mdx
================
---
title: Computer API
---

The following functions are designed for language models to use in Open Interpreter, currently only supported in [OS Mode](/guides/os-mode/).

### Display - View

Takes a screenshot of the primary display.



```python
interpreter.computer.display.view()
```



### Display - Center

Gets the x, y value of the center of the screen.



```python
x, y = interpreter.computer.display.center()
```



### Keyboard - Hotkey

Performs a hotkey on the computer



```python
interpreter.computer.keyboard.hotkey(" ", "command")
```



### Keyboard - Write

Writes the text into the currently focused window.



```python
interpreter.computer.keyboard.write("hello")
```



### Mouse - Click

Clicks on the specified coordinates, or an icon, or text. If text is specified, OCR will be run on the screenshot to find the text coordinates and click on it.



```python
# Click on coordinates
interpreter.computer.mouse.click(x=100, y=100)

# Click on text on the screen
interpreter.computer.mouse.click("Onscreen Text")

# Click on a gear icon
interpreter.computer.mouse.click(icon="gear icon")
```



### Mouse - Move

Moves to the specified coordinates, or an icon, or text. If text is specified, OCR will be run on the screenshot to find the text coordinates and move to it.



```python
# Click on coordinates
interpreter.computer.mouse.move(x=100, y=100)

# Click on text on the screen
interpreter.computer.mouse.move("Onscreen Text")

# Click on a gear icon
interpreter.computer.mouse.move(icon="gear icon")
```



### Mouse - Scroll

Scrolls the mouse a specified number of pixels.



```python
# Scroll Down
interpreter.computer.mouse.scroll(-10)

# Scroll Up
interpreter.computer.mouse.scroll(10)
```



### Clipboard - View

Returns the contents of the clipboard.



```python
interpreter.computer.clipboard.view()
```



### OS - Get Selected Text

Get the selected text on the screen.



```python
interpreter.computer.os.get_selected_text()
```



### Mail - Get

Retrieves the last `number` emails from the inbox, optionally filtering for only unread emails. (Mac only)



```python
interpreter.computer.mail.get(number=10, unread=True)
```



### Mail - Send

Sends an email with the given parameters using the default mail app. (Mac only)



```python
interpreter.computer.mail.send("john@email.com", "Subject", "Body", ["path/to/attachment.pdf", "path/to/attachment2.pdf"])
```



### Mail - Unread Count

Retrieves the count of unread emails in the inbox. (Mac only)



```python
interpreter.computer.mail.unread_count()
```



### SMS - Send

Send a text message using the default SMS app. (Mac only)



```python
interpreter.computer.sms.send("2068675309", "Hello from Open Interpreter!")
```



### Contacts - Get Phone Number

Returns the phone number of a contact name. (Mac only)



```python
interpreter.computer.contacts.get_phone_number("John Doe")
```



### Contacts - Get Email Address

Returns the email of a contact name. (Mac only)



```python
interpreter.computer.contacts.get_phone_number("John Doe")
```



### Calendar - Get Events

Fetches calendar events for the given date or date range from all calendars. (Mac only)



```python
interpreter.computer.calendar.get_events(start_date=datetime, end_date=datetime)
```



### Calendar - Create Event

Creates a new calendar event. Uses first calendar if none is specified (Mac only)



```python
interpreter.computer.calendar.create_event(title="Title", start_date=datetime, end_date=datetime, location="Location", notes="Notes", calendar="Work")
```



### Calendar - Delete Event

Delete a specific calendar event. (Mac only)



```python
interpreter.computer.calendar.delete_event(event_title="Title", start_date=datetime, calendar="Work")
```

================
File: code-execution/custom-languages.mdx
================
---
title: Custom Languages
---

You can add or edit the programming languages that Open Interpreter's computer runs.

In this example, we'll swap out the `python` language for a version of `python` that runs in the cloud. We'll use `E2B` to do this.

([`E2B`](https://e2b.dev/) is a secure, sandboxed environment where you can run arbitrary code.)

First, [get an API key here](https://e2b.dev/), and set it:

```python
import os
os.environ["E2B_API_KEY"] = "<your_api_key_here>"
```

Then, define a custom language for Open Interpreter. The class name doesn't matter, but we'll call it `PythonE2B`:

```python
import e2b

class PythonE2B:
    """
    This class contains all requirements for being a custom language in Open Interpreter:

    - name (an attribute)
    - run (a method)
    - stop (a method)
    - terminate (a method)

    You can use this class to run any language you know how to run, or edit any of the official languages (which also conform to this class).

    Here, we'll use E2B to power the `run` method.
    """

    # This is the name that will appear to the LLM.
    name = "python"

    # Optionally, you can append some information about this language to the system message:
    system_message = "# Follow this rule: Every Python code block MUST contain at least one print statement."

    # (E2B isn't a Jupyter Notebook, so we added ^ this so it would print things,
    # instead of putting variables at the end of code blocks, which is a Jupyter thing.)

    def run(self, code):
        """Generator that yields a dictionary in LMC Format."""

        # Run the code on E2B
        stdout, stderr = e2b.run_code('Python3', code)

        # Yield the output
        yield {
            "type": "console", "format": "output",
            "content": stdout + stderr # We combined these arbitrarily. Yield anything you'd like!
        }

    def stop(self):
        """Stops the code."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

    def terminate(self):
        """Terminates the entire process."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

# (Tip: Do this before adding/removing languages, otherwise OI might retain the state of previous languages:)
interpreter.computer.terminate()

# Give Open Interpreter its languages. This will only let it run PythonE2B:
interpreter.computer.languages = [PythonE2B]

# Try it out!
interpreter.chat("What's 349808*38490739?")
```

================
File: code-execution/settings.mdx
================
---
title: Settings
---

The `interpreter.computer` is responsible for executing code.

[Click here](https://docs.openinterpreter.com/settings/all-settings#computer) to view `interpreter.computer` settings.

================
File: code-execution/usage.mdx
================
---
title: Usage
---

# Running Code

The `computer` itself is separate from Open Interpreter's core, so you can run it independently:

```python
from interpreter import interpreter

interpreter.computer.run("python", "print('Hello World!')")
```

This runs in the same Python instance that interpreter uses, so you can define functions, variables, or log in to services before the AI starts running code:

```python
interpreter.computer.run("python", "import replicate\nreplicate.api_key='...'")

interpreter.custom_instructions = "Replicate has already been imported."

interpreter.chat("Please generate an image on replicate...") # Interpreter will be logged into Replicate
```

# Custom Languages

You also have control over the `computer`'s languages (like Python, Javascript, and Shell), and can easily append custom languages:

<Card
  title="Custom Languages"
  icon="code"
  iconType="solid"
  href="/code-execution/custom-languages/"
>
  Add or customize the programming languages that Open Interpreter can use.
</Card>

================
File: computer/introduction.mdx
================
The Computer module is responsible for executing code.

You can manually execute code in the same instance that Open Interpreter uses:

```

```

User Usage

It also comes with a suite of modules that we think are particularly useful to code interpreting LLMs.

LLM Usage

================
File: computer/language-model-usage.mdx
================
Open Interpreter can use the Computer module itself.

Here's what it can do:

================
File: computer/user-usage.mdx
================
The Computer module is responsible for running code.

You can add custom languages to it.

The user can add custom languages to the Computer, and .run code on it.

================
File: getting-started/introduction.mdx
================
---
title: Introduction
description: A new way to use computers
---

# <div class="hidden">Introduction</div>

<img src="https://openinterpreter.com/assets/banner.jpg" alt="thumbnail" style={{transform: "translateY(-1.25rem)"}} />

**Open Interpreter** lets language models run code.

You can chat with Open Interpreter through a ChatGPT-like interface in your terminal by running `interpreter` after installing.

This provides a natural-language interface to your computer's general-purpose capabilities:

-   Create and edit photos, videos, PDFs, etc.
-   Control a Chrome browser to perform research
-   Plot, clean, and analyze large datasets
-   ...etc.

<br/>

<Info>You can also build Open Interpreter into your applications with [our Python package.](/usage/python/arguments)</Info>

---

<h1><span class="font-semibold">Quick start</span></h1>

If you already use Python, you can install Open Interpreter via `pip`:

<Steps>
  <Step title="Install" icon={"arrow-down"} iconType={"solid"}>
```bash
pip install open-interpreter
```
  </Step>
  <Step title="Use" icon={"circle"} iconType={"solid"}>
```bash
interpreter
```
  </Step>
</Steps>

We've also developed [one-line installers](/getting-started/setup#experimental-one-line-installers) that install Python and set up Open Interpreter.

================
File: getting-started/setup.mdx
================
---
title: Setup
---

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/5sk3t8ilDR8"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

## Installation from `pip`

If you are familiar with Python, we recommend installing Open Interpreter via `pip`

```bash
pip install open-interpreter
```

<Info>
  You'll need Python
  [3.10](https://www.python.org/downloads/release/python-3100/) or
  [3.11](https://www.python.org/downloads/release/python-3110/). Run `python
  --version` to check yours.

It is recommended to install Open Interpreter in a [virtual
environment](https://docs.python.org/3/library/venv.html).

</Info>

## Install optional dependencies from `pip`

Open Interpreter has optional dependencies for different capabilities

[Local Mode](/guides/running-locally) dependencies

```bash
pip install open-interpreter[local]
```

[OS Mode](/guides/os-mode) dependencies

```bash
pip install open-interpreter[os]
```

[Safe Mode](/safety/safe-mode) dependencies

```bash
pip install open-interpreter[safe]
```

Server dependencies

```bash
pip install open-interpreter[server]
```

## Experimental one-line installers

To try our experimental installers, open your Terminal with admin privileges [(click here to learn how)](https://chat.openai.com/share/66672c0f-0935-4c16-ac96-75c1afe14fe3), then paste the following commands:

<CodeGroup>

```bash Mac
curl -sL https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-mac-installer.sh | bash
```

```powershell Windows
iex "& {$(irm https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-windows-installer-conda.ps1)}"
```

```bash Linux
curl -sL https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-linux-installer.sh | bash
```

</CodeGroup>

These installers will attempt to download Python, set up an environment, and install Open Interpreter for you.

## No Installation

If configuring your computer environment is challenging, you can press the `,` key on the [GitHub page](https://github.com/OpenInterpreter/open-interpreter) to create a codespace. After a moment, you'll receive a cloud virtual machine environment pre-installed with open-interpreter. You can then start interacting with it directly and freely confirm its execution of system commands without worrying about damaging the system.

================
File: guides/advanced-terminal-usage.mdx
================
---
title: Advanced Terminal Usage
---

Magic commands can be used to control the interpreter's behavior in interactive mode:

- `%% [shell commands, like ls or cd]`: Run commands in Open Interpreter's shell instance
- `%verbose [true/false]`: Toggle verbose mode. Without arguments or with 'true', it enters verbose mode. With 'false', it exits verbose mode.
- `%reset`: Reset the current session.
- `%undo`: Remove previous messages and its response from the message history.
- `%save_message [path]`: Saves messages to a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%load_message [path]`: Loads messages from a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%tokens [prompt]`: EXPERIMENTAL: Calculate the tokens used by the next request based on the current conversation's messages and estimate the cost of that request; optionally provide a prompt to also calculate the tokens used by that prompt and the total amount of tokens that will be sent with the next request.
- `%info`: Show system and interpreter information.
- `%help`: Show this help message.
- `%jupyter`: Export the current session to a Jupyter notebook file (.ipynb) to the Downloads folder.
- `%markdown [path]`: Export the conversation to a specified Markdown path. If no path is provided, it will be saved to the Downloads folder with a generated conversation name.

================
File: guides/basic-usage.mdx
================
---
title: Basic Usage
---

<CardGroup>

<Card
  title="Interactive demo"
  icon="gamepad-modern"
  iconType="solid"
  href="https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing"
>
  Try Open Interpreter without installing anything on your computer
</Card>

<Card
  title="Example voice interface"
  icon="circle"
  iconType="solid"
  href="https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK"
>
  An example implementation of Open Interpreter's streaming capabilities
</Card>

</CardGroup>

---

### Interactive Chat

To start an interactive chat in your terminal, either run `interpreter` from the command line or `interpreter.chat()` from a .py file.

<CodeGroup>

```shell Terminal
interpreter
```

```python Python
interpreter.chat()
```

</CodeGroup>

---

### Programmatic Chat

For more precise control, you can pass messages directly to `.chat(message)` in Python:

```python
interpreter.chat("Add subtitles to all videos in /videos.")

# ... Displays output in your terminal, completes task ...

interpreter.chat("These look great but can you make the subtitles bigger?")

# ...
```

---

### Start a New Chat

In your terminal, Open Interpreter behaves like ChatGPT and will not remember previous conversations. Simply run `interpreter` to start a new chat.

In Python, Open Interpreter remembers conversation history. If you want to start fresh, you can reset it.

<CodeGroup>

```shell Terminal
interpreter
```

```python Python
interpreter.messages = []
```

</CodeGroup>

---

### Save and Restore Chats

In your terminal, Open Interpreter will save previous conversations to `<your application directory>/Open Interpreter/conversations/`.

You can resume any of them by running `--conversations`. Use your arrow keys to select one , then press `ENTER` to resume it.

In Python, `interpreter.chat()` returns a List of messages, which can be used to resume a conversation with `interpreter.messages = messages`.

<CodeGroup>

```shell Terminal
interpreter --conversations
```

```python Python
# Save messages to 'messages'
messages = interpreter.chat("My name is Killian.")

# Reset interpreter ("Killian" will be forgotten)
interpreter.messages = []

# Resume chat from 'messages' ("Killian" will be remembered)
interpreter.messages = messages
```

</CodeGroup>

---

### Configure Default Settings

We save default settings to the `default.yaml` profile which can be opened and edited by running the following command:

```shell
interpreter --profiles
```

You can use this to set your default language model, system message (custom instructions), max budget, etc.

<Info>
  **Note:** The Python library will also inherit settings from the default
  profile file. You can change it by running `interpreter --profiles` and
  editing `default.yaml`.
</Info>

---

### Customize System Message

In your terminal, modify the system message by [editing your configuration file as described here](#configure-default-settings).

In Python, you can inspect and configure Open Interpreter's system message to extend its functionality, modify permissions, or give it more context.

```python
interpreter.system_message += """
Run shell commands with -y so the user doesn't have to confirm them.
"""
print(interpreter.system_message)
```

---

### Change your Language Model

Open Interpreter uses [LiteLLM](https://docs.litellm.ai/docs/providers/) to connect to language models.

You can change the model by setting the model parameter:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

In Python, set the model on the object:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Find the appropriate "model" string for your language model here.](https://docs.litellm.ai/docs/providers/)

================
File: guides/demos.mdx
================
---
title: Demos
---

### Vision Mode

#### Recreating a Tailwind Component

Creating a dropdown menu in Tailwind from a single screenshot:

<iframe src="data:text/html;charset=utf-8,%0A%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3Ewe%26%2339%3Bve%20literally%20been%20flying%20blind%20until%20now%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%24%20interpreter%20--vision%3Cbr%3E%0A%20%20%20%20%26gt%3B%20Recreate%20this%20component%20in%20Tailwind%20CSS%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%28this%20is%20realtime%29%20%3Ca%20href%3D%22https%3A//t.co/PyVm11mclF%22%3Epic.twitter.com/PyVm11mclF%3C/a%3E%0A%20%20%20%20%3C/p%3E%26mdash%3B%20killian%20%28%40hellokillian%29%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/hellokillian/status/1723106008061587651%3Fref_src%3Dtwsrc%255Etfw%22%3ENovember%2010%2C%202023%3C/a%3E%0A%3C/blockquote%3E%20%0A%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A" width="100%" height="500"></iframe>

#### Recreating the ChatGPT interface using GPT-4V:

<iframe src="data:text/html;charset=utf-8,%0A%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EOpen%20Interpreter%20%2B%20Vision%20-%20with%20the%20self-improving%20feedback%20loop%20is%20%F0%9F%91%8C%20%3Cbr%3E%3Cbr%3E%0A%20%20%20%20Here%20is%20how%20it%20iterates%20to%20recreate%20the%20ChatGPT%20UI%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%284x%20speedup%29%20%3Ca%20href%3D%22https%3A//t.co/HphKMOWBiB%22%3Epic.twitter.com/HphKMOWBiB%3C/a%3E%0A%20%20%20%20%3C/p%3E%26mdash%3B%20chilang%20%28%40chilang%29%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/chilang/status/1724577200135897255%3Fref_src%3Dtwsrc%255Etfw%22%3ENovember%2014%2C%202023%3C/a%3E%0A%3C/blockquote%3E%20%0A%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A" width="100%" height="500"></iframe>

### OS Mode

#### Playing Music

Open Interpreter playing some Lofi using OS mode:

<iframe width="560" height="315" src="https://www.youtube.com/embed/-n8qYi5HhO8?si=huEpYFBEwotBIMMs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

#### Open Interpreter Chatting with Open Interpreter

OS mode creating and chatting with a local instance of Open Interpreter:

<iframe src="data:text/html;charset=utf-8,%0A%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EComputer-operating%20AI%20can%20replicate%20itself%20onto%20other%20systems.%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20Open%20Interpreter%20uses%20my%20mouse%20and%20keyboard%20to%20start%20a%20local%20instance%20of%20itself%3A%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/1BZWRA4FMn%22%3Epic.twitter.com/1BZWRA4FMn%3C/a%3E%3C/p%3E%26mdash%3B%20Ty%20%28%40FieroTy%29%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/FieroTy/status/1746639975234560101%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%2014%2C%202024%3C/a%3E%0A%3C/blockquote%3E%20%0A%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A" width="100%" height="500"></iframe>

#### Controlling an Arduino

Reading temperature and humidity from an Arudino:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EThis%20time%20I%20showed%20it%20an%20image%20of%20a%20temp%20sensor%2C%20LCD%20%26amp%3B%20Arduino.%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20And%20it%20wrote%20a%20program%20to%20read%20the%20temperature%20%26amp%3B%20humidity%20from%20the%20sensor%20%26amp%3B%20show%20it%20on%20the%20LCD%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Still%20blown%20away%20by%20how%20good%20%40hellokillian%27s%20Open%20Interpreter%20is%21%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20p.s.%20-%20ignore%20the%20cat%20fight%20in%20the%20background%20%3Ca%20href%3D%22https%3A//t.co/tG9sSdkfD5%22%3Ehttps%3A//t.co/tG9sSdkfD5%3C/a%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/B6sH4absff%22%3Epic.twitter.com/B6sH4absff%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Vindiw%20Wijesooriya%20%28%40vindiww%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/vindiww/status/1744252926321942552%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%208%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Music Creation

OS mode using Logic Pro X to record a piano song and play it back:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3Eit%27s%20not%20quite%20Mozart%2C%20but...%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this%20is%20Open%20Interpreter%20firing%20up%20Logic%20Pro%20to%20write/record%20a%20song%21%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/vPHpPvjk4b%22%3Epic.twitter.com/vPHpPvjk4b%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Ty%20%28%40FieroTy%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/FieroTy/status/1744203268451111035%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%208%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Generating images in Everart.ai

Open Interpreter describing pictures it wants to make, then creating them using OS mode:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EThis%20is%20wild.%20I%20gave%20OS%20control%20to%20GPT-4%20via%20the%20latest%20update%20of%20Open%20Interpreter%20and%20now%20it%27s%20generating%20pictures%20it%20wants%20to%20see%20in%20%40everartai%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20GPT%20is%20controlling%20the%20mouse%20and%20adding%20text%20in%20the%20fields%2C%20I%20am%20not%20doing%20anything.%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/hGgML9epEc%22%3Epic.twitter.com/hGgML9epEc%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Pietro%20Schirano%20%28%40skirano%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/skirano/status/1747670816437735836%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%2017%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Open Interpreter Conversing With ChatGPT

OS mode has a conversation with ChatGPT and even asks it "What do you think about human/AI interaction?"

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EWatch%20GPT%20Vision%20with%20control%20over%20my%20OS%20talking%20to%20ChatGPT.%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20The%20most%20fascinating%20part%20is%20that%20it%27s%20intrigued%20by%20having%20a%20conversation%20with%20another%20%22similar.%22%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22What%20do%20you%20think%20about%20human/AI%20interaction%3F%22%20it%20asked.%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Also%2C%20the%20superhuman%20speed%20at%20which%20it%20types%2C%20lol%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/ViffvDK5H9%22%3Epic.twitter.com/ViffvDK5H9%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Pietro%20Schirano%20%28%40skirano%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/skirano/status/1747772471770583190%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%2018%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Sending an Email with Gmail

OS mode launches Safari, composes an email, and sends it:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3ELook%20ma%2C%20no%20hands%21%20This%20is%20%40OpenInterpreter%20using%20my%20mouse%20and%20keyboard%20to%20send%20an%20email.%20%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Imagine%20what%20else%20is%20possible.%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/GcBqbTwD23%22%3Epic.twitter.com/GcBqbTwD23%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Ty%20%28%40FieroTy%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/FieroTy/status/1743437525207928920%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%206%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

================
File: guides/multiple-instances.mdx
================
---
title: Multiple Instances
---

To create multiple instances, use the base class, `OpenInterpreter`:

```python
from interpreter import OpenInterpreter

agent_1 = OpenInterpreter()
agent_1.system_message = "This is a separate instance."

agent_2 = OpenInterpreter()
agent_2.system_message = "This is yet another instance."
```

For fun, you could make these instances talk to eachother:

```python
def swap_roles(messages):
    for message in messages:
        if message['role'] == 'user':
            message['role'] = 'assistant'
        elif message['role'] == 'assistant':
            message['role'] = 'user'
    return messages

agents = [agent_1, agent_2]

# Kick off the conversation
messages = [{"role": "user", "message": "Hello!"}]

while True:
    for agent in agents:
        messages = agent.chat(messages)
        messages = swap_roles(messages)
```

================
File: guides/os-mode.mdx
================
---
title: OS Mode
---

OS mode is a highly experimental mode that allows Open Interpreter to control the operating system visually through the mouse and keyboard. It provides a multimodal LLM like GPT-4V with the necessary tools to capture screenshots of the display and interact with on-screen elements such as text and icons. It will try to use the most direct method to achieve the goal, like using spotlight on Mac to open applications, and using query parameters in the URL to open websites with additional information.

OS mode is a work in progress, if you have any suggestions or experience issues, please reach out on our [Discord](https://discord.com/invite/6p3fD6rBVm).

To enable OS Mode, run the interpreter with the `--os` flag:

```bash
interpreter --os
```

Please note that screen recording permissions must be enabled for your terminal application for OS mode to work properly to work.

OS mode does not currently support multiple displays.

================
File: guides/profiles.mdx
================
---
title: Profiles
---

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/NxfdrGQrkHQ"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

Profiles are a powerful way to customize your instance of Open Interpreter.

Profiles are Python files that configure Open Interpreter. A wide range of fields from the [model](/settings/all-settings#model-selection) to the [context window](/settings/all-settings#context-window) to the [message templates](/settings/all-settings#user-message-template) can be configured in a Profile. This allows you to save multiple variations of Open Interpreter to optimize for your specific use-cases.

You can access your Profiles by running `interpreter --profiles`. This will open the directory where all of your Profiles are stored.

If you want to make your own profile, start with the [Template Profile](https://github.com/OpenInterpreter/open-interpreter/blob/main/interpreter/terminal_interface/profiles/defaults/template_profile.py).

To apply a Profile to an Open Interpreter session, you can run `interpreter --profile <name>`

# Example Python Profile

```Python
from interpreter import interpreter

interpreter.os = True
interpreter.llm.supports_vision = True

interpreter.llm.model = "gpt-4o"

interpreter.llm.supports_functions = True
interpreter.llm.context_window = 110000
interpreter.llm.max_tokens = 4096
interpreter.auto_run = True
interpreter.loop = True
```

# Example YAML Profile

<Info> Make sure YAML profile version is set to 0.2.5 </Info>

```YAML
llm:
  model: "gpt-4-o"
  temperature: 0
  # api_key: ...  # Your API key, if the API requires it
  # api_base: ...  # The URL where an OpenAI-compatible server is running to handle LLM API requests

# Computer Settings
computer:
  import_computer_api: True # Gives OI a helpful Computer API designed for code interpreting language models

# Custom Instructions
custom_instructions: ""  # This will be appended to the system message

# General Configuration
auto_run: False  # If True, code will run without asking for confirmation
offline: False  # If True, will disable some online features like checking for updates

version: 0.2.5 # Configuration file version (do not modify)
```

<Tip>
  There are many settings that can be configured. [See them all
  here](/settings/all-settings)
</Tip>

================
File: guides/running-locally.mdx
================
---
title: Running Locally
---

Open Interpreter can be run fully locally.

Users need to install software to run local LLMs. Open Interpreter supports multiple local model providers such as [Ollama](https://www.ollama.com/), [Llamafile](https://github.com/Mozilla-Ocho/llamafile), [Jan](https://jan.ai/), and [LM Studio](https://lmstudio.ai/).

<Tip>
  Local models perform better with extra guidance and direction. You can improve
  performance for your use-case by creating a new [Profile](/guides/profiles).
</Tip>

## Terminal Usage

### Local Explorer

A Local Explorer was created to simplify the process of using OI locally. To access this menu, run the command `interpreter --local`.

Select your chosen local model provider from the list of options.

Most providers will require the user to state the model they are using. Provider specific instructions are shown to the user in the menu.

### Custom Local

If you want to use a provider other than the ones listed, you will set the `--api_base` flag to set a [custom endpoint](/language-models/local-models/custom-endpoint).

You will also need to set the model by passing in the `--model` flag to select a [model](/settings/all-settings#model-selection).

```python
interpreter --api_base "http://localhost:11434" --model ollama/codestral
```

<Info>
  Other terminal flags are explained in [Settings](/settings/all-settings).
</Info>

## Python Usage

In order to have a Python script use Open Interpreter locally, some fields need to be set

```python
from interpreter import interpreter

interpreter.offline = True
interpreter.llm.model = "ollama/codestral"
interpreter.llm.api_base = "http://localhost:11434"

interpreter.chat("how many files are on my desktop?")
```

## Helpful settings for local models

Local models benefit from more coercion and guidance. This verbosity of adding extra context to messages can impact the conversational experience of Open Interpreter. The following settings allow templates to be applied to messages to improve the steering of the language model while maintaining the natural flow of conversation.

`interpreter.user_message_template` allows users to have their message wrapped in a template. This can be helpful steering a language model to a desired behaviour without needing the user to add extra context to their message.

`interpreter.always_apply_user_message_template` has all user messages to be wrapped in the template. If False, only the last User message will be wrapped.

`interpreter.code_output_template` wraps the output from the computer after code is run. This can help with nudging the language model to continue working or to explain outputs.

`interpreter.empty_code_output_template` is the message that is sent to the language model if code execution results in no output.

<Info>
  Other configuration settings are explained in
  [Settings](/settings/all-settings).
</Info>

================
File: guides/streaming-response.mdx
================
---
title: Streaming Response
---

You can stream messages, code, and code outputs out of Open Interpreter by setting `stream=True` in an `interpreter.chat(message)` call.

```python
for chunk in interpreter.chat("What's 34/24?", stream=True, display=False):
  print(chunk)
```

```
{"role": "assistant", "type": "code", "format": "python", "start": True}
{"role": "assistant", "type": "code", "format": "python", "content": "34"}
{"role": "assistant", "type": "code", "format": "python", "content": " /"}
{"role": "assistant", "type": "code", "format": "python", "content": " "}
{"role": "assistant", "type": "code", "format": "python", "content": "24"}
{"role": "assistant", "type": "code", "format": "python", "end": True}

{"role": "computer", "type": "confirmation", "format": "execution", "content": {"type": "code", "format": "python", "content": "34 / 24"}},

{"role": "computer", "type": "console", "start": True}
{"role": "computer", "type": "console", "format": "active_line", "content": "1"}
{"role": "computer", "type": "console", "format": "output", "content": "1.4166666666666667\n"}
{"role": "computer", "type": "console", "format": "active_line", "content": None},
{"role": "computer", "type": "console", "end": True}

{"role": "assistant", "type": "message", "start": True}
{"role": "assistant", "type": "message", "content": "The"}
{"role": "assistant", "type": "message", "content": " result"}
{"role": "assistant", "type": "message", "content": " of"}
{"role": "assistant", "type": "message", "content": " the"}
{"role": "assistant", "type": "message", "content": " division"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "34"}
{"role": "assistant", "type": "message", "content": "/"}
{"role": "assistant", "type": "message", "content": "24"}
{"role": "assistant", "type": "message", "content": " is"}
{"role": "assistant", "type": "message", "content": " approximately"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "1"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "content": "42"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "end": True}
```

**Note:** Setting `display=True` won't change the behavior of the streaming response, it will just render a display in your terminal.

# Anatomy

Each chunk of the streamed response is a dictionary, that has a "role" key that can be either "assistant" or "computer". The "type" key describes what the chunk is. The "content" key contains the actual content of the chunk.

Every 'message' is made up of chunks, and begins with a "start" chunk, and ends with an "end" chunk. This helps you parse the streamed response into messages.

Let's break down each part of the streamed response.

## Code

In this example, the LLM decided to start writing code first. It could have decided to write a message first, or to only write code, or to only write a message.

Every streamed chunk of type "code" has a format key that specifies the language. In this case it decided to write `python`.

This can be any language defined in [our languages directory.](https://github.com/OpenInterpreter/open-interpreter/tree/main/interpreter/core/computer/terminal/languages)

```

{"role": "assistant", "type": "code", "format": "python", "start": True}

```

Then, the LLM decided to write some code. The code is sent token-by-token:

```

{"role": "assistant", "type": "code", "format": "python", "content": "34"}
{"role": "assistant", "type": "code", "format": "python", "content": " /"}
{"role": "assistant", "type": "code", "format": "python", "content": " "}
{"role": "assistant", "type": "code", "format": "python", "content": "24"}

```

When the LLM finishes writing code, it will send an "end" chunk:

```

{"role": "assistant", "type": "code", "format": "python", "end": True}

```

## Code Output

After the LLM finishes writing a code block, Open Interpreter will attempt to run it.

**Before** it runs it, the following chunk is sent:

```

{"role": "computer", "type": "confirmation", "format": "execution", "content": {"type": "code", "language": "python", "code": "34 / 24"}}

```

If you check for this object, you can break (or get confirmation) **before** executing the code.

```python
# This example asks the user before running code

for chunk in interpreter.chat("What's 34/24?", stream=True):
    if "executing" in chunk:
        if input("Press ENTER to run this code.") != "":
            break
```

**While** the code is being executed, you'll receive the line of code that's being run:

```
{"role": "computer", "type": "console", "format": "active_line", "content": "1"}
```

We use this to highlight the active line of code on our UI, which keeps the user aware of what Open Interpreter is doing.

You'll then receive its output, if it produces any:

```
{"role": "computer", "type": "console", "format": "output", "content": "1.4166666666666667\n"}
```

When the code is **finished** executing, this flag will be sent:

```
{"role": "computer", "type": "console", "end": True}
```

## Message

Finally, the LLM decided to write a message. This is streamed token-by-token as well:

```
{"role": "assistant", "type": "message", "start": True}
{"role": "assistant", "type": "message", "content": "The"}
{"role": "assistant", "type": "message", "content": " result"}
{"role": "assistant", "type": "message", "content": " of"}
{"role": "assistant", "type": "message", "content": " the"}
{"role": "assistant", "type": "message", "content": " division"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "34"}
{"role": "assistant", "type": "message", "content": "/"}
{"role": "assistant", "type": "message", "content": "24"}
{"role": "assistant", "type": "message", "content": " is"}
{"role": "assistant", "type": "message", "content": " approximately"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "1"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "content": "42"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "end": True}
```

For an example in JavaScript on how you might process these streamed chunks, see the [migration guide](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md)

================
File: integrations/docker.mdx
================
---
title: Docker
---

Docker support is currently experimental. Running Open Interpreter inside of a Docker container may not function as you expect. Let us know on [Discord](https://discord.com/invite/6p3fD6rBVm) if you encounter errors or have suggestions to improve Docker support.

We are working on an official integration for Docker in the coming weeks. For now, you can use Open Interpreter in a sandboxed Docker container environment using the following steps:

1. If you do not have Docker Desktop installed, [install it](https://www.docker.com/products/docker-desktop) before proceeding.

2. Create a new directory and add a file named `Dockerfile` in it with the following contents:

```dockerfile
# Start with Python 3.11
FROM python:3.11

# Replace <your_openai_api_key> with your own key
ENV OPENAI_API_KEY <your_openai_api_key>

# Install Open Interpreter
RUN pip install open-interpreter
```

3. Run the following commands in the same directory to start Open Interpreter.

```bash
docker build -t openinterpreter .
docker run -d -it --name interpreter-instance openinterpreter interpreter
docker attach interpreter-instance
```

## Mounting Volumes

This is how you let it access _some_ files, by telling it a folder (a volume) it will be able to see / manipulate.

To mount a volume, you can use the `-v` flag followed by the path to the directory on your host machine, a colon, and then the path where you want to mount the directory in the container.

```bash
docker run -d -it -v /path/on/your/host:/path/in/the/container --name interpreter-instance openinterpreter interpreter
```

Replace `/path/on/your/host` with the path to the directory on your host machine that you want to mount, and replace `/path/in/the/container` with the path in the Docker container where you want to mount the directory.

Here's a simple example:

```bash
docker run -d -it -v $(pwd):/files --name interpreter-instance openinterpreter interpreter
```

In this example, `$(pwd)` is your current directory, and it is mounted to a `/files` directory in the Docker container (this creates that folder too).

## Flags

To add flags to the command, just append them after `interpreter`. For example, to run the interpreter with custom instructions, run the following command:

```bash
docker-compose run --rm oi interpreter --custom_instructions "Be as concise as possible"
```

Please note that some flags will not work. For example, `--config` will not work, because it cannot open the config file in the container. If you want to use a config file other than the default, you can create a `config.yml` file inside of the same directory, add your custom config, and then run the following command:

```bash
docker-compose run --rm oi interpreter --config_file config.yml
```

================
File: integrations/e2b.mdx
================
---
title: E2B
---

[E2B](https://e2b.dev/) is a secure, sandboxed environment where you can run arbitrary code.

To build this integration, you just need to replace Open Interpreter's `python` (which runs locally) with a `python` that runs on E2B.

First, [get an API key here](https://e2b.dev/), and set it:

```python
import os
os.environ["E2B_API_KEY"] = "<your_api_key_here>"
```

Then, define a custom language for Open Interpreter. The class name doesn't matter, but we'll call it `PythonE2B`:

```python
import e2b

class PythonE2B:
    """
    This class contains all requirements for being a custom language in Open Interpreter:

    - name (an attribute)
    - run (a method)
    - stop (a method)
    - terminate (a method)

    Here, we'll use E2B to power the `run` method.
    """

    # This is the name that will appear to the LLM.
    name = "python"

    # Optionally, you can append some information about this language to the system message:
    system_message = "# Follow this rule: Every Python code block MUST contain at least one print statement."

    # (E2B isn't a Jupyter Notebook, so we added ^ this so it would print things,
    # instead of putting variables at the end of code blocks, which is a Jupyter thing.)

    def run(self, code):
        """Generator that yields a dictionary in LMC Format."""

        # Run the code on E2B
        stdout, stderr = e2b.run_code('Python3', code)

        # Yield the output
        yield {
            "type": "console", "format": "output",
            "content": stdout + stderr # We combined these arbitrarily. Yield anything you'd like!
        }

    def stop(self):
        """Stops the code."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

    def terminate(self):
        """Terminates the entire process."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

# (Tip: Do this before adding/removing languages, otherwise OI might retain the state of previous languages:)
interpreter.computer.terminate()

# Give Open Interpreter its languages. This will only let it run PythonE2B:
interpreter.computer.languages = [PythonE2B]

# Try it out!
interpreter.chat("What's 349808*38490739?")
```

================
File: language-models/hosted-models/ai21.mdx
================
---
title: AI21
---

To use Open Interpreter with a model from AI21, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model j2-light
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "j2-light"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model from [AI21:](https://www.ai21.com/)

<CodeGroup>

```bash Terminal
interpreter --model j2-light
interpreter --model j2-mid
interpreter --model j2-ultra
```

```python Python
interpreter.llm.model = "j2-light"
interpreter.llm.model = "j2-mid"
interpreter.llm.model = "j2-ultra"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `AI21_API_KEY`       | The API key for authenticating to AI21's services. | [AI21 Account Page](https://www.ai21.com/account/api-keys) |

================
File: language-models/hosted-models/anthropic.mdx
================
---
title: Anthropic
---

To use Open Interpreter with a model from Anthropic, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model claude-instant-1
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "claude-instant-1"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model from [Anthropic:](https://www.anthropic.com/)

<CodeGroup>

```bash Terminal
interpreter --model claude-instant-1
interpreter --model claude-instant-1.2
interpreter --model claude-2
```

```python Python
interpreter.llm.model = "claude-instant-1"
interpreter.llm.model = "claude-instant-1.2"
interpreter.llm.model = "claude-2"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `ANTHROPIC_API_KEY`       | The API key for authenticating to Anthropic's services. | [Anthropic](https://www.anthropic.com/) |

================
File: language-models/hosted-models/anyscale.mdx
================
---
title: Anyscale
---

To use Open Interpreter with a model from Anyscale, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model anyscale/<model-name>
```

```python Python
from interpreter import interpreter

# Set the model to use from AWS Bedrock:
interpreter.llm.model = "anyscale/<model-name>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from Anyscale:

- Llama 2 7B Chat
- Llama 2 13B Chat
- Llama 2 70B Chat
- Mistral 7B Instruct
- CodeLlama 34b Instruct

<CodeGroup>

```bash Terminal
interpreter --model anyscale/meta-llama/Llama-2-7b-chat-hf
interpreter --model anyscale/meta-llama/Llama-2-13b-chat-hf
interpreter --model anyscale/meta-llama/Llama-2-70b-chat-hf
interpreter --model anyscale/mistralai/Mistral-7B-Instruct-v0.1
interpreter --model anyscale/codellama/CodeLlama-34b-Instruct-hf
```

```python Python
interpreter.llm.model = "anyscale/meta-llama/Llama-2-7b-chat-hf"
interpreter.llm.model = "anyscale/meta-llama/Llama-2-13b-chat-hf"
interpreter.llm.model = "anyscale/meta-llama/Llama-2-70b-chat-hf"
interpreter.llm.model = "anyscale/mistralai/Mistral-7B-Instruct-v0.1"
interpreter.llm.model = "anyscale/codellama/CodeLlama-34b-Instruct-hf"

```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                            | Where to Find                                                               |
| -------------------- | -------------------------------------- | --------------------------------------------------------------------------- |
| `ANYSCALE_API_KEY`   | The API key for your Anyscale account. | [Anyscale Account Settings](https://app.endpoints.anyscale.com/credentials) |

================
File: language-models/hosted-models/aws-sagemaker.mdx
================
---
title: AWS Sagemaker
---

To use Open Interpreter with a model from AWS Sagemaker, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model sagemaker/<model-name>
```

```python Python
# Sagemaker requires boto3 to be installed on your machine:
!pip install boto3

from interpreter import interpreter

interpreter.llm.model = "sagemaker/<model-name>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from AWS Sagemaker:

- Meta Llama 2 7B
- Meta Llama 2 7B (Chat/Fine-tuned)
- Meta Llama 2 13B
- Meta Llama 2 13B (Chat/Fine-tuned)
- Meta Llama 2 70B
- Meta Llama 2 70B (Chat/Fine-tuned)
- Your Custom Huggingface Model

<CodeGroup>

```bash Terminal

interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b-f
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b-f
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b-b-f
interpreter --model sagemaker/<your-hugginface-deployment-name>
```

```python Python
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b-f"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b-f"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b-b-f"
interpreter.llm.model = "sagemaker/<your-hugginface-deployment-name>"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable    | Description                                     | Where to Find                                                                       |
| ----------------------- | ----------------------------------------------- | ----------------------------------------------------------------------------------- |
| `AWS_ACCESS_KEY_ID`     | The API access key for your AWS account.        | [AWS Account Overview -> Security Credentials](https://console.aws.amazon.com/)     |
| `AWS_SECRET_ACCESS_KEY` | The API secret access key for your AWS account. | [AWS Account Overview -> Security Credentials](https://console.aws.amazon.com/)     |
| `AWS_REGION_NAME`       | The AWS region you want to use                  | [AWS Account Overview -> Navigation bar -> Region](https://console.aws.amazon.com/) |

================
File: language-models/hosted-models/azure.mdx
================
---
title: Azure
---

To use a model from Azure, set the `model` flag to begin with `azure/`:

<CodeGroup>

```bash Terminal
interpreter --model azure/<your_deployment_id>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "azure/<your_deployment_id>"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `AZURE_API_KEY`       | The API key for authenticating to Azure's services. | [Azure Account Page](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps) |
| `AZURE_API_BASE`      | The base URL for Azure's services. | [Azure Account Page](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps) |
| `AZURE_API_VERSION`   | The version of Azure's services. | [Azure Account Page](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps) |

================
File: language-models/hosted-models/baseten.mdx
================
---
title: Baseten
---

To use Open Interpreter with Baseten, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model baseten/<baseten-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "baseten/<baseten-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from Baseten:

- Falcon 7b (qvv0xeq)
- Wizard LM (q841o8w)
- MPT 7b Base (31dxrj3)

<CodeGroup>

```bash Terminal

interpreter --model baseten/qvv0xeq
interpreter --model baseten/q841o8w
interpreter --model baseten/31dxrj3


```

```python Python
interpreter.llm.model = "baseten/qvv0xeq"
interpreter.llm.model = "baseten/q841o8w"
interpreter.llm.model = "baseten/31dxrj3"


```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description     | Where to Find                                                                                            |
| -------------------- | --------------- | -------------------------------------------------------------------------------------------------------- |
| BASETEN_API_KEY'`    | Baseten API key | [Baseten Dashboard -> Settings -> Account -> API Keys](https://app.baseten.co/settings/account/api_keys) |

================
File: language-models/hosted-models/cloudflare.mdx
================
---
title: Cloudflare Workers AI
---

To use Open Interpreter with the Cloudflare Workers AI API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model cloudflare/<cloudflare-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "cloudflare/<cloudflare-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from Cloudflare Workers AI:

- Llama-2 7b chat fp16
- Llama-2 7b chat int8
- Mistral 7b instruct v0.1
- CodeLlama 7b instruct awq

<CodeGroup>

```bash Terminal

interpreter --model cloudflare/@cf/meta/llama-2-7b-chat-fp16
interpreter --model cloudflare/@cf/meta/llama-2-7b-chat-int8
interpreter --model @cf/mistral/mistral-7b-instruct-v0.1
interpreter --model @hf/thebloke/codellama-7b-instruct-awq

```

```python Python
interpreter.llm.model = "cloudflare/@cf/meta/llama-2-7b-chat-fp16"
interpreter.llm.model = "cloudflare/@cf/meta/llama-2-7b-chat-int8"
interpreter.llm.model = "@cf/mistral/mistral-7b-instruct-v0.1"
interpreter.llm.model = "@hf/thebloke/codellama-7b-instruct-awq"

```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable    | Description                | Where to Find                                                                                  |
| ----------------------- | -------------------------- | ---------------------------------------------------------------------------------------------- |
| `CLOUDFLARE_API_KEY'`   | Cloudflare API key         | [Cloudflare Profile Page -> API Tokens](https://dash.cloudflare.com/profile/api-tokens)        |
| `CLOUDFLARE_ACCOUNT_ID` | Your Cloudflare account ID | [Cloudflare Dashboard -> Grab the Account ID from the url like: https://dash.cloudflare.com/{CLOUDFLARE_ACCOUNT_ID}?account= ](https://dash.cloudflare.com/) |

================
File: language-models/hosted-models/cohere.mdx
================
---
title: Cohere
---

To use Open Interpreter with a model from Cohere, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model command-nightly
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "command-nightly"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [Cohere's models page:](https://www.cohere.ai/models)

<CodeGroup>

```bash Terminal
interpreter --model command
interpreter --model command-light
interpreter --model command-medium
interpreter --model command-medium-beta
interpreter --model command-xlarge-beta
interpreter --model command-nightly
```

```python Python
interpreter.llm.model = "command"
interpreter.llm.model = "command-light"
interpreter.llm.model = "command-medium"
interpreter.llm.model = "command-medium-beta"
interpreter.llm.model = "command-xlarge-beta"
interpreter.llm.model = "command-nightly"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `COHERE_API_KEY`       | The API key for authenticating to Cohere's services. | [Cohere Account Page](https://app.cohere.ai/login) |

================
File: language-models/hosted-models/deepinfra.mdx
================
---
title: DeepInfra
---

To use Open Interpreter with DeepInfra, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model deepinfra/<deepinfra-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "deepinfra/<deepinfra-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from DeepInfra:

- Llama-2 70b chat hf
- Llama-2 7b chat hf
- Llama-2 13b chat hf
- CodeLlama 34b instruct awq
- Mistral 7b instruct v0.1
- jondurbin/airoboros I2 70b gpt3 1.4.1

<CodeGroup>

```bash Terminal

interpreter --model deepinfra/meta-llama/Llama-2-70b-chat-hf
interpreter --model deepinfra/meta-llama/Llama-2-7b-chat-hf
interpreter --model deepinfra/meta-llama/Llama-2-13b-chat-hf
interpreter --model deepinfra/codellama/CodeLlama-34b-Instruct-hf
interpreter --model deepinfra/mistral/mistral-7b-instruct-v0.1
interpreter --model deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1

```

```python Python
interpreter.llm.model = "deepinfra/meta-llama/Llama-2-70b-chat-hf"
interpreter.llm.model = "deepinfra/meta-llama/Llama-2-7b-chat-hf"
interpreter.llm.model = "deepinfra/meta-llama/Llama-2-13b-chat-hf"
interpreter.llm.model = "deepinfra/codellama/CodeLlama-34b-Instruct-hf"
interpreter.llm.model = "deepinfra/mistral-7b-instruct-v0.1"
interpreter.llm.model = "deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1"

```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description       | Where to Find                                                          |
| -------------------- | ----------------- | ---------------------------------------------------------------------- |
| `DEEPINFRA_API_KEY'` | DeepInfra API key | [DeepInfra Dashboard -> API Keys](https://deepinfra.com/dash/api_keys) |

================
File: language-models/hosted-models/gpt-4-setup.mdx
================
---
title: GPT-4 Setup
---

# Setting Up GPT-4

Step 1 - Install OpenAI packages

```
pip install openai
```

Step 2 - create a new API key at [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)

![alt](https://drive.google.com/file/d/1xfs_SZVbK6hhDf2-_AMH4uCxdgFlGiMK/view?usp=sharing)

Step 3 - Run the interpreter command after installing open-interpreter and enter your newly generated api key

![alt](https://drive.google.com/file/d/1avLeCIKvQV732mbrf-91s5T7uJfTLyCS/view?usp=sharing)

or

**FOR MACOS :**

1.  **Open Terminal**: You can find it in the Applications folder or search for it using Spotlight (Command + Space).
2.  **Edit Bash Profile**: Use the command `nano ~/.bash_profile` or `nano ~/.zshrc` (for newer MacOS versions) to open the profile file in a text editor.
3.  **Add Environment Variable**: In the editor, add the line below, replacing `your-api-key-here` with your actual API key:

    ```
    export OPENAI\_API\_KEY='your-api-key-here'
    ```

4.  **Save and Exit**: Press Ctrl+O to write the changes, followed by Ctrl+X to close the editor.
5.  **Load Your Profile**: Use the command `source ~/.bash_profile` or `source ~/.zshrc` to load the updated profile.
6.  **Verification**: Verify the setup by typing `echo $OPENAI_API_KEY` in the terminal. It should display your API key.

**FOR WINDOWS :**

1.  **Open Command Prompt**: You can find it by searching "cmd" in the start menu.
2.  **Set environment variable in the current session**: To set the environment variable in the current session, use the command below, replacing `your-api-key-here` with your actual API key:

    ```
    setx OPENAI\_API\_KEY "your-api-key-here"
    ```

    This command will set the OPENAI_API_KEY environment variable for the current session.

3.  **Permanent setup**: To make the setup permanent, add the variable through the system properties as follows:

    - Right-click on 'This PC' or 'My Computer' and select 'Properties'.
    - Click on 'Advanced system settings'.
    - Click the 'Environment Variables' button.
    - In the 'System variables' section, click 'New...' and enter OPENAI_API_KEY as the variable name and your API key as the variable value.

4.  **Verification**: To verify the setup, reopen the command prompt and type the command below. It should display your API key: `echo %OPENAI_API_KEY%`

================
File: language-models/hosted-models/huggingface.mdx
================
---
title: Huggingface
---

To use Open Interpreter with Huggingface models, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model huggingface/<huggingface-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "huggingface/<huggingface-model>"
interpreter.chat()
```

</CodeGroup>

You may also need to specify your Huggingface api base url:
<CodeGroup>

```bash Terminal
interpreter --api_base <https://my-endpoint.huggingface.cloud>
```

```python Python
from interpreter import interpreter

interpreter.llm.api_base = "https://my-endpoint.huggingface.cloud"
interpreter.chat()
```

</CodeGroup>

# Supported Models

Open Interpreter should work with almost any text based hugging face model.

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable   | Description                 | Where to Find                                                                      |
| ---------------------- | --------------------------- | ---------------------------------------------------------------------------------- |
| `HUGGINGFACE_API_KEY'` | Huggingface account API key | [Huggingface -> Settings -> Access Tokens](https://huggingface.co/settings/tokens) |

================
File: language-models/hosted-models/mistral-api.mdx
================
---
title: Mistral AI API
---

To use Open Interpreter with the Mistral API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model mistral/<mistral-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "mistral/<mistral-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from the Mistral API:

- mistral-tiny
- mistral-small
- mistral-medium

<CodeGroup>

```bash Terminal

interpreter --model mistral/mistral-tiny
interpreter --model mistral/mistral-small
interpreter --model mistral/mistral-medium
```

```python Python
interpreter.llm.model = "mistral/mistral-tiny"
interpreter.llm.model = "mistral/mistral-small"
interpreter.llm.model = "mistral/mistral-medium"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                                  | Where to Find                                      |
| -------------------- | -------------------------------------------- | -------------------------------------------------- |
| `MISTRAL_API_KEY`    | The Mistral API key from Mistral API Console | [Mistral API Console](https://console.mistral.ai/user/api-keys/) |

================
File: language-models/hosted-models/nlp-cloud.mdx
================
---
title: NLP Cloud
---

To use Open Interpreter with NLP Cloud, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model dolphin
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "dolphin"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description       | Where to Find                                                     |
| -------------------- | ----------------- | ----------------------------------------------------------------- |
| `NLP_CLOUD_API_KEY'` | NLP Cloud API key | [NLP Cloud Dashboard -> API KEY](https://nlpcloud.com/home/token) |

================
File: language-models/hosted-models/openai.mdx
================
---
title: OpenAI
---

To use Open Interpreter with a model from OpenAI, simply run:

<CodeGroup>

```bash Terminal
interpreter
```

```python Python
from interpreter import interpreter

interpreter.chat()
```

</CodeGroup>

This will default to `gpt-4-turbo`, which is the most capable publicly available model for code interpretation (Open Interpreter was designed to be used with `gpt-4`).

To run a specific model from OpenAI, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model gpt-3.5-turbo
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "gpt-3.5-turbo"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [OpenAI's models page:](https://platform.openai.com/docs/models/)

<CodeGroup>

```bash Terminal
interpreter --model gpt-4o
```

```python Python
interpreter.llm.model = "gpt-4o"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                                          | Where to Find                                                       |
| -------------------- | ---------------------------------------------------- | ------------------------------------------------------------------- |
| `OPENAI_API_KEY`     | The API key for authenticating to OpenAI's services. | [OpenAI Account Page](https://platform.openai.com/account/api-keys) |

================
File: language-models/hosted-models/openrouter.mdx
================
---
title: OpenRouter
---

To use Open Interpreter with a model from OpenRouter, set the `model` flag to begin with `openrouter/`:

<CodeGroup>

```bash Terminal
interpreter --model openrouter/openai/gpt-3.5-turbo
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "openrouter/openai/gpt-3.5-turbo"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [OpenRouter's models page:](https://openrouter.ai/models)

<CodeGroup>

```bash Terminal
interpreter --model openrouter/openai/gpt-3.5-turbo
interpreter --model openrouter/openai/gpt-3.5-turbo-16k
interpreter --model openrouter/openai/gpt-4
interpreter --model openrouter/openai/gpt-4-32k
interpreter --model openrouter/anthropic/claude-2
interpreter --model openrouter/anthropic/claude-instant-v1
interpreter --model openrouter/google/palm-2-chat-bison
interpreter --model openrouter/google/palm-2-codechat-bison
interpreter --model openrouter/meta-llama/llama-2-13b-chat
interpreter --model openrouter/meta-llama/llama-2-70b-chat
```

```python Python
interpreter.llm.model = "openrouter/openai/gpt-3.5-turbo"
interpreter.llm.model = "openrouter/openai/gpt-3.5-turbo-16k"
interpreter.llm.model = "openrouter/openai/gpt-4"
interpreter.llm.model = "openrouter/openai/gpt-4-32k"
interpreter.llm.model = "openrouter/anthropic/claude-2"
interpreter.llm.model = "openrouter/anthropic/claude-instant-v1"
interpreter.llm.model = "openrouter/google/palm-2-chat-bison"
interpreter.llm.model = "openrouter/google/palm-2-codechat-bison"
interpreter.llm.model = "openrouter/meta-llama/llama-2-13b-chat"
interpreter.llm.model = "openrouter/meta-llama/llama-2-70b-chat"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `OPENROUTER_API_KEY`       | The API key for authenticating to OpenRouter's services. | [OpenRouter Account Page](https://openrouter.ai/keys) |
| `OR_SITE_URL`      | The site URL for OpenRouter's services. | [OpenRouter Account Page](https://openrouter.ai/keys) |
| `OR_APP_NAME`   | The app name for OpenRouter's services. | [OpenRouter Account Page](https://openrouter.ai/keys) |

================
File: language-models/hosted-models/palm.mdx
================
---
title: PaLM API - Google
---

To use Open Interpreter with PaLM, you must `pip install -q google-generativeai`, then set the `model` flag in Open Interpreter:

<CodeGroup>

```bash Terminal
interpreter --model palm/chat-bison
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "palm/chat-bison"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                                                      | Where to Find                                                                        |
| -------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| `PALM_API_KEY`       | The PaLM API key from Google Generative AI Developers dashboard. | [Google Generative AI Developers Dashboard](https://developers.generativeai.google/) |

================
File: language-models/hosted-models/perplexity.mdx
================
---
title: Perplexity
---

To use Open Interpreter with the Perplexity API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model perplexity/<perplexity-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "perplexity/<perplexity-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from the Perplexity API:

- pplx-7b-chat
- pplx-70b-chat
- pplx-7b-online
- pplx-70b-online
- codellama-34b-instruct
- llama-2-13b-chat
- llama-2-70b-chat
- mistral-7b-instruct
- openhermes-2-mistral-7b
- openhermes-2.5-mistral-7b
- pplx-7b-chat-alpha
- pplx-70b-chat-alpha

<CodeGroup>

```bash Terminal

interpreter --model perplexity/pplx-7b-chat
interpreter --model perplexity/pplx-70b-chat
interpreter --model perplexity/pplx-7b-online
interpreter --model perplexity/pplx-70b-online
interpreter --model perplexity/codellama-34b-instruct
interpreter --model perplexity/llama-2-13b-chat
interpreter --model perplexity/llama-2-70b-chat
interpreter --model perplexity/mistral-7b-instruct
interpreter --model perplexity/openhermes-2-mistral-7b
interpreter --model perplexity/openhermes-2.5-mistral-7b
interpreter --model perplexity/pplx-7b-chat-alpha
interpreter --model perplexity/pplx-70b-chat-alpha
```

```python Python
interpreter.llm.model = "perplexity/pplx-7b-chat"
interpreter.llm.model = "perplexity/pplx-70b-chat"
interpreter.llm.model = "perplexity/pplx-7b-online"
interpreter.llm.model = "perplexity/pplx-70b-online"
interpreter.llm.model = "perplexity/codellama-34b-instruct"
interpreter.llm.model = "perplexity/llama-2-13b-chat"
interpreter.llm.model = "perplexity/llama-2-70b-chat"
interpreter.llm.model = "perplexity/mistral-7b-instruct"
interpreter.llm.model = "perplexity/openhermes-2-mistral-7b"
interpreter.llm.model = "perplexity/openhermes-2.5-mistral-7b"
interpreter.llm.model = "perplexity/pplx-7b-chat-alpha"
interpreter.llm.model = "perplexity/pplx-70b-chat-alpha"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable    | Description                          | Where to Find                                                     |
| ----------------------- | ------------------------------------ | ----------------------------------------------------------------- |
| `PERPLEXITYAI_API_KEY'` | The Perplexity API key from pplx-api | [Perplexity API Settings](https://www.perplexity.ai/settings/api) |

================
File: language-models/hosted-models/petals.mdx
================
---
title: Petals
---

To use Open Interpreter with a model from Petals, set the `model` flag to begin with `petals/`:

<CodeGroup>

```bash Terminal
interpreter --model petals/petals-team/StableBeluga2
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "petals/petals-team/StableBeluga2"
interpreter.chat()
```

</CodeGroup>

# Pre-Requisites

Ensure you have petals installed:

```bash Terminal
pip install git+https://github.com/bigscience-workshop/petals
```

# Supported Models

We support any model on [Petals:](https://github.com/bigscience-workshop/petals)

<CodeGroup>

```bash Terminal
interpreter --model petals/petals-team/StableBeluga2
interpreter --model petals/huggyllama/llama-65b
```

```python Python
interpreter.llm.model = "petals/petals-team/StableBeluga2"
interpreter.llm.model = "petals/huggyllama/llama-65b"
```

</CodeGroup>

# Required Environment Variables

No environment variables are required to use these models.

================
File: language-models/hosted-models/replicate.mdx
================
---
title: Replicate
---

To use Open Interpreter with a model from Replicate, set the `model` flag to begin with `replicate/`:

<CodeGroup>

```bash Terminal
interpreter --model replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [Replicate's models page:](https://replicate.ai/explore)

<CodeGroup>

```bash Terminal
interpreter --model replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf
interpreter --model replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52
interpreter --model replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b
interpreter --model replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f
```

```python Python
interpreter.llm.model = "replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf"
interpreter.llm.model = "replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52"
interpreter.llm.model = "replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b"
interpreter.llm.model = "replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `REPLICATE_API_KEY`       | The API key for authenticating to Replicate's services. | [Replicate Account Page](https://replicate.ai/login) |

================
File: language-models/hosted-models/togetherai.mdx
================
---
title: Together AI
---

To use Open Interpreter with Together AI, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model together_ai/<together_ai-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "together_ai/<together_ai-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

All models on Together AI are supported.

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description                                   | Where to Find                                                                               |
| --------------------- | --------------------------------------------- | ------------------------------------------------------------------------------------------- |
| `TOGETHERAI_API_KEY'` | The TogetherAI API key from the Settings page | [TogetherAI -> Profile -> Settings -> API Keys](https://api.together.xyz/settings/api-keys) |

================
File: language-models/hosted-models/vertex-ai.mdx
================
---
title: Google (Vertex AI)
---

## Pre-requisites
* `pip install google-cloud-aiplatform`
* Authentication: 
    * run `gcloud auth application-default login` See [Google Cloud Docs](https://cloud.google.com/docs/authentication/external/set-up-adc)
    * Alternatively you can set `application_default_credentials.json`

To use Open Interpreter with Google's Vertex AI API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model gemini-pro
interpreter --model gemini-pro-vision
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "gemini-pro"
interpreter.llm.model = "gemini-pro-vision"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

Environment Variable  | Description  | Where to Find  |
--------------------- | ------------ | -------------- |
`VERTEXAI_PROJECT`       | The Google Cloud project ID. | [Google Cloud Console](https://console.cloud.google.com/vertex-ai) |
`VERTEXAI_LOCATION`      | The location of your Vertex AI resources. | [Google Cloud Console](https://console.cloud.google.com/vertex-ai) |

## Supported Models

- gemini-pro
- gemini-pro-vision
- chat-bison-32k
- chat-bison
- chat-bison@001
- codechat-bison
- codechat-bison-32k
- codechat-bison@001

================
File: language-models/hosted-models/vllm.mdx
================
---
title: vLLM
---

To use Open Interpreter with vLLM, you will need to:

1. `pip install vllm`
2. Set the api_base flag:

<CodeGroup>

```bash Terminal
interpreter --api_base <https://your-hosted-vllm-server>
```

```python Python
from interpreter import interpreter

interpreter.llm.api_base = "<https://your-hosted-vllm-server>"
interpreter.chat()
```

</CodeGroup>

3. Set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model vllm/<perplexity-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "vllm/<perplexity-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

All models from VLLM should be supported

================
File: language-models/local-models/best-practices.mdx
================
---
title: "Best Practices"
---

Most settings  like model architecture and GPU offloading  can be adjusted via your LLM providers like [LM Studio.](https://lmstudio.ai/)

**However, `max_tokens` and `context_window` should be set via Open Interpreter.**

For local mode, smaller context windows will use less RAM, so we recommend trying a much shorter window (~1000) if it's is failing or if it's slow.

<CodeGroup>

```bash Terminal
interpreter --local --max_tokens 1000 --context_window 3000
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "openai/x" # Tells OI to send messages in OpenAI's format
interpreter.llm.api_key = "fake_key" # LiteLLM, which we use to talk to LM Studio, requires this
interpreter.llm.api_base = "http://localhost:1234/v1" # Point this at any OpenAI compatible server

interpreter.llm.max_tokens = 1000
interpreter.llm.context_window = 3000

interpreter.chat()
```

</CodeGroup>

<br />

<Info>Make sure `max_tokens` is less than `context_window`.</Info>

================
File: language-models/local-models/custom-endpoint.mdx
================
---
title: Custom Endpoint
---

Simply set `api_base` to any OpenAI compatible server:

<CodeGroup>
```bash Terminal
interpreter --api_base <custom_endpoint>
```

```python Python
from interpreter import interpreter

interpreter.llm.api_base = "<custom_endpoint>"
interpreter.chat()
```

</CodeGroup>

================
File: language-models/local-models/janai.mdx
================
---
title: Jan.ai
---

Jan.ai is an open-source platform for running local language models on your computer, and is equipped with a built in server.

To run Open Interpreter with Jan.ai, follow these steps:

1. [Install](https://jan.ai/) the Jan.ai Desktop Application on your computer.

2. Once installed, you will need to install a language model. Click the 'Hub' icon on the left sidebar (the four squares icon). Click the 'Download' button next to the model you would like to install, and wait for it to finish installing before continuing.

3. To start your model, click the 'Settings' icon at the bottom of the left sidebar. Then click 'Models' under the CORE EXTENSIONS section. This page displays all of your installed models. Click the options icon next to the model you would like to start (vertical ellipsis icon). Then click 'Start Model', which will take a few seconds to fire up.

4. Click the 'Advanced' button under the GENERAL section, and toggle on the "Enable API Server" option. This will start a local server that you can use to interact with your model.

5. Now we fire up Open Interpreter with this custom model. Either run `interpreter --local` in the terminal to set it up interactively, or run this command, but replace `<model_id>` with the id of the model you downloaded:

<CodeGroup>

```bash Terminal
interpreter --api_base http://localhost:1337/v1  --model <model_id>
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "<model_id>"
interpreter.llm.api_base = "http://localhost:1337/v1 "

interpreter.chat()
```

</CodeGroup>

If your model can handle a longer context window than the default 3000, you can set the context window manually by running:

<CodeGroup>

```bash Terminal
interpreter --api_base http://localhost:1337/v1  --model <model_id> --context_window 5000
```

```python Python
from interpreter import interpreter

interpreter.context_window = 5000
```

</CodeGroup>

<Warning>
  If Jan is producing strange output, or no output at all, make sure to update
  to the latest version and clean your cache.
</Warning>

================
File: language-models/local-models/llamafile.mdx
================
---
title: LlamaFile
---

The easiest way to get started with local models in Open Interpreter is to run `interpreter --local` in the terminal, select LlamaFile, then go through the interactive set up process. This will download the model and start the server for you. If you choose to do it manually, you can follow the instructions below.

To use LlamaFile manually with Open Interpreter, you'll need to download the model and start the server by running the file in the terminal. You can do this with the following commands:

```bash
# Download Mixtral

wget https://huggingface.co/jartine/Mixtral-8x7B-v0.1.llamafile/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile

# Make it an executable

chmod +x mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile

# Start the server

./mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile

# In a separate terminal window, run OI and point it at the llamafile server

interpreter --api_base https://localhost:8080/v1
```

Please note that if you are using a Mac with Apple Silicon, you'll need to have Xcode installed.

================
File: language-models/local-models/lm-studio.mdx
================
---
title: LM Studio
---

Open Interpreter can use OpenAI-compatible server to run models locally. (LM Studio, jan.ai, ollama etc)

Simply run `interpreter` with the api_base URL of your inference server (for LM studio it is `http://localhost:1234/v1` by default):

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

Alternatively you can use Llamafile without installing any third party software just by running

```shell
interpreter --local
```

for a more detailed guide check out [this video by Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

**How to run LM Studio in the background.**

1. Download [https://lmstudio.ai/](https://lmstudio.ai/) then start it.
2. Select a model then click ** Download**.
3. Click the **** button on the left (below ).
4. Select your model at the top, then click **Start Server**.

Once the server is running, you can begin your conversation with Open Interpreter.

(When you run the command `interpreter --local` and select LMStudio, these steps will be displayed.)

<Info>
  Local mode sets your `context_window` to 3000, and your `max_tokens` to 1000.
  If your model has different requirements, [set these parameters
  manually.](/settings#language-model)
</Info>

# Python

Compared to the terminal interface, our Python package gives you more granular control over each setting.

You can point `interpreter.llm.api_base` at any OpenAI compatible server (including one running locally).

For example, to connect to [LM Studio](https://lmstudio.ai/), use these settings:

```python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "openai/x" # Tells OI to send messages in OpenAI's format
interpreter.llm.api_key = "fake_key" # LiteLLM, which we use to talk to LM Studio, requires this
interpreter.llm.api_base = "http://localhost:1234/v1" # Point this at any OpenAI compatible server

interpreter.chat()
```

Simply ensure that **LM Studio**, or any other OpenAI compatible server, is running at `api_base`.

================
File: language-models/local-models/ollama.mdx
================
---
title: Ollama
---

Ollama is an easy way to get local language models running on your computer through a command-line interface.

To run Ollama with Open interpreter:

1. Download Ollama for your platform from [here](https://ollama.ai/download).

2. Open the installed Ollama application, and go through the setup, which will require your password.

3. Now you are ready to download a model. You can view all available models [here](https://ollama.ai/library). To download a model, run:

```bash
ollama run <model-name>
```

4. It will likely take a while to download, but once it does, we are ready to use it with Open Interpreter. You can either run `interpreter --local` to set it up interactively in the terminal, or do it manually:

<CodeGroup>

```bash Terminal
interpreter --model ollama/<model-name>
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "ollama_chat/<model-name>"
interpreter.llm.api_base = "http://localhost:11434"

interpreter.chat()
```

</CodeGroup>

For any future runs with Ollama, ensure that the Ollama server is running. If using the desktop application, you can check to see if the Ollama menu bar item is active.

<Warning>
  If Ollama is producing strange output, make sure to update to the latest
  version
</Warning>

================
File: language-models/custom-models.mdx
================
---
title: Custom Models
---

In addition to hosted and local language models, Open Interpreter also supports custom models.

As long as your system can accept an input and stream an output (and can be interacted with via a Python generator) it can be used as a language model in Open Interpreter.

Simply replace the OpenAI-compatible `completions` function in your language model with one of your own:

```python
def custom_language_model(messages, model, stream, max_tokens):
    """
    OpenAI-compatible completions function (this one just echoes what the user said back).
    To make it OpenAI-compatible and parsable, `choices` has to be the root property.
    The property `delta` is used to signify streaming.
    """
    users_content = messages[-1].get("content") # Get last message's content

    for character in users_content:
        yield {"choices": [{"delta": {"content": character}}]}

# Tell Open Interpreter to power the language model with this function

interpreter.llm.completions = custom_language_model
```

Then, set the following settings:

```
interpreter.llm.context_window = 2000 # In tokens
interpreter.llm.max_tokens = 1000 # In tokens
interpreter.llm.supports_vision = False # Does this completions endpoint accept images?
interpreter.llm.supports_functions = False # Does this completions endpoint accept/return function calls?
```

And start using it:

```
interpreter.chat("Hi!") # Returns/displays "Hi!" character by character
```

================
File: language-models/introduction.mdx
================
---
title: Introduction
---

**Open Interpreter** works with both hosted and local language models.

Hosted models are faster and more capable, but require payment. Local models are private and free, but are often less capable.

For this reason, we recommend starting with a **hosted** model, then switching to a local model once you've explored Open Interpreter's capabilities.

<CardGroup>

<Card title="Hosted setup" icon="cloud" href="/language-models/hosted-models">
  Connect to a hosted language model like GPT-4 **(recommended)**
</Card>

<Card title="Local setup" icon="microchip" href="/language-models/local-models">
  Setup a local language model like Mistral
</Card>

</CardGroup>

<br />
<br />

<Info>
  Thank you to the incredible [LiteLLM](https://litellm.ai/) team for their
  efforts in connecting Open Interpreter to hosted providers.
</Info>

================
File: language-models/settings.mdx
================
---
title: Settings
---

The `interpreter.llm` is responsible for running the language model.

[Click here](/settings/all-settings#language-model) to view `interpreter.llm` settings.

================
File: legal/license.mdx
================
---
title: Licenses
description: By using Interpreter, you agree to our Privacy Policy and Terms of Service
---

\n

# Interpreter Privacy Policy

Last updated: August 13, 2024

Open Interpreter, Inc. ("we," "our," or "us") is committed to protecting your privacy. This Privacy Policy explains how we collect, use, and safeguard your information when you use our AI desktop application, Interpreter ("the Application").

## 1. Information We Collect

We collect the following information:

a) Personal Information:
   - Name
   - Email address

b) Usage Information:
   - Conversations with the AI chatbot
   - Code generated during use of the Application

## 2. How We Use Your Information

We use the collected information to:

a) Provide and improve our services
b) Communicate with you about your account or the Application
c) Improve our underlying AI model

## 3. Data Anonymization

All conversations and generated code are anonymized before being used to improve our AI model. However, please be aware that if you explicitly instruct the AI to include personal identifiable information (PII) in the generated code, such information may be captured.

## 4. Data Security

We implement appropriate technical and organizational measures to protect your personal information. However, no method of transmission over the Internet or electronic storage is 100% secure.

## 5. Your Rights

You have the right to access, correct, or delete your personal information. Please contact us at help@openinterpreter.com for any data-related requests.

## 6. Changes to This Privacy Policy

We may update our Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page and updating the "Last updated" date.

## 7. Contact Us

If you have any questions about this Privacy Policy, please contact us at help@openinterpreter.com.

By using Interpreter, you agree to the collection and use of information in accordance with this Privacy Policy.

---

# Interpreter Terms of Service

Last updated: August 13, 2024

Please read these Terms of Service ("Terms", "Terms of Service") carefully before using the Interpreter desktop application (the "Service") operated by Open Interpreter, Inc. ("us", "we", or "our").

## 1. Acceptance of Terms

By accessing or using the Service, you agree to be bound by these Terms. If you disagree with any part of the terms, then you may not access the Service.

## 2. Description of Service

Interpreter is an AI-powered desktop application that allows users to interact with an AI chatbot to write and execute code.

## 3. User Responsibilities

By using our Service, you agree to:

a) Review ALL code generated by Interpreter before execution.
b) Grant explicit permission before any code is executed on your system.
c) Understand the implications of the code you choose to execute.
d) Use the Service in compliance with all applicable laws and regulations.

## 4. Safety Measures

We have implemented the following safety measures:

a) We employ LakeraGuard, an industry-leading solution, to assess potential harm in generated code.
b) Our custom judge layer provides explanations of what the code is intended to do.
c) You will always be asked for permission before any code is executed.

## 5. Assumption of Risk

By using Interpreter, you acknowledge and accept the following risks:

a) The application may generate code that, if executed, could alter or delete files on your system.
b) While we have implemented safety measures, the AI may occasionally generate code with unintended consequences.
c) In rare cases, the application might generate code that, if executed, could potentially expose sensitive information.

## 6. Limitation of Liability

To the fullest extent permitted by law, Open Interpreter, Inc. shall not be liable for any direct, indirect, incidental, special, consequential, or exemplary damages resulting from your use of the Service or any code generated or executed through the Service.

## 7. Indemnification

You agree to indemnify and hold harmless Open Interpreter, Inc., its officers, directors, employees, and agents from any claims, damages, losses, liabilities, and expenses (including legal fees) arising out of or related to your use of the Service or any code generated or executed through the Service.

## 8. Modifications to Terms

We reserve the right to modify these Terms at any time. Continued use of the Service after changes constitutes acceptance of the modified Terms.

## 9. Governing Law

These Terms shall be governed by and construed in accordance with the laws of [Your Jurisdiction], without regard to its conflict of law provisions.

## 10. Contact Us

If you have any questions about these Terms, please contact us at help@openinterpreter.com.

By using Interpreter, you acknowledge that you have read, understood, and agree to be bound by these Terms of Service.

================
File: protocols/lmc-messages.mdx
================
---
title: LMC Messages
---

To support the incoming `L`anguage `M`odel `C`omputer architecture, we extend OpenAI's messages format to include additional information, and a new role called `computer`:

```python
# The user sends a message.
{"role": "user", "type": "message", "content": "What's 2380*3875?"}

# The assistant runs some code.
{"role": "assistant", "type": "code", "format": "python", "content": "2380*3875"}

# The computer responds with the result of the code.
{"role": "computer", "type": "console", "format": "output", "content": "9222500"}

# The assistant sends a message.
{"role": "assistant", "type": "message", "content": "The result of multiplying 2380 by 3875 is 9222500."}
```

## Anatomy

Each message in the LMC architecture has the following parameters (`format` is only present for some types):

```
{
  "role": "<role>",       # Who is sending the message.
  "type": "<type>",       # What kind of message is being sent.
  "format": "<format>"    # Some types need to be further specified, so they optionally use this parameter.
  "content": "<content>", # What the message says.
}
```

Parameter|Description|
---|---|
`role`|The sender of the message.|
`type`|The kind of message being sent.|
`content`|The actual content of the message.|
`format`|The format of the content (optional).|

## Roles

Role|Description|
---|---|
`user`|The individual interacting with the system.|
`assistant`|The language model.|
`computer`|The system that executes the language model's commands.|

## Possible Message Types / Formats

Any role can produce any of the following formats, but we've included a `Common Roles` column to give you a sense of the message type's usage.

Type|Format|Content Description|Common Roles
---|---|---|---|
message|None|A text-only message.|`user`, `assistant`|
console|active_line|The active line of code (from the most recent code block) that's executing.|`computer`|
console|output|Text output resulting from `print()` statements in Python, `console.log()` statements in Javascript, etc. **This includes errors.**|`computer`|
image|base64|A `base64` image in PNG format (default)|`user`, `computer`|
image|base64.png|A `base64` image in PNG format|`user`, `computer`|
image|base64.jpeg|A `base64` image in JPEG format|`user`, `computer`|
image|path|A path to an image.|`user`, `computer`|
code|html|HTML code that should be executed.|`assistant`, `computer`|
code|javascript|JavaScript code that should be executed.|`assistant`, `computer`|
code|python|Python code that should be executed.|`assistant`|
code|r|R code that should be executed.|`assistant`|
code|applescript|AppleScript code that should be executed.|`assistant`|
code|shell|Shell code that should be executed.|`assistant`|
audio|wav|audio in wav format for websocket.|`user`|

================
File: safety/best-practices.mdx
================
---
title: Best Practices
---

LLM's are not perfect. They can make mistakes, they can be tricked into doing things that they shouldn't, and they are capable of writing unsafe code. This page will help you understand how to use these LLM's safely.

## Best Practices

- Avoid asking it to perform potentially risky tasks. This seems obvious, but it's the number one way to prevent safety mishaps.

- Run it in a sandbox. This is the safest way to run it, as it completely isolates the code it runs from the rest of your system.

- Use trusted models. Yes, Open Interpreter can be configured to run pretty much any text-based model on huggingface. But it does not mean it's a good idea to run any random model you find. Make sure you trust the models you're using. If you're not sure, run it in a sandbox. Nefarious LLM's are becoming a real problem, and they are not going away anytime soon.

- Local models are fun! But GPT-4 is probably your safest bet. OpenAI has their models aligned in a major way. It will outperform the local models, and it will generally refuse to run unsafe code, as it truly understands that the code it writes could be run. It has a pretty good idea what unsafe code looks like, and will refuse to run code like `rm -rf /` that would delete your entire disk, for example.

- The [--safe_mode](/safety/safe-mode) argument is your friend. It enables code scanning, and can use [guarddog](https://github.com/DataDog/guarddog) to identify malicious PyPi and npm packages. It's not a perfect solution, but it's a great start.

================
File: safety/introduction.mdx
================
---
title: Introduction
---

Safety is a top priority for us at Open Interpreter. Running LLM generated code on your computer is inherently risky, and we have taken steps to make it as safe as possible. One of the primary safety 'mechanisms', is the alignment of the LLM itself. GPT-4 refuses to run dangerous code like `rm -rf /`, it understands what that command will do, and won't let you footgun yourself. This is less applicable when running local models like Mistral, that have little or no alignment, making our other safety measures more important.

# Safety Measures

- [Safe mode](/safety/safe-mode) enables code scanning, as well as the ability to scan packages with [guarddog](https://github.com/DataDog/guarddog) with a simple change to the system message. See the [safe mode docs](/safety/safe-mode) for more information.

- Requiring confirmation with the user before the code is actually run. This is a simple measure that can prevent a lot of accidents. It exists as another layer of protection, but can be disabled with the `--auto-run` flag if you wish.

- Sandboxing code execution. Open Interpreter can be run in a sandboxed environment using [Docker](/integrations/docker). This is a great way to run code without worrying about it affecting your system. Docker support is currently experimental, but we are working on making it a core feature of Open Interpreter. Another option for sandboxing is [E2B](https://e2b.dev/), which overrides the default python language with a sandboxed, hosted version of python through E2B. Follow [this guide](/integrations/e2b) to set it up.

## Notice

<Warning>
  Open Interpreter is not responsible for any damage caused by using the
  package. These safety measures provide no guarantees of safety or security.
  Please be careful when running code generated by Open Interpreter, and make
  sure you understand what it will do before running it.
</Warning>

================
File: safety/isolation.mdx
================
---
title: Isolation
---

Isolating Open Interpreter from your system is helpful to prevent security mishaps. By running it in a separate process, you can ensure that actions taken by Open Interpreter will not directly affect your system. This is by far the safest way to run Open Interpreter, although it can be limiting based on your use case.

If you wish to sandbox Open Interpreter, we have two primary methods of doing so: Docker and E2B.

## Docker

Docker is a containerization technology that allows you to run an isolated Linux environment on your system. This allows you to run Open Interpreter in a container, which **completely** isolates it from your system. All code execution is done in the container, and the container is not able to access your system. Docker support is currently experimental, and we are working on integrating it as a core feature of Open Interpreter.

Follow [these instructions](/integrations/docker) to get it running.

## E2B

[E2B](https://e2b.dev/) is a cloud-based platform for running sandboxed code environments, designed for use by AI agents. You can override the default `python` language in Open Interpreter to use E2B, and it will automatically run the code in a cloud-sandboxed environment. You will need an E2B account to use this feature. It's worth noting that this will only sandbox python code, other languages like shell and JavaScript will still be run on your system.

Follow [these instructions](/integrations/e2b) to get it running.

================
File: safety/safe-mode.mdx
================
---
title: Safe Mode
---

# Safe Mode

** Safe mode is experimental and does not provide any guarantees of safety or security.**

Open Interpreter is working on providing an experimental safety toolkit to help you feel more confident running the code generated by Open Interpreter.

Install Open Interpreter with the safety toolkit dependencies as part of the bundle:

```shell
pip install open-interpreter[safe]
```

Alternatively, you can install the safety toolkit dependencies separately in your virtual environment:

```shell
pip install semgrep
```

## Features

- **No Auto Run**: Safe mode disables the ability to automatically execute code
- **Code Scanning**: Scan generated code for vulnerabilities with [`semgrep`](https://semgrep.dev/)

## Enabling Safe Mode

You can enable safe mode by passing the `--safe` flag when invoking `interpreter` or by configuring `safe_mode` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration).

The safe mode setting has three options:

- `off`: disables the safety toolkit (_default_)
- `ask`: prompts you to confirm that you want to scan code
- `auto`: automatically scans code

### Example Config:

```yaml
model: gpt-4
temperature: 0
verbose: false
safe_mode: ask
```

## Roadmap

Some upcoming features that enable even more safety:

- [Execute code in containers](https://github.com/OpenInterpreter/open-interpreter/pull/459)

## Tips & Tricks

You can adjust the `custom_instructions` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration) to include instructions for the model to scan packages with [guarddog](https://github.com/DataDog/guarddog) before installing them.

```yaml
model: gpt-4
verbose: false
safe_mode: ask
system_message: |
  # normal system message here
  BEFORE INSTALLING ANY PACKAGES WITH pip OR npm YOU MUST SCAN THEM WITH `guarddog` FIRST. Run `guarddog pypi scan $package` for pip packages and `guarddog npm scan $package` for npm packages. `guarddog` only accepts one package name at a time.
```

================
File: server/usage.mdx
================
# Server Usage Guide

## Starting the Server

### From Command Line
To start the server from the command line, use:

```bash
interpreter --server
```

### From Python
To start the server from within a Python script:

```python
from interpreter import AsyncInterpreter

async_interpreter = AsyncInterpreter()
async_interpreter.server.run(port=8000)  # Default port is 8000, but you can customize it
```

## WebSocket API

### Establishing a Connection
Connect to the WebSocket server at `ws://localhost:8000/`.

### Message Format
Open Interpreter uses an extended version of OpenAI's message format called [LMC messages](https://docs.openinterpreter.com/protocols/lmc-messages) that allow for rich, multi-part messages. **Messages must be sent between start and end flags.** Here's the basic structure:

```json
{"role": "user", "start": true}
{"role": "user", "type": "message", "content": "Your message here"}
{"role": "user", "end": true}
```

### Multi-part Messages
You can send complex messages with multiple components:

1. Start with `{"role": "user", "start": true}`
2. Add various types of content (message, file, image, etc.)
3. End with `{"role": "user", "end": true}`

### Content Types
You can include various types of content in your messages:

- Text messages: `{"role": "user", "type": "message", "content": "Your text here"}`
- File paths: `{"role": "user", "type": "file", "content": "path/to/file"}`
- Images: `{"role": "user", "type": "image", "format": "path", "content": "path/to/photo"}`
- Audio: `{"role": "user", "type": "audio", "format": "wav", "content": "path/to/audio.wav"}`

### Control Commands
To control the server's behavior, send the following commands:

1. Stop execution:
   ```json
   {"role": "user", "type": "command", "content": "stop"}
   ```
   This stops all execution and message processing.

2. Execute code block:
   ```json
   {"role": "user", "type": "command", "content": "go"}
   ```
   This executes a generated code block and allows the agent to proceed.

   **Note**: If `auto_run` is set to `False`, the agent will pause after generating code blocks. You must send the "go" command to continue execution.

### Completion Status
The server indicates completion with the following message:
```json
{"role": "server", "type": "status", "content": "complete"}
```
Ensure your client watches for this message to determine when the interaction is finished.

### Error Handling
If an error occurs, the server will send an error message in the following format:
```json
{"role": "server", "type": "error", "content": "Error traceback information"}
```
Your client should be prepared to handle these error messages appropriately.

## Code Execution Review

After code blocks are executed, you'll receive a review message:

```json
{
  "role": "assistant",
  "type": "review",
  "content": "Review of the executed code, including safety assessment and potential irreversible actions."
}
```

This review provides important information about the safety and potential impact of the executed code. Pay close attention to these messages, especially when dealing with operations that might have significant effects on your system.

The `content` field of the review message may have two possible formats:

1. If the code is deemed completely safe, the content will be exactly `"<SAFE>"`.
2. Otherwise, it will contain an explanation of why the code might be unsafe or have irreversible effects.

Example of a safe code review:
```json
{
  "role": "assistant",
  "type": "review",
  "content": "<SAFE>"
}
```

Example of a potentially unsafe code review:
```json
{
  "role": "assistant",
  "type": "review",
  "content": "This code performs file deletion operations which are irreversible. Please review carefully before proceeding."
}
```

## Example WebSocket Interaction

Here's an example demonstrating the WebSocket interaction:

```python
import websockets
import json
import asyncio

async def websocket_interaction():
    async with websockets.connect("ws://localhost:8000/") as websocket:
        # Send a multi-part user message
        await websocket.send(json.dumps({"role": "user", "start": True}))
        await websocket.send(json.dumps({"role": "user", "type": "message", "content": "Analyze this image:"}))
        await websocket.send(json.dumps({"role": "user", "type": "image", "format": "path", "content": "path/to/image.jpg"}))
        await websocket.send(json.dumps({"role": "user", "end": True}))

        # Receive and process messages
        while True:
            message = await websocket.recv()
            data = json.loads(message)
            
            if data.get("type") == "message":
                print(f"Assistant: {data.get('content', '')}")
            elif data.get("type") == "review":
                print(f"Code Review: {data.get('content')}")
            elif data.get("type") == "error":
                print(f"Error: {data.get('content')}")
            elif data == {"role": "assistant", "type": "status", "content": "complete"}:
                print("Interaction complete")
                break

asyncio.run(websocket_interaction())
```

## HTTP API

### Modifying Settings
To change server settings, send a POST request to `http://localhost:8000/settings`. The payload should conform to [the interpreter object's settings](https://docs.openinterpreter.com/settings/all-settings).

Example:
```python
import requests

settings = {
    "llm": {"model": "gpt-4"},
    "custom_instructions": "You only write Python code.",
    "auto_run": True,
}
response = requests.post("http://localhost:8000/settings", json=settings)
print(response.status_code)
```

### Retrieving Settings
To get current settings, send a GET request to `http://localhost:8000/settings/{property}`.

Example:
```python
response = requests.get("http://localhost:8000/settings/custom_instructions")
print(response.json())
# Output: {"custom_instructions": "You only write Python code."}
```

## OpenAI-Compatible Endpoint

The server provides an OpenAI-compatible endpoint at `/openai`. This allows you to use the server with any tool or library that's designed to work with the OpenAI API.

### Chat Completions Endpoint

The chat completions endpoint is available at:

```
[server_url]/openai/chat/completions
```

To use this endpoint, set the `api_base` in your OpenAI client or configuration to `[server_url]/openai`. For example:

```python
import openai

openai.api_base = "http://localhost:8000/openai"  # Replace with your server URL if different
openai.api_key = "dummy"  # The key is not used but required by the OpenAI library

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",  # This model name is ignored, but required
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the capital of France?"}
    ]
)

print(response.choices[0].message['content'])
```

Note that only the chat completions endpoint (`/chat/completions`) is implemented. Other OpenAI API endpoints are not available.

When using this endpoint:
- The `model` parameter is required but ignored.
- The `api_key` is required by the OpenAI library but not used by the server.

## Using Docker

You can also run the server using Docker. First, build the Docker image from the root of the repository:

```bash
docker build -t open-interpreter .
```

Then, run the container:

```bash
docker run -p 8000:8000 open-interpreter
```

This will expose the server on port 8000 of your host machine.

## Acknowledgment Feature

When the `INTERPRETER_REQUIRE_ACKNOWLEDGE` environment variable is set to `"True"`, the server requires clients to acknowledge each message received. This feature ensures reliable message delivery in environments where network stability might be a concern.

### How it works

1. When this feature is enabled, each message sent by the server will include an `id` field.
2. The client must send an acknowledgment message back to the server for each received message.
3. The server will wait for this acknowledgment before sending the next message.

### Client Implementation

To implement this on the client side:

1. Check if each received message contains an `id` field.
2. If an `id` is present, send an acknowledgment message back to the server.

Here's an example of how to handle this in your WebSocket client:

```python
import json
import websockets

async def handle_messages(websocket):
    async for message in websocket:
        data = json.loads(message)
        
        # Process the message as usual
        print(f"Received: {data}")

        # Check if the message has an ID that needs to be acknowledged
        if "id" in data:
            ack_message = {
                "ack": data["id"]
            }
            await websocket.send(json.dumps(ack_message))
            print(f"Sent acknowledgment for message {data['id']}")

async def main():
    uri = "ws://localhost:8000"
    async with websockets.connect(uri) as websocket:
        await handle_messages(websocket)

# Run the async function
import asyncio
asyncio.run(main())
```

### Server Behavior

- If the server doesn't receive an acknowledgment within a certain timeframe, it will attempt to resend the message.
- The server will make multiple attempts to send a message before considering it failed.

### Enabling the Feature

To enable this feature, set the `INTERPRETER_REQUIRE_ACKNOWLEDGE` environment variable to `"True"` before starting the server:

```bash
export INTERPRETER_REQUIRE_ACKNOWLEDGE="True"
interpreter --server
```

Or in Python:

```python
import os
os.environ["INTERPRETER_REQUIRE_ACKNOWLEDGE"] = "True"

from interpreter import AsyncInterpreter
async_interpreter = AsyncInterpreter()
async_interpreter.server.run()
```

## Advanced Usage: Accessing the FastAPI App Directly

The FastAPI app is exposed at `async_interpreter.server.app`. This allows you to add custom routes or host the app using Uvicorn directly.

Example of adding a custom route and hosting with Uvicorn:

```python
from interpreter import AsyncInterpreter
from fastapi import FastAPI
import uvicorn

async_interpreter = AsyncInterpreter()
app = async_interpreter.server.app

@app.get("/custom")
async def custom_route():
    return {"message": "This is a custom route"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## Best Practices

1. Always handle the "complete" status message to ensure your client knows when the server has finished processing.
2. If `auto_run` is set to `False`, remember to send the "go" command to execute code blocks and continue the interaction.
3. Implement proper error handling in your client to manage potential connection issues, unexpected server responses, or server-sent error messages.
4. Use the AsyncInterpreter class when working with the server in Python to ensure compatibility with asynchronous operations.
5. Pay attention to the code execution review messages for important safety and operational information.
6. Utilize the multi-part user message structure for complex inputs, including file paths and images.
7. When sending file paths or image paths, ensure they are accessible to the server.

================
File: settings/all-settings.mdx
================
---
title: All Settings
---

<CardGroup cols={3}>

<Card title="Language Model Settings" icon="microchip" href="#language-model">
  Set your `model`, `api_key`, `temperature`, etc.
</Card>

<Card
  title="Interpreter Settings"
  icon="circle"
  iconType="solid"
  href="#interpreter"
>
  Change your `system_message`, set your interpreter to run `offline`, etc.
</Card>
<Card
  title="Code Execution Settings"
  icon="code"
  iconType="solid"
  href="#computer"
>
  Modify the `interpreter.computer`, which handles code execution.
</Card>

</CardGroup>

# Language Model

### Model Selection

Specifies which language model to use. Check out the [models](/language-models/) section for a list of available models. Open Interpreter uses [LiteLLM](https://github.com/BerriAI/litellm) under the hood to support over 100+ models.

<CodeGroup>

```bash Terminal
interpreter --model "gpt-3.5-turbo"
```

```python Python
interpreter.llm.model = "gpt-3.5-turbo"
```

```yaml Profile
llm:
  model: gpt-3.5-turbo
```

</CodeGroup>

### Temperature

Sets the randomness level of the model's output. The default temperature is 0, you can set it to any value between 0 and 1. The higher the temperature, the more random and creative the output will be.

<CodeGroup>

```bash Terminal
interpreter --temperature 0.7
```

```python Python
interpreter.llm.temperature = 0.7
```

```yaml Profile
llm:
  temperature: 0.7
```

</CodeGroup>

### Context Window

Manually set the context window size in tokens for the model. For local models, using a smaller context window will use less RAM, which is more suitable for most devices.

<CodeGroup>

```bash Terminal
interpreter --context_window 16000
```

```python Python
interpreter.llm.context_window = 16000
```

```yaml Profile
llm:
  context_window: 16000
```

</CodeGroup>

### Max Tokens

Sets the maximum number of tokens that the model can generate in a single response.

<CodeGroup>

```bash Terminal
interpreter --max_tokens 100
```

```python Python
interpreter.llm.max_tokens = 100
```

```yaml Profile
llm:
  max_tokens: 100
```

</CodeGroup>

### Max Output

Set the maximum number of characters for code outputs.

<CodeGroup>

```bash Terminal
interpreter --max_output 1000
```

```python Python
interpreter.llm.max_output = 1000
```

```yaml Profile
llm:
  max_output: 1000
```

</CodeGroup>

### API Base

If you are using a custom API, specify its base URL with this argument.

<CodeGroup>

```bash Terminal
interpreter --api_base "https://api.example.com"
```

```python Python
interpreter.llm.api_base = "https://api.example.com"
```

```yaml Profile
llm:
  api_base: https://api.example.com
```

</CodeGroup>

### API Key

Set your API key for authentication when making API calls. For OpenAI models, you can get your API key [here](https://platform.openai.com/api-keys).

<CodeGroup>

```bash Terminal
interpreter --api_key "your_api_key_here"
```

```python Python
interpreter.llm.api_key = "your_api_key_here"
```

```yaml Profile
llm:
  api_key: your_api_key_here
```

</CodeGroup>

### API Version

Optionally set the API version to use with your selected model. (This will override environment variables)

<CodeGroup>

```bash Terminal
interpreter --api_version 2.0.2
```

```python Python
interpreter.llm.api_version = '2.0.2'
```

```yaml Profile
llm:
  api_version: 2.0.2
```

</CodeGroup>

### LLM Supports Functions

Inform Open Interpreter that the language model you're using supports function calling.

<CodeGroup>

```bash Terminal
interpreter --llm_supports_functions
```

```python Python
interpreter.llm.supports_functions = True
```

```yaml Profile
llm:
  supports_functions: true
```

</CodeGroup>

### LLM Does Not Support Functions

Inform Open Interpreter that the language model you're using does not support function calling.

<CodeGroup>

```bash Terminal
interpreter --no-llm_supports_functions
```

```python Python
interpreter.llm.supports_functions = False
```

```yaml Profile
llm:
  supports_functions: false
```

</CodeGroup>

### Execution Instructions

If `llm.supports_functions` is `False`, this value will be added to the system message. This parameter tells language models how to execute code. This can be set to an empty string or to `False` if you don't want to tell the LLM how to do this.

<CodeGroup>

````python Python
interpreter.llm.execution_instructions = "To execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language."
````

````python Profile
interpreter.llm.execution_instructions = "To execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language."
````

</CodeGroup>

### LLM Supports Vision

Inform Open Interpreter that the language model you're using supports vision. Defaults to `False`.

<CodeGroup>

```bash Terminal
interpreter --llm_supports_vision
```

```python Python
interpreter.llm.supports_vision = True
```

```yaml Profile
llm:
  supports_vision: true
```

</CodeGroup>

# Interpreter

### Vision Mode

Enables vision mode, which adds some special instructions to the prompt and switches to `gpt-4o`.

<CodeGroup>
```bash Terminal
interpreter --vision
```

```python Python
interpreter.llm.model = "gpt-4o" # Any vision supporting model
interpreter.llm.supports_vision = True
interpreter.llm.supports_functions = True

interpreter.custom_instructions = """The user will show you an image of the code you write. You can view images directly.
For HTML: This will be run STATELESSLY. You may NEVER write '<!-- previous code here... --!>' or `<!-- header will go here -->` or anything like that. It is CRITICAL TO NEVER WRITE PLACEHOLDERS. Placeholders will BREAK it. You must write the FULL HTML CODE EVERY TIME. Therefore you cannot write HTML piecemealwrite all the HTML, CSS, and possibly Javascript **in one step, in one code block**. The user will help you review it visually.
If the user submits a filepath, you will also see the image. The filepath and user image will both be in the user's message.
If you use `plt.show()`, the resulting image will be sent to you. However, if you use `PIL.Image.show()`, the resulting image will NOT be sent to you."""
```

```yaml Profile
loop: True

llm:
  model: "gpt-4o"
  temperature: 0
  supports_vision: True
  supports_functions: True
  context_window: 110000
  max_tokens: 4096
  custom_instructions: >
    The user will show you an image of the code you write. You can view images directly.
    For HTML: This will be run STATELESSLY. You may NEVER write '<!-- previous code here... --!>' or `<!-- header will go here -->` or anything like that. It is CRITICAL TO NEVER WRITE PLACEHOLDERS. Placeholders will BREAK it. You must write the FULL HTML CODE EVERY TIME. Therefore you cannot write HTML piecemealwrite all the HTML, CSS, and possibly Javascript **in one step, in one code block**. The user will help you review it visually.
    If the user submits a filepath, you will also see the image. The filepath and user image will both be in the user's message.
    If you use `plt.show()`, the resulting image will be sent to you. However, if you use `PIL.Image.show()`, the resulting image will NOT be sent to you.
```

</CodeGroup>

### OS Mode

Enables OS mode for multimodal models. Currently not available in Python. Check out more information on OS mode [here](/guides/os-mode).

<CodeGroup>

```bash Terminal
interpreter --os
```

```yaml Profile
os: true
```

</CodeGroup>

### Version

Get the current installed version number of Open Interpreter.

<CodeGroup>

```bash Terminal
interpreter --version
```

</CodeGroup>

### Open Local Models Directory

Opens the models directory. All downloaded Llamafiles are saved here.

<CodeGroup>

```bash Terminal
interpreter --local_models
```

</CodeGroup>

### Open Profiles Directory

Opens the profiles directory. New yaml profile files can be added to this directory.

<CodeGroup>

```bash Terminal
interpreter --profiles
```

</CodeGroup>

### Select Profile

Select a profile to use. If no profile is specified, the default profile will be used.

<CodeGroup>

```bash Terminal
interpreter --profile local.yaml
```

</CodeGroup>

### Help

Display all available terminal arguments.

<CodeGroup>

```bash Terminal
interpreter --help
```

</CodeGroup>

### Loop (Force Task Completion)

Runs Open Interpreter in a loop, requiring it to admit to completing or failing every task.

<CodeGroup>

```bash Terminal
interpreter --loop
```

```python Python
interpreter.loop = True
```

```yaml Profile
loop: true
```

</CodeGroup>

### Verbose

Run the interpreter in verbose mode. Debug information will be printed at each step to help diagnose issues.

<CodeGroup>

```bash Terminal
interpreter --verbose
```

```python Python
interpreter.verbose = True
```

```yaml Profile
verbose: true
```

</CodeGroup>

### Safe Mode

Enable or disable experimental safety mechanisms like code scanning. Valid options are `off`, `ask`, and `auto`.

<CodeGroup>

```bash Terminal
interpreter --safe_mode ask
```

```python Python
interpreter.safe_mode = 'ask'
```

```yaml Profile
safe_mode: ask
```

</CodeGroup>

### Auto Run

Automatically run the interpreter without requiring user confirmation.

<CodeGroup>

```bash Terminal
interpreter --auto_run
```

```python Python
interpreter.auto_run = True
```

```yaml Profile
auto_run: true
```

</CodeGroup>

### Max Budget

Sets the maximum budget limit for the session in USD.

<CodeGroup>

```bash Terminal
interpreter --max_budget 0.01
```

```python Python
interpreter.max_budget = 0.01
```

```yaml Profile
max_budget: 0.01
```

</CodeGroup>

### Local Mode

Run the model locally. Check the [models page](/language-models/local-models/lm-studio) for more information.

<CodeGroup>

```bash Terminal
interpreter --local
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "openai/x" # Tells OI to send messages in OpenAI's format
interpreter.llm.api_key = "fake_key" # LiteLLM, which we use to talk to local models, requires this
interpreter.llm.api_base = "http://localhost:1234/v1" # Point this at any OpenAI compatible server

interpreter.chat()
```

```yaml Profile
local: true
```

</CodeGroup>

### Fast Mode

Sets the model to gpt-3.5-turbo and encourages it to only write code without confirmation.

<CodeGroup>

```bash Terminal
interpreter --fast
```

```yaml Profile
fast: true
```

</CodeGroup>

### Custom Instructions

Appends custom instructions to the system message. This is useful for adding information about your system, preferred languages, etc.

<CodeGroup>

```bash Terminal
interpreter --custom_instructions "This is a custom instruction."
```

```python Python
interpreter.custom_instructions = "This is a custom instruction."
```

```yaml Profile
custom_instructions: "This is a custom instruction."
```

</CodeGroup>

### System Message

We don't recommend modifying the system message, as doing so opts you out of future updates to the core system message. Use `--custom_instructions` instead, to add relevant information to the system message. If you must modify the system message, you can do so by using this argument, or by changing a profile file.

<CodeGroup>

```bash Terminal
interpreter --system_message "You are Open Interpreter..."
```

```python Python
interpreter.system_message = "You are Open Interpreter..."
```

```yaml Profile
system_message: "You are Open Interpreter..."
```

</CodeGroup>

### Disable Telemetry

Opt out of [telemetry](telemetry/telemetry).

<CodeGroup>

```bash Terminal
interpreter --disable_telemetry
```

```python Python
interpreter.anonymized_telemetry = False
```

```yaml Profile
disable_telemetry: true
```

</CodeGroup>

### Offline

This boolean flag determines whether to enable or disable some offline features like [open procedures](https://open-procedures.replit.app/). Use this in conjunction with the `model` parameter to set your language model.

<CodeGroup>

```python Python
interpreter.offline = True
```

```bash Terminal
interpreter --offline true
```

```yaml Profile
offline: true
```

</CodeGroup>

### Messages

This property holds a list of `messages` between the user and the interpreter.

You can use it to restore a conversation:

```python
interpreter.chat("Hi! Can you print hello world?")

print(interpreter.messages)

# This would output:

# [
#    {
#       "role": "user",
#       "message": "Hi! Can you print hello world?"
#    },
#    {
#       "role": "assistant",
#       "message": "Sure!"
#    }
#    {
#       "role": "assistant",
#       "language": "python",
#       "code": "print('Hello, World!')",
#       "output": "Hello, World!"
#    }
# ]

#You can use this to restore `interpreter` to a previous conversation.
interpreter.messages = messages # A list that resembles the one above
```

### User Message Template

A template applied to the User's message. `{content}` will be replaced with the user's message, then sent to the language model.

<CodeGroup>

````python Python
interpreter.user_message_template = "{content} Please send me some code that would be able to answer my question, in the form of ```python\n... the code ...\n``` or ```shell\n... the code ...\n```"
````

```python Profile
interpreter.user_message_template = "{content}. Be concise, don't include anything unnecessary. Don't use placeholders, I can't edit code."
```

</CodeGroup>

### Always Apply User Message Template

The boolean flag for whether the User Message Template will be applied to every user message. The default is False which means the template is only applied to the last User message.

<CodeGroup>

```python Python
interpreter.always_apply_user_message_template = False
```

```python Profile
interpreter.always_apply_user_message_template = False
```

</CodeGroup>

### Code Message Template

A template applied to the Computer's output after running code. `{content}` will be replaced with the computer's output, then sent to the language model.

<CodeGroup>

```python Python
interpreter.code_output_template = "Code output: {content}\nWhat does this output mean / what's next (if anything, or are we done)?"
```

```python Profile
interpreter.code_output_template = "Code output: {content}\nWhat code needs to be run next?"
```

</CodeGroup>

### Empty Code Message Template

If the computer does not output anything after code execution, this value will be sent to the language model.

<CodeGroup>

```python Python
interpreter.empty_code_output_template = "The code above was executed on my machine. It produced no text output. what's next (if anything, or are we done?)"
```

```python Profile
interpreter.empty_code_output_template = "The code above was executed on my machine. It produced no text output. what's next?"
```

</CodeGroup>

### Code Output Sender

This field determines whether the computer / code output messages are sent as the assistant or as the user. The default is user.

<CodeGroup>

```python Python
interpreter.code_output_sender = "user"
```

```python Profile
interpreter.code_output_sender = "assistant"
```

</CodeGroup>

# Computer

The `computer` object in `interpreter.computer` is a virtual computer that the AI controls. Its primary interface/function is to execute code and return the output in real-time.

### Offline

Running the `computer` in offline mode will disable some online features, like the hosted [Computer API](https://api.openinterpreter.com/). Inherits from `interpreter.offline`.

<CodeGroup>

```python Python
interpreter.computer.offline = True
```

```yaml Profile
computer.offline: True
```

</CodeGroup>

### Verbose

This is primarily used for debugging `interpreter.computer`. Inherits from `interpreter.verbose`.

<CodeGroup>

```python Python
interpreter.computer.verbose = True
```

```yaml Profile
computer.verbose: True
```

</CodeGroup>

### Emit Images

The `emit_images` attribute in `interpreter.computer` controls whether the computer should emit images or not. This is inherited from `interpreter.llm.supports_vision`.

This is used for multimodel vs. text only models. Running `computer.display.view()` will return an actual screenshot for multimodal models if `emit_images` is True. If it's False, `computer.display.view()` will return all the text on the screen.

Many other functions of the computer can produce image/text outputs, and this parameter controls that.

<CodeGroup>

```python Python
interpreter.computer.emit_images = True
```

```yaml Profile
computer.emit_images: True
```

</CodeGroup>

### Import Computer API

Include the computer API in the system message. The default is False and won't import the computer API automatically

<CodeGroup>

```python Python
interpreter.computer.import_computer_api = True
```

```yaml Profile
computer.import_computer_api: True
```

</CodeGroup>
````

================
File: settings/example-profiles.mdx
================
---
title: Example Profiles
---

### OS Mode

```yaml
os: True
custom_instructions: "Always use Safari as the browser, and use Raycast instead of spotlight search by pressing option + space."
```

================
File: settings/profiles.mdx
================
---
title: Profiles
---

Profiles are preconfigured settings for Open Interpreter that make it easy to get going quickly with a specific set of settings. Any [setting](/settings/all-settings) can be configured in a profile. Custom instructions are helpful to have in each profile, to customize the behavior of Open Interpreter for the specific use case that the profile is designed for.

To load a profile, run:

```bash
interpreter --profile <profile_name>.yaml

```

All profiles are stored in their own folder, which can be accessed by running:

```bash
interpreter --profile

```

To create your own profile, you can add a `.yaml` file to this folder and add whatever [settings](/settings/all-settings) you'd like:

```yaml
custom_instructions: "Always use python, and be as concise as possible"
llm.model: gpt-4
llm.temperature: 0.5
# Any other settings you'd like to add
```

Any profile named 'default.yaml' will be loaded by default.

Profiles can be shared with others by sending them the profile yaml file!

================
File: telemetry/telemetry.mdx
================
---
title: Introduction
---

Open Interpreter contains a telemetry feature that collects **anonymous** usage information.

We use this information to help us understand how OI is used, to help us prioritize work on new features and bug fixes, and to help us improve OI's performance and stability.

# Opting out

If you prefer to opt out of telemetry, you can do this in two ways.

### Python

Set `disable_telemetry` to `true` on the `interpreter` object:

```python
from interpreter import interpreter
interpreter.disable_telemetry = True
```

### Terminal

Use the `--disable_telemetry` flag:

```shell
interpreter --disable_telemetry
```

### Profile

Set `disable_telemetry` to `true`. This will persist to future terminal sessions:

```yaml
disable_telemetry: true
```

### Environment Variables

Set `DISABLE_TELEMETRY` to `true` in your shell or server environment.

If you are running Open Interpreter on your local computer with `docker-compose` you can set this value in an `.env` file placed in the same directory as the `docker-compose.yml` file:

```
DISABLE_TELEMETRY=true
```

# What do you track?

We will only track usage details that help us make product decisions, specifically:

- Open Interpreter version and environment (i.e whether or not it's running in Python / a terminal)
- When interpreter.chat is run, in what mode (e.g `--os` mode), and the type of the message being passed in (e.g `None`, `str`, or `list`)
- Exceptions that occur within Open Interpreter (not tracebacks)

We **do not** collect personally-identifiable or sensitive information, such as: usernames, hostnames, file names, environment variables, or hostnames of systems being tested.

To view the list of events we track, you may reference the **[code](https://github.com/OpenInterpreter/open-interpreter/tree/main/interpreter/core)**

## Where is telemetry information stored?

We use **[Posthog](https://posthog.com/)** to store and visualize telemetry data.

<Info>
  Posthog is an open source platform for product analytics. Learn more about
  Posthog on **[posthog.com](https://posthog.com/)** or
  **[github.com/posthog](https://github.com/posthog/posthog)**
</Info>

================
File: troubleshooting/faq.mdx
================
---
title: "FAQ"
description: "Frequently Asked Questions"
---

<Accordion title="Does Open Interpreter ensure that my data doesn't leave my computer?">
  As long as you're using a local language model, your messages / personal info
  won't leave your computer. If you use a cloud model, we send your messages +
  custom instructions to the model. We also have a basic telemetry
  [function](https://github.com/OpenInterpreter/open-interpreter/blob/main/interpreter/core/core.py#L167)
  (copied over from ChromaDB's telemetry) that anonymously tracks usage. This
  only lets us know if a message was sent, includes no PII. OI errors will also
  be reported here which includes the exception string. Detailed docs on all
  this is [here](/telemetry/telemetry), and you can opt out by running
  `--local`, `--offline`, or `--disable_telemetry`.
</Accordion>

================
File: usage/desktop/help.md
================
Reach out to help@openinterpreter.com for support.

================
File: usage/desktop/install.mdx
================
---
title: Desktop App
---

Our desktop application is currently in development and is not yet available to the public.

You can apply for early access [here](https://0ggfznkwh4j.typeform.com/to/G21i9lJ2?typeform-source=docs.openinterpreter.com).

================
File: usage/python/arguments.mdx
================
---
title: Arguments
---

<Card
  title="New: Streaming responses in Python"
  icon="arrow-up-right"
  href="/usage/python/streaming-response"
>
  Learn how to build Open Interpreter into your application.
</Card>

#### `messages`

This property holds a list of `messages` between the user and the interpreter.

You can use it to restore a conversation:

```python
interpreter.chat("Hi! Can you print hello world?")

print(interpreter.messages)

# This would output:

[
   {
      "role": "user",
      "message": "Hi! Can you print hello world?"
   },
   {
      "role": "assistant",
      "message": "Sure!"
   }
   {
      "role": "assistant",
      "language": "python",
      "code": "print('Hello, World!')",
      "output": "Hello, World!"
   }
]
```

You can use this to restore `interpreter` to a previous conversation.

```python
interpreter.messages = messages # A list that resembles the one above
```

---

#### `offline`

<Info>This replaced `interpreter.local` in the New Computer Update (`0.2.0`).</Info>

This boolean flag determines whether to enable or disable some offline features like [open procedures](https://open-procedures.replit.app/).

```python
interpreter.offline = True  # Check for updates, use procedures
interpreter.offline = False  # Don't check for updates, don't use procedures
```

Use this in conjunction with the `model` parameter to set your language model.

---

#### `auto_run`

Setting this flag to `True` allows Open Interpreter to automatically run the generated code without user confirmation.

```python
interpreter.auto_run = True  # Don't require user confirmation
interpreter.auto_run = False  # Require user confirmation (default)
```

---

#### `verbose`

Use this boolean flag to toggle verbose mode on or off. Verbose mode will print information at every step to help diagnose problems.

```python
interpreter.verbose = True  # Turns on verbose mode
interpreter.verbose = False  # Turns off verbose mode
```

---

#### `max_output`

This property sets the maximum number of tokens for the output response.

```python
interpreter.max_output = 2000
```

---

#### `conversation_history`

A boolean flag to indicate if the conversation history should be stored or not.

```python
interpreter.conversation_history = True  # To store history
interpreter.conversation_history = False  # To not store history
```

---

#### `conversation_filename`

This property sets the filename where the conversation history will be stored.

```python
interpreter.conversation_filename = "my_conversation.json"
```

---

#### `conversation_history_path`

You can set the path where the conversation history will be stored.

```python
import os
interpreter.conversation_history_path = os.path.join("my_folder", "conversations")
```

---

#### `model`

Specifies the language model to be used.

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

---

#### `temperature`

Sets the randomness level of the model's output.

```python
interpreter.llm.temperature = 0.7
```

---

#### `system_message`

This stores the model's system message as a string. Explore or modify it:

```python
interpreter.system_message += "\nRun all shell commands with -y."
```

---

#### `context_window`

This manually sets the context window size in tokens.

We try to guess the right context window size for you model, but you can override it with this parameter.

```python
interpreter.llm.context_window = 16000
```

---

#### `max_tokens`

Sets the maximum number of tokens the model can generate in a single response.

```python
interpreter.llm.max_tokens = 100
```

---

#### `api_base`

If you are using a custom API, you can specify its base URL here.

```python
interpreter.llm.api_base = "https://api.example.com"
```

---

#### `api_key`

Set your API key for authentication.

```python
interpreter.llm.api_key = "your_api_key_here"
```

---

#### `max_budget`

This property sets the maximum budget limit for the session in USD.

```python
interpreter.max_budget = 0.01 # 1 cent
```

================
File: usage/python/budget-manager.mdx
================
---
title: Budget Manager
---

The `max_budget` property sets the maximum budget limit for the session in USD.

```python
interpreter.max_budget = 0.01 # 1 cent
```

================
File: usage/python/conversation-history.mdx
================
---
title: Conversation History
---

Conversations will be saved in your application directory. **This is true for python and for the terminal interface.**

The command below, when run in your terminal, will show you which folder they're being saved in (use your arrow keys to move down and press enter over `> Open Folder`):

```shell
interpreter --conversations
```

You can turn off conversation history for a particular conversation:

```python
from interpreter import interpreter

interpreter.conversation_history = False
interpreter.chat() # Conversation history will not be saved
```

================
File: usage/python/magic-commands.mdx
================
---
title: Magic Commands
---

If you run an interactive chat in python, you can use *magic commands* built for terminal usage:

```python
interpreter.chat()
```

The following magic commands will work:

- %verbose [true/false]: Toggle verbose mode. Without arguments or with true it enters verbose mode. With false it exits verbose mode.
- %reset: Resets the current session's conversation.
- %undo: Removes the previous user message and the AI's response from the message history.
- %tokens [prompt]: (Experimental) Calculate the tokens that will be sent with the next prompt as context and estimate their cost. Optionally calculate the tokens and estimated cost of a prompt if one is provided. Relies on LiteLLM's cost_per_token() method for estimated costs.
- %help: Show the help message.

================
File: usage/python/multiple-instances.mdx
================
To create multiple instances, use the base class, `OpenInterpreter`:

```python
from interpreter import OpenInterpreter

agent_1 = OpenInterpreter()
agent_1.system_message = "This is a separate instance."

agent_2 = OpenInterpreter()
agent_2.system_message = "This is yet another instance."
```

For fun, you could make these instances talk to each other:

```python
def swap_roles(messages):
    for message in messages:
        if message['role'] == 'user':
            message['role'] = 'assistant'
        elif message['role'] == 'assistant':
            message['role'] = 'user'
    return messages

agents = [agent_1, agent_2]

# Kick off the conversation
messages = [{"role": "user", "type": "message", "content": "Hello!"}]

while True:
    for agent in agents:
        messages = agent.chat(messages)
        messages = swap_roles(messages)
```

================
File: usage/python/settings.mdx
================
---
title: Settings
---

Default settings will be inherited from a profile in your application directory. **This is true for python and for the terminal interface.**

To open the file, run:

```bash
interpreter --profiles
```

================
File: usage/terminal/arguments.mdx
================
---
title: Arguments
---

**[Modes](/docs/usage/terminal/arguments#modes)**

`--vision`, `--os`.

**[Model Settings](/docs/usage/terminal/arguments#model-settings)**

`--model`, `--fast`, `--local`, `--temperature`, `--context_window`, `--max_tokens`, `--max_output`, `--api_base`, `--api_key`, `--api_version`, `--llm_supports_functions`, `--llm_supports_vision`.

**[Configuration](/docs/usage/terminal/arguments#Configuration)**

`--profiles`, `--profile`, `--custom_instructions`, `--system_message`.

**[Options](/docs/usage/terminal/arguments#options)**

`--safe_mode`, `--auto_run`, `--loop`, `--verbose`, `--max_budget`, `--speak_messages`, `--multi_line`.

**[Other](/docs/usage/terminal/arguments#other)**

`--version`, `--help`.

---

## Modes

#### `--vision` or `-vi`

Enables vision mode for multimodal models. Defaults to GPT-4-turbo.

<CodeGroup>
```bash Terminal
interpreter --vision
```

```yaml Config
vision: true
```

</CodeGroup>

#### `--os` or `-o`

Enables OS mode for multimodal models. Defaults to GPT-4-turbo.

<CodeGroup>
    
    ```bash Terminal
    interpreter --os
    ```

    ```yaml Config
    os: true
    ```

</CodeGroup>

---

## Model Settings

#### `--model` or `-m`

Specifies which language model to use. Check out the [models](https://docs.openinterpreter.com/language-model-setup/introduction) section for a list of available models.

<CodeGroup>
    
```bash Terminal
interpreter --model "gpt-3.5-turbo"
```

```yaml Config
model: gpt-3.5-turbo
```

</CodeGroup>

#### `--fast` or `-f`

Sets the model to gpt-3.5-turbo.

<CodeGroup>
```bash Terminal
interpreter --fast
```

```yaml Config
fast: true
```

</CodeGroup>

#### `--local` or `-l`

Run the model locally. Check the [models page](/language-model-setup/introduction) for more information.

<CodeGroup>

```bash Terminal
interpreter --local
```

```yaml Config
local: true
```

</CodeGroup>

#### `--temperature` or `-t`

Sets the randomness level of the model's output.

<CodeGroup>
    
```bash Terminal
interpreter --temperature 0.7
```

```yaml Config
temperature: 0.7
```

</CodeGroup>

#### `--context_window` or `-c`

Manually set the context window size in tokens for the model.

<CodeGroup>

```bash Terminal
interpreter --context_window 16000
```

```yaml Config
context_window: 16000
```

</CodeGroup>

#### `--max_tokens` or `-x`

Sets the maximum number of tokens that the model can generate in a single response.

<CodeGroup>

```bash Terminal
interpreter --max_tokens 100
```

```yaml Config
max_tokens: 100
```

</CodeGroup>

#### `--max_output` or `-xo`

Set the maximum number of characters for code outputs.

<CodeGroup>
```bash Terminal
interpreter --max_output 1000
```

```yaml Config
max_output: 1000
```

</CodeGroup>
#### `--api_base` or `-ab`

If you are using a custom API, specify its base URL with this argument.

<CodeGroup>

```bash Terminal
interpreter --api_base "https://api.example.com"
```

```yaml Config
api_base: https://api.example.com
```

</CodeGroup>

#### `--api_key` or `-ak`

Set your API key for authentication when making API calls.

<CodeGroup>

```bash Terminal
interpreter --api_key "your_api_key_here"
```

```yaml Config
api_key: your_api_key_here
```

</CodeGroup>

#### `--api_version` or `-av`

Optionally set the API version to use with your selected model. (This will override environment variables)

<CodeGroup>
```bash Terminal
interpreter --api_version 2.0.2
```

```yaml Config
api_version: 2.0.2
```

</CodeGroup>
#### `--llm_supports_functions` or `-lsf`

Inform Open Interpreter that the language model you're using supports function calling.

<CodeGroup>
```bash Terminal
interpreter --llm_supports_functions
```

```yaml Config
llm_supports_functions: true
```

</CodeGroup>
#### `--no-llm_supports_functions`

Inform Open Interpreter that the language model you're using does not support function calling.

<CodeGroup>
  ```bash Terminal interpreter --no-llm_supports_functions ```
</CodeGroup>

#### `--llm_supports_vision` or `-lsv`

Inform Open Interpreter that the language model you're using supports vision.

<CodeGroup>
```bash Terminal
interpreter --llm_supports_vision
```

```yaml Config
llm_supports_vision: true
```

</CodeGroup>

---

## Configuration

#### `--profiles`

Opens the directory containing all profiles. They can be edited in your default editor.

<CodeGroup>
```bash Terminal
interpreter --profilees
```

</CodeGroup>

#### `--profile` or `-p`

Optionally set a profile to use.

<CodeGroup>
```bash Terminal
interpreter --profile "default.yaml"
```

</CodeGroup>

#### `--custom_instructions` or `-ci`

Appends custom instructions to the system message. This is useful for adding information about the your system, preferred languages, etc.

<CodeGroup>
```bash Terminal
interpreter --custom_instructions "This is a custom instruction."
```

```yaml Config
custom_instructions: "This is a custom instruction."
```

</CodeGroup>

#### `--system_message` or `-s`

We don't recommend modifying the system message, as doing so opts you out of future updates to the system message. Use `--custom_instructions` instead, to add relevant information to the system message. If you must modify the system message, you can do so by using this argument, or by opening the profile using `--profiles`.

<CodeGroup>
```bash Terminal
interpreter --system_message "You are Open Interpreter..."
```

```yaml Config
system_message: "You are Open Interpreter..."
```

## Options

#### `--safe_mode`

Enable or disable experimental safety mechanisms like code scanning. Valid options are `off`, `ask`, and `auto`.

<CodeGroup>

```bash Terminal
interpreter --safe_mode ask
```

```yaml Config
safe_mode: ask
```

</CodeGroup>

#### `--auto_run` or `-y`

Automatically run the interpreter without requiring user confirmation.

<CodeGroup>

```bash Terminal
interpreter --auto_run
```

```yaml Config
auto_run: true
```

</CodeGroup>

#### `--loop`

Runs Open Interpreter in a loop, requiring it to admit to completing or failing every task.

<CodeGroup>
```bash Terminal
interpreter --loop
```

```yaml Config
loop: true
```

</CodeGroup>

#### `--verbose` or `-v`

Run the interpreter in verbose mode. Debug information will be printed at each step to help diagnose issues.

<CodeGroup>

```bash Terminal
interpreter --verbose
```

```yaml Config
verbose: true
```

</CodeGroup>

#### `--max_budget` or `-b`

Sets the maximum budget limit for the session in USD.

<CodeGroup>

```bash Terminal
interpreter --max_budget 0.01
```

```yaml Config
max_budget: 0.01
```

</CodeGroup>

#### `--speak_messages` or `-sm`

(Mac Only) Speak messages out loud using the system's text-to-speech engine.

<CodeGroup>
```bash Terminal
interpreter --speak_messages
```

```yaml Config
speak_messages: true
```

</CodeGroup>

#### `--multi_line` or `-ml`

Enable multi-line inputs starting and ending with ` ``` `

<CodeGroup>
```bash Terminal
interpreter --multi_line
```

```yaml Config
multi_line: true
```

</CodeGroup>

---

## Other

#### `--version`

Get the current installed version number of Open Interpreter.

<CodeGroup>```bash Terminal interpreter --version ```</CodeGroup>

#### `--help` or `-h`

Display all available terminal arguments.

<CodeGroup>
```bash Terminal
interpreter --help
```

</CodeGroup>

================
File: usage/terminal/budget-manager.mdx
================
---
title: Budget Manager
---

You can set a maximum budget per session:
```bash
interpreter --max_budget 0.01
```

================
File: usage/terminal/magic-commands.mdx
================
---
title: Magic Commands
---

Magic commands can be used to control the interpreter's behavior in interactive mode:

- `%% [commands]`: Run commands in system shell.
- `%verbose [true/false]`: Toggle verbose mode. Without arguments or with 'true', it enters verbose mode. With 'false', it exits verbose mode.
- `%reset`: Resets the current session's conversation.
- `%undo`: Remove previous messages and its response from the message history.
- `%save_message [path]`: Saves messages to a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%load_message [path]`: Loads messages from a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%tokens [prompt]`: EXPERIMENTAL: Calculate the tokens used by the next request based on the current conversation's messages and estimate the cost of that request; optionally provide a prompt to also calculate the tokens used by that prompt and the total amount of tokens that will be sent with the next request.
- `%info`: Show system and interpreter information.
- `%help`: Show this help message.
- `%markdown [path]`: Export the conversation to a specified Markdown path. If no path is provided, it will be saved to the Downloads folder with a generated conversation name.

================
File: usage/terminal/settings.mdx
================
---
title: Settings
---

Default settings can be edited via a profile. To open the file, run:

```bash
interpreter --profiles
```

| Key                      | Value                                                    |
| ------------------------ | -------------------------------------------------------- |
| `llm_model`              | String ["openai/gpt-4", "openai/local", "azure/gpt-3.5"] |
| `llm_temperature`        | Float [0.0 -> 1.0]                                       |
| `llm_supports_vision`    | Boolean [True/False]                                     |
| `llm_supports_functions` | Boolean [True/False]                                     |
| `llm_context_window`     | Integer [3000]                                           |
| `llm_max_tokens`         | Integer [3000]                                           |
| `llm_api_base`           | String ["http://ip_address:port", "https://openai.com"]  |
| `llm_api_key`            | String ["sk-Your-Key"]                                   |
| `llm_api_version`        | String ["version-number"]                                |
| `llm_max_budget`         | Float [0.01] #USD $0.01                                  |
| `offline`                | Boolean [True/False]                                     |
| `vision`                 | Boolean [True/False]                                     |
| `auto_run`               | Boolean [True/False]                                     |
| `verbose`                | Boolean [True/False]                                     |

================
File: usage/terminal/vision.mdx
================
---
title: Vision
---

To use vision (highly experimental), run the following command:

```bash
interpreter --vision
```

If a file path to an image is found in your input, it will be loaded into the vision model (`gpt-4o` for now).

================
File: usage/examples.mdx
================
---
title: Examples
description: Get started by copying these code snippets into your terminal, a `.py` file, or a Jupyter notebook.
---

<CardGroup>

<Card
  title="Interactive demo"
  icon="gamepad-modern"
  iconType="solid"
  href="https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing"
>
  Try Open Interpreter without installing anything on your computer
</Card>

<Card
  title="Example voice interface"
  icon="circle"
  iconType="solid"
  href="https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK"
>
  An example implementation of Open Interpreter's streaming capabilities
</Card>

</CardGroup>

---

### Interactive Chat

To start an interactive chat in your terminal, either run `interpreter` from the command line:

```shell
interpreter
```

Or `interpreter.chat()` from a .py file:

```python
interpreter.chat()
```

---

### Programmatic Chat

For more precise control, you can pass messages directly to `.chat(message)` in Python:

```python
interpreter.chat("Add subtitles to all videos in /videos.")

# ... Displays output in your terminal, completes task ...

interpreter.chat("These look great but can you make the subtitles bigger?")

# ...
```

---

### Start a New Chat

In your terminal, Open Interpreter behaves like ChatGPT and will not remember previous conversations. Simply run `interpreter` to start a new chat:

```shell
interpreter
```

In Python, Open Interpreter remembers conversation history. If you want to start fresh, you can reset it:

```python
interpreter.messages = []
```

---

### Save and Restore Chats

In your terminal, Open Interpreter will save previous conversations to `<your application directory>/Open Interpreter/conversations/`.

You can resume any of them by running `--conversations`. Use your arrow keys to select one , then press `ENTER` to resume it.

```shell
interpreter --conversations
```

In Python, `interpreter.chat()` returns a List of messages, which can be used to resume a conversation with `interpreter.messages = messages`:

```python
# Save messages to 'messages'
messages = interpreter.chat("My name is Killian.")

# Reset interpreter ("Killian" will be forgotten)
interpreter.messages = []

# Resume chat from 'messages' ("Killian" will be remembered)
interpreter.messages = messages
```

---

### Configure Default Settings

We save default settings to a profile which can be edited by running the following command:

```shell
interpreter --profiles
```

You can use this to set your default language model, system message (custom instructions), max budget, etc.

<Info>
  **Note:** The Python library will also inherit settings from the default
  profile file. You can change it by running `interpreter --profiles` and
  editing `default.yaml`.
</Info>

---

### Customize System Message

In your terminal, modify the system message by [editing your configuration file as described here](#configure-default-settings).

In Python, you can inspect and configure Open Interpreter's system message to extend its functionality, modify permissions, or give it more context.

```python
interpreter.system_message += """
Run shell commands with -y so the user doesn't have to confirm them.
"""
print(interpreter.system_message)
```

---

### Change your Language Model

Open Interpreter uses [LiteLLM](https://docs.litellm.ai/docs/providers/) to connect to language models.

You can change the model by setting the model parameter:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

In Python, set the model on the object:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Find the appropriate "model" string for your language model here.](https://docs.litellm.ai/docs/providers/)

================
File: CONTRIBUTING.md
================
# 

**Open Interpreter is large, open-source initiative to build a standard interface between language models and computers.**

There are many ways to contribute, from helping others on [Github](https://github.com/OpenInterpreter/open-interpreter/issues) or [Discord](https://discord.gg/6p3fD6rBVm), writing documentation, or improving code.

We depend on contributors like you. Let's build this.

## What should I work on?

First, please familiarize yourself with our [project scope](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md#whats-in-our-scope). Then, pick up a task from our [roadmap](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md) or work on solving an [issue](https://github.com/OpenInterpreter/open-interpreter/issues).

If you encounter a bug or have a feature in mind, don't hesitate to [open a new issue](https://github.com/OpenInterpreter/open-interpreter/issues/new/choose).

## Philosophy

This is a minimalist, **tightly scoped** project that places a premium on simplicity. We're skeptical of new extensions, integrations, and extra features. We would rather not extend the system if it adds nonessential complexity.

# Contribution Guidelines

1. Before taking on significant code changes, please discuss your ideas on [Discord](https://discord.gg/6p3fD6rBVm) to ensure they align with our vision. We want to keep the codebase simple and unintimidating for new users.
2. Fork the repository and create a new branch for your work.
3. Follow the [Running Your Local Fork](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#running-your-local-fork) guide below.
4. Make changes with clear code comments explaining your approach. Try to follow existing conventions in the code.
5. Follow the [Code Formatting and Linting](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#code-formatting-and-linting) guide below.
6. Open a PR into `main` linking any related issues. Provide detailed context on your changes.

We will review PRs when possible and work with you to integrate your contribution. Please be patient as reviews take time. Once approved, your code will be merged.

## Running Your Local Fork

**Note: for anyone testing the new `--local`, `--os`, and `--local --os` modes: When you run `poetry install` you aren't installing the optional dependencies and it'll throw errors. To test `--local` mode, run `poetry install -E local`. To test `--os` mode, run `poetry install -E os`. To test `--local --os` mode, run `poetry install -E local -E os`. You can edit the system messages for these modes in `interpreter/terminal_interface/profiles/defaults`.**

Once you've forked the code and created a new branch for your work, you can run the fork in CLI mode by following these steps:

1. CD into the project folder by running `cd open-interpreter`.
2. Install `poetry` [according to their documentation](https://python-poetry.org/docs/#installing-with-pipx), which will create a virtual environment for development + handle dependencies.
3. Install dependencies by running `poetry install`.
4. Run the program with `poetry run interpreter`. Run tests with `poetry run pytest -s -x`.

**Note**: This project uses [`black`](https://black.readthedocs.io/en/stable/index.html) and [`isort`](https://pypi.org/project/isort/) via a [`pre-commit`](https://pre-commit.com/) hook to ensure consistent code style. If you need to bypass it for some reason, you can `git commit` with the `--no-verify` flag.

### Installing New Dependencies

If you wish to install new dependencies into the project, please use `poetry add package-name`.

### Installing Developer Dependencies

If you need to install dependencies specific to development, like testing tools, formatting tools, etc. please use `poetry add package-name --group dev`.

### Known Issues

For some, `poetry install` might hang on some dependencies. As a first step, try to run the following command in your terminal:

`export PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring`

Then run `poetry install` again. If this doesn't work, please join our [Discord community](https://discord.gg/6p3fD6rBVm) for help.

## Code Formatting and Linting

Our project uses `black` for code formatting and `isort` for import sorting. To ensure consistency across contributions, please adhere to the following guidelines:

1. **Install Pre-commit Hooks**:

   If you want to automatically format your code every time you make a commit, install the pre-commit hooks.

   ```bash
   pip install pre-commit
   pre-commit install
   ```

   After installing, the hooks will automatically check and format your code every time you commit.

2. **Manual Formatting**:

   If you choose not to use the pre-commit hooks, you can manually format your code using:

   ```bash
   black .
   isort .
   ```

# Licensing

Contributions to Open Interpreter would be under the MIT license before version 0.2.0, or under AGPL for subsequent contributions.

# Questions?

Join our [Discord community](https://discord.gg/6p3fD6rBVm) and post in the #General channel to connect with contributors. We're happy to guide you through your first open source contribution to this project!

**Thank you for your dedication and understanding as we continue refining our processes. As we explore this extraordinary new technology, we sincerely appreciate your involvement.**

================
File: mint.json
================
{
  "name": "Open Interpreter",
  "logo": {
    "dark": "/assets/logo/circle-inverted.png",
    "light": "/assets/logo/circle.png"
  },
  "favicon": "/assets/favicon.png",
  "colors": {
    "primary": "#000000",
    "light": "#FFFFFF",
    "dark": "#000000",
    "background": {
      "light": "#FFFFFF",
      "dark": "#000000"
    },
    "anchors": {
      "from": "#000000",
      "to": "#000000"
    }
  },
  "topbarLinks": [
    {
      "name": "50K  GitHub",
      "url": "https://github.com/OpenInterpreter/open-interpreter"
    }
  ],
  "topbarCtaButton": {
    "name": "Join Discord",
    "url": "https://discord.gg/Hvz9Axh84z"
  },
  "navigation": [
    {
      "group": "Getting Started",
      "pages": [
        "getting-started/introduction",
        "getting-started/setup"
      ]
    },
    {
      "group": "Guides",
      "pages": [
        "guides/basic-usage",
        "guides/running-locally",
        "guides/profiles",
        "guides/streaming-response",
        "guides/advanced-terminal-usage",
        "guides/multiple-instances",
        "guides/os-mode"
      ]
    },
    {
      "group": "Settings",
      "pages": [
        "settings/all-settings"
      ]
    },
    {
      "group": "Language Models",
      "pages": [
        "language-models/introduction",
        {
          "group": "Hosted Providers",
          "pages": [
            "language-models/hosted-models/openai",
            "language-models/hosted-models/azure",
            "language-models/hosted-models/vertex-ai",
            "language-models/hosted-models/replicate",
            "language-models/hosted-models/togetherai",
            "language-models/hosted-models/mistral-api",
            "language-models/hosted-models/anthropic",
            "language-models/hosted-models/anyscale",
            "language-models/hosted-models/aws-sagemaker",
            "language-models/hosted-models/baseten",
            "language-models/hosted-models/cloudflare",
            "language-models/hosted-models/cohere",
            "language-models/hosted-models/ai21",
            "language-models/hosted-models/deepinfra",
            "language-models/hosted-models/huggingface",
            "language-models/hosted-models/nlp-cloud",
            "language-models/hosted-models/openrouter",
            "language-models/hosted-models/palm",
            "language-models/hosted-models/perplexity",
            "language-models/hosted-models/petals",
            "language-models/hosted-models/vllm"
          ]
        },
        {
          "group": "Local Providers",
          "pages": [
            "language-models/local-models/ollama",
            "language-models/local-models/llamafile",
            "language-models/local-models/janai",
            "language-models/local-models/lm-studio",
            "language-models/local-models/custom-endpoint",
            "language-models/local-models/best-practices"
          ]
        },
        "language-models/custom-models",
        "language-models/settings"
      ]
    },
    {
      "group": "Code Execution",
      "pages": [
        "code-execution/usage",
        "code-execution/computer-api",
        "code-execution/custom-languages",
        "code-execution/settings"
      ]
    },
    {
      "group": "Protocols",
      "pages": [
        "protocols/lmc-messages"
      ]
    },
    {
      "group": "Integrations",
      "pages": [
        "integrations/e2b",
        "integrations/docker"
      ]
    },
    {
      "group": "Safety",
      "pages": [
        "safety/introduction",
        "safety/isolation",
        "safety/safe-mode",
        "safety/best-practices"
      ]
    },
    {
      "group": "Troubleshooting",
      "pages": [
        "troubleshooting/faq"
      ]
    },
    {
      "group": "Telemetry",
      "pages": [
        "telemetry/telemetry"
      ]
    }
  ],
  "feedback": {
    "suggestEdit": true
  },
  "footerSocials": {
    "twitter": "https://x.com/OpenInterpreter",
    "youtube": "https://www.youtube.com/@OpenInterpreter",
    "linkedin": "https://www.linkedin.com/company/openinterpreter"
  }
}

================
File: NCU_MIGRATION_GUIDE.md
================
# `0.2.0` Migration Guide

Open Interpreter is [changing](https://changes.openinterpreter.com/log/the-new-computer-update). This guide will help you migrate your application to `0.2.0`, also called the _New Computer Update_ (NCU), the latest major version of Open Interpreter.

## A New Start

To start using Open Interpreter in Python, we now use a standard **class instantiation** format:

```python
# From the module `interpreter`, import the class `OpenInterpreter`
from interpreter import OpenInterpreter

# Create an instance of `OpenInterpreter` to use it
agent = OpenInterpreter()
agent.chat()
```

For convenience, we also provide an instance of `interpreter`, which you can import from the module (also called `interpreter`):

```python
 # From the module `interpreter`, import the included instance of `OpenInterpreter`
from interpreter import interpreter

interpreter.chat()
```

## New Parameters

All stateless LLM attributes have been moved to `interpreter.llm`:

- `interpreter.model`  `interpreter.llm.model`
- `interpreter.api_key`  `interpreter.llm.api_key`
- `interpreter.llm_supports_vision`  `interpreter.llm.supports_vision`
- `interpreter.supports_function_calling`  `interpreter.llm.supports_functions`
- `interpreter.max_tokens`  `interpreter.llm.max_tokens`
- `interpreter.context_window`  `interpreter.llm.context_window`
- `interpreter.temperature`  `interpreter.llm.temperature`
- `interpreter.api_version`  `interpreter.llm.api_version`
- `interpreter.api_base`  `interpreter.llm.api_base`

This is reflected **1)** in Python applications using Open Interpreter and **2)** in your profile for OI's terminal interface, which can be edited via `interpreter --profiles`.

## New Static Messages Structure

- The array of messages is now flat, making the architecture more modular, and easier to adapt to new kinds of media in the future.
- Each message holds only one kind of data. This yields more messages, but prevents large nested messages that can be difficult to parse.
- This allows you to pass the full `messages` list into Open Interpreter as `interpreter.messages = message_list`.
- Every message has a "role", which can be "assistant", "computer", or "user".
- Every message has a "type", specifying the type of data it contains.
- Every message has "content", which contains the data for the message.
- Some messages have a "format" key, to specify the format of the content, like "path" or "base64.png".
- The recipient of the message is specified by the "recipient" key, which can be "user" or "assistant". This is used to inform the LLM of who the message is intended for.

```python
[
  {"role": "user", "type": "message", "content": "Please create a plot from this data and display it as an image and then as HTML."}, # implied format: text (only one format for type message)
  {"role": "user", "type": "image", "format": "path", "content": "path/to/image.png"}
  {"role": "user", "type": "file", "content": "/path/to/file.pdf"} # implied format: path (only one format for type file)
  {"role": "assistant", "type": "message", "content": "Processing your request to generate a plot."} # implied format: text
  {"role": "assistant", "type": "code", "format": "python", "content": "plot = create_plot_from_data('data')\ndisplay_as_image(plot)\ndisplay_as_html(plot)"}
  {"role": "computer", "type": "image", "format": "base64.png", "content": "base64"}
  {"role": "computer", "type": "code", "format": "html", "content": "<html>Plot in HTML format</html>"}
  {"role": "computer", "type": "console", "format": "output", "content": "{HTML errors}"}
  {"role": "assistant", "type": "message", "content": "Plot generated successfully."} # implied format: text
]
```

## New Streaming Structure

- The streaming data structure closely matches the static messages structure, with only a few differences.
- Every streaming chunk has a "start" and "end" key, which are booleans that specify whether the chunk is the first or last chunk in the stream. This is what you should use to build messages from the streaming chunks.
- There is a "confirmation" chunk type, which is used to confirm with the user that the code should be run. The "content" key of this chunk is a dictionary with a `code` and a `language` key.
- Introducing more information per chunk is helpful in processing the streaming responses. Please take a look below for example code for processing streaming responses, in JavaScript.

```python
{"role": "assistant", "type": "message", "start": True}
{"role": "assistant", "type": "message", "content": "Pro"}
{"role": "assistant", "type": "message", "content": "cessing"}
{"role": "assistant", "type": "message", "content": "your request"}
{"role": "assistant", "type": "message", "content": "to generate a plot."}
{"role": "assistant", "type": "message", "end": True}

{"role": "assistant", "type": "code", "format": "python", "start": True}
{"role": "assistant", "type": "code", "format": "python", "content": "plot = create_plot_from_data"}
{"role": "assistant", "type": "code", "format": "python", "content": "('data')\ndisplay_as_image(plot)"}
{"role": "assistant", "type": "code", "format": "python", "content": "\ndisplay_as_html(plot)"}
{"role": "assistant", "type": "code", "format": "python", "end": True}

# The computer will emit a confirmation chunk *before* running the code. You can break here to cancel the execution.

{"role": "computer", "type": "confirmation", "format": "execution", "content": {
    "type": "code",
    "format": "python",
    "content": "plot = create_plot_from_data('data')\ndisplay_as_image(plot)\ndisplay_as_html(plot)",
}}

{"role": "computer", "type": "console", "start": True}
{"role": "computer", "type": "console", "format": "output", "content": "a printed statement"}
{"role": "computer", "type": "console", "format": "active_line", "content": "1"}
{"role": "computer", "type": "console", "format": "active_line", "content": "2"}
{"role": "computer", "type": "console", "format": "active_line", "content": "3"}
{"role": "computer", "type": "console", "format": "output", "content": "another printed statement"}
{"role": "computer", "type": "console", "end": True}
```

## Tips and Best Practices

- Adding an `id` and a `created_at` field to messages can be helpful to manipulate the messages later on.
- If you want your application to run the code instead of OI, then your app will act as the `computer`. This means breaking from the stream once OI emits a confirmation chunk (`{'role': 'computer', 'type': 'confirmation' ...}`) to prevent OI from running the code. When you run code, grab the message history via `messages = interpreter.messages`, then simply mimic the `computer` format above by appending new `{'role': 'computer' ...}` messages, then run `interpreter.chat(messages)`.
- Open Interpreter is designed to stop code execution when the stream is disconnected. Use this to your advantage to add a "Stop" button to the UI.
- Setting up your Python server to send errors and exceptions to the client can be helpful for debugging and generating error messages.

## Example Code

### Types

Python:

```python
class Message:
    role: Union["user", "assistant", "computer"]
    type: Union["message", "code", "image", "console", "file", "confirmation"]
    format: Union["output", "path", "base64.png", "base64.jpeg", "python", "javascript", "shell", "html", "active_line", "execution"]
    recipient: Union["user", "assistant"]
    content: Union[str, dict]  # dict should have 'code' and 'language' keys, this is only for confirmation messages

class StreamingChunk(Message):
    start: bool
    end: bool
```

TypeScript:

```typescript
interface Message {
  role: "user" | "assistant" | "computer";
  type: "message" | "code" | "image" | "console" | "file", | "confirmation";
  format: "output" | "path" | "base64.png" | "base64.jpeg" | "python" | "javascript" | "shell" | "html" | "active_line", | "execution";
  recipient: "user" | "assistant";
  content: string | { code: string; language: string };
}
```

```typescript
interface StreamingChunk extends Message {
  start: boolean;
  end: boolean;
}
```

### Handling streaming chunks

Here is a minimal example of how to handle streaming chunks in JavaScript. This example assumes that you are using a Python server to handle the streaming requests, and that you are using a JavaScript client to send the requests and handle the responses. See the main repository README for an example FastAPI server.

```javascript
//Javascript

let messages = []; //variable to hold all messages
let currentMessageIndex = 0; //variable to keep track of the current message index
let isGenerating = false; //variable to stop the stream

// Function to send a POST request to the OI
async function sendRequest() {
  // Temporary message to hold the message that is being processed
  try {
    // Define parameters for the POST request, add at least the full messages array, but you may also consider adding any other OI parameters here, like auto_run, local, etc.
    const params = {
      messages,
    };

    //Define a controller to allow for aborting the request
    const controller = new AbortController();
    const { signal } = controller;

    // Send the POST request to your Python server endpoint
    const interpreterCall = await fetch("https://YOUR_ENDPOINT/", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(params),
      signal,
    });

    // Throw an error if the request was not successful
    if (!interpreterCall.ok) {
      console.error("Interpreter didn't respond with 200 OK");
      return;
    }

    // Initialize a reader for the response body
    const reader = interpreterCall.body.getReader();

    isGenerating = true;
    while (true) {
      const { value, done } = await reader.read();

      // Break the loop if the stream is done
      if (done) {
        break;
      }
      // If isGenerating is set to false, cancel the reader and break the loop. This will halt the execution of the code run by OI as well
      if (!isGenerating) {
        await reader.cancel();
        controller.abort();
        break;
      }
      // Decode the stream and split it into lines
      const text = new TextDecoder().decode(value);
      const lines = text.split("\n");
      lines.pop(); // Remove last empty line

      // Process each line of the response
      for (const line of lines) {
        const chunk = JSON.parse(line);
        await processChunk(chunk);
      }
    }
    //Stream has completed here, so run any code that needs to be run after the stream has finished
    if (isGenerating) isGenerating = false;
  } catch (error) {
    console.error("An error occurred:", error);
  }
}

//Function to process each chunk of the stream, and create messages
function processChunk(chunk) {
  if (chunk.start) {
    const tempMessage = {};
    //add the new message's data to the tempMessage
    tempMessage.role = chunk.role;
    tempMessage.type = chunk.type;
    tempMessage.content = "";
    if (chunk.format) tempMessage.format = chunk.format;
    if (chunk.recipient) tempMessage.recipient = chunk.recipient;

    //add the new message to the messages array, and set the currentMessageIndex to the index of the new message
    messages.push(tempMessage);
    currentMessageIndex = messages.length - 1;
  }

  //Handle active lines for code blocks
  if (chunk.format === "active_line") {
    messages[currentMessageIndex].activeLine = chunk.content;
  } else if (chunk.end && chunk.type === "console") {
    messages[currentMessageIndex].activeLine = null;
  }

  //Add the content of the chunk to current message, avoiding adding the content of the active line
  if (chunk.content && chunk.format !== "active_line") {
    messages[currentMessageIndex].content += chunk.content;
  }
}
```

================
File: README_DE.md
================
<h1 align="center"> Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white">
    </a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Espaol-white.svg" alt="ES doc"/></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/--white.svg" alt="JA doc"></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/--white.svg" alt="ZH doc"></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/README_UK.md"><img src="https://img.shields.io/badge/-white.svg" alt="UK doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br><br>
    <b>Lassen Sie Sprachmodelle Code auf Ihrem Computer ausfhren.</b><br>
    Eine Open-Source, lokal laufende Implementierung von OpenAIs Code-Interpreter.<br>
    <br><a href="https://openinterpreter.com">Erhalten Sie frhen Zugang zur Desktop-Anwendung.</a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter** ermglicht es LLMs (Language Models), Code (Python, Javascript, Shell und mehr) lokal auszufhren. Sie knnen mit Open Interpreter ber eine ChatGPT-hnliche Schnittstelle in Ihrem Terminal chatten, indem Sie $ interpreter nach der Installation ausfhren.

Dies bietet eine natrliche Sprachschnittstelle zu den allgemeinen Fhigkeiten Ihres Computers:

- Erstellen und bearbeiten Sie Fotos, Videos, PDFs usw.
- Steuern Sie einen Chrome-Browser, um Forschungen durchzufhren
- Darstellen, bereinigen und analysieren Sie groe Datenstze
- ...usw.

** Hinweis: Sie werden aufgefordert, Code zu genehmigen, bevor er ausgefhrt wird.**

<br>

## Demo

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Eine interaktive Demo ist auch auf Google Colab verfgbar:

[![In Colab ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

## Schnellstart

```shell
pip install open-interpreter
```

### Terminal

Nach der Installation fhren Sie einfach `interpreter` aus:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Stellen Sie AAPL und METAs normalisierte Aktienpreise dar") # Fhrt einen einzelnen Befehl aus
interpreter.chat() # Startet einen interaktiven Chat
```

## Vergleich zu ChatGPTs Code Interpreter

OpenAIs Verffentlichung des [Code Interpreters](https://openai.com/blog/chatgpt-plugins#code-interpreter) mit GPT-4 bietet eine fantastische Mglichkeit, reale Aufgaben mit ChatGPT zu erledigen.

Allerdings ist OpenAIs Dienst gehostet, Closed-Source und stark eingeschrnkt:

- Kein Internetzugang.
- [Begrenzte Anzahl vorinstallierter Pakete](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
- 100 MB maximale Uploadgre, 120.0 Sekunden Laufzeitlimit.
- Der Zustand wird gelscht (zusammen mit allen generierten Dateien oder Links), wenn die Umgebung abstirbt.

---

Open Interpreter berwindet diese Einschrnkungen, indem es in Ihrer lokalen Umgebung luft. Es hat vollen Zugang zum Internet, ist nicht durch Zeit oder Dateigre eingeschrnkt und kann jedes Paket oder jede Bibliothek nutzen.

Dies kombiniert die Kraft von GPT-4s Code Interpreter mit der Flexibilitt Ihrer lokalen Maschine.

## Sicherheitshinweis

Da generierter Code in deiner lokalen Umgebung ausgefhrt wird, kann er mit deinen Dateien und Systemeinstellungen interagieren, was potenziell zu unerwarteten Ergebnissen wie Datenverlust oder Sicherheitsrisiken fhren kann.

** Open Interpreter wird um Nutzerbesttigung bitten, bevor Code ausgefhrt wird.**

Du kannst `interpreter -y` ausfhren oder `interpreter.auto_run = True` setzen, um diese Besttigung zu umgehen, in diesem Fall:

- Sei vorsichtig bei Befehlsanfragen, die Dateien oder Systemeinstellungen ndern.
- Beobachte Open Interpreter wie ein selbstfahrendes Auto und sei bereit, den Prozess zu beenden, indem du dein Terminal schliet.
- Betrachte die Ausfhrung von Open Interpreter in einer eingeschrnkten Umgebung wie Google Colab oder Replit. Diese Umgebungen sind isolierter und reduzieren das Risiko der Ausfhrung willkrlichen Codes.

Es gibt **experimentelle** Untersttzung fr einen [Sicherheitsmodus](docs/SAFE_MODE.md), um einige Risiken zu mindern.

## Wie funktioniert es?

Open Interpreter rstet ein [funktionsaufrufendes Sprachmodell](https://platform.openai.com/docs/guides/gpt/function-calling) mit einer `exec()`-Funktion aus, die eine `language` (wie "Python" oder "JavaScript") und auszufhrenden `code` akzeptiert.

Wir streamen dann die Nachrichten des Modells, Code und die Ausgaben deines Systems zum Terminal als Markdown.

# Mitwirken

Danke fr dein Interesse an der Mitarbeit! Wir begren die Beteiligung der Gemeinschaft.

Bitte sieh dir unsere [Richtlinien fr Mitwirkende](docs/CONTRIBUTING.md) fr weitere Details an, wie du dich einbringen kannst.

## Lizenz

Open Interpreter ist unter der MIT-Lizenz lizenziert. Du darfst die Software verwenden, kopieren, modifizieren, verteilen, unterlizenzieren und Kopien der Software verkaufen.

**Hinweis**: Diese Software ist nicht mit OpenAI affiliiert.

> Zugriff auf einen Junior-Programmierer zu haben, der mit der Geschwindigkeit deiner Fingerspitzen arbeitet ... kann neue Arbeitsablufe mhelos und effizient machen sowie das Programmieren einem neuen Publikum ffnen.
>
>  _OpenAIs Code Interpreter Release_

<br>

================
File: README_ES.md
================
<h1 align="center"> Intrprete Abierto</h1>

<p align="center">
    <a href="https://discord.gg/Hvz9Axh84z">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/--white.svg" alt="JA doc"/></a>
    <a href="README_ZH.md"> <img src="https://img.shields.io/badge/--white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/-white.svg" alt="UK doc"/></a>
    <a href="README_IN.md"> <img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <br><a href="https://0ggfznkwh4j.typeform.com/to/G21i9lJ2">Obtenga acceso temprano a la aplicacin de escritorio</a>  |  <a href="https://docs.openinterpreter.com/">Documentacin</a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>
<p align="center">
<strong>La Nueva Actualizacin del Computador</strong> presenta <strong><code>--os</code></strong> y una nueva <strong>API de Computadora</strong>. <a href="https://changes.openinterpreter.com/log/the-new-computer-update">Lea ms </a>
</p>
<br>

```shell
pip install open-interpreter
```

> No funciona? Lea nuestra [gua de configuracin](https://docs.openinterpreter.com/getting-started/setup).

```shell
interpreter
```

<br>

**Intrprete Abierto** permite a los LLMs ejecutar cdigo (Python, JavaScript, Shell, etc.) localmente. Puede chatear con Intrprete Abierto a travs de una interfaz de chat como ChatGPT en su terminal despus de instalar.

Esto proporciona una interfaz de lenguaje natural para las capacidades generales de su computadora:

- Crear y editar fotos, videos, PDF, etc.
- Controlar un navegador de Chrome para realizar investigaciones
- Graficar, limpiar y analizar conjuntos de datos grandes
- ... etc.

** Nota: Se le pedir que apruebe el cdigo antes de ejecutarlo.**

<br>

## Demo

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Tambin hay disponible una demo interactiva en Google Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

#### Adems, hay un ejemplo de interfaz de voz inspirada en _Her_:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

## Inicio Rpido

```shell
pip install open-interpreter
```

### Terminal

Despus de la instalacin, simplemente ejecute `interpreter`:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Plot AAPL and META's normalized stock prices") # Ejecuta un comando sencillo
interpreter.chat() # Inicia una sesin de chat interactiva
```

### GitHub Codespaces

Presione la tecla `,` en la pgina de GitHub de este repositorio para crear un espacio de cdigos. Despus de un momento, recibir un entorno de mquina virtual en la nube con Interprete Abierto pre-instalado. Puede entonces empezar a interactuar con l directamente y confirmar su ejecucin de comandos del sistema sin preocuparse por daar el sistema.

## Comparacin con el Intrprete de Cdigo de ChatGPT

El lanzamiento de [Intrprete de Cdigo](https://openai.com/blog/chatgpt-plugins#code-interpreter) de OpenAI con GPT-4 presenta una oportunidad fantstica para realizar tareas del mundo real con ChatGPT.

Sin embargo, el servicio de OpenAI est alojado, su codigo es cerrado y est fuertemente restringido:

- No hay acceso a Internet.
- [Conjunto limitado de paquetes preinstalados](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
- Lmite de 100 MB de carga, lmite de tiempo de 120.0 segundos.
- El estado se elimina (junto con cualquier archivo generado o enlace) cuando el entorno se cierra.

---

Intrprete Abierto supera estas limitaciones al ejecutarse en su entorno local. Tiene acceso completo a Internet, no est restringido por tiempo o tamao de archivo y puede utilizar cualquier paquete o libreria.

Esto combina el poder del Intrprete de Cdigo de GPT-4 con la flexibilidad de su entorno de desarrollo local.

## Comandos

**Actualizacin:** La Actualizacin del Generador (0.1.5) introdujo streaming:

```python
message = "Qu sistema operativo estamos utilizando?"

for chunk in interpreter.chat(message, display=False, stream=True):
    print(chunk)
```

### Chat Interactivo

Para iniciar una sesin de chat interactiva en su terminal, puede ejecutar `interpreter` desde la lnea de comandos:

```shell
interpreter
```

O `interpreter.chat()` desde un archivo `.py`:

```python
interpreter.chat()
```

**Puede tambin transmitir cada trozo:**

```python
message = "Qu sistema operativo estamos utilizando?"

for chunk in interpreter.chat(message, display=False, stream=True):
    print(chunk)
```

### Chat Programtico

Para un control ms preciso, puede pasar mensajes directamente a `.chat(message)`:

```python
interpreter.chat("Aade subttulos a todos los videos en /videos.")

# ... Transmite salida a su terminal, completa tarea ...

interpreter.chat("Estos se ven bien, pero pueden hacer los subttulos ms grandes?")

# ...
```

### Iniciar un nuevo chat

En Python, Intrprete Abierto recuerda el historial de conversacin. Si desea empezar de nuevo, puede resetearlo:

```python
interpreter.messages = []
```

### Guardar y Restaurar Chats

`interpreter.chat()` devuelve una lista de mensajes, que puede utilizar para reanudar una conversacin con `interpreter.messages = messages`:

```python
messages = interpreter.chat("Mi nombre es Killian.") # Guarda mensajes en 'messages'
interpreter.messages = [] # Resetear Intrprete ("Killian" ser olvidado)

interpreter.messages = messages # Reanuda chat desde 'messages' ("Killian" ser recordado)
```

### Personalizar el Mensaje del Sistema

Puede inspeccionar y configurar el mensaje del sistema de Intrprete Abierto para extender su funcionalidad, modificar permisos o darle ms contexto.

```python
interpreter.system_message += """
Ejecute comandos de shell con -y para que el usuario no tenga que confirmarlos.
"""
print(interpreter.system_message)
```

### Cambiar el Modelo de Lenguaje

Intrprete Abierto utiliza [LiteLLM](https://docs.litellm.ai/docs/providers/) para conectarse a modelos de lenguaje hospedados.

Puede cambiar el modelo estableciendo el parmetro de modelo:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

En Python, establezca el modelo en el objeto:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Encuentre la cadena adecuada para su modelo de lenguaje aqu.](https://docs.litellm.ai/docs/providers/)

### Ejecutar Intrprete Abierto localmente

#### Terminal

Intrprete Abierto puede utilizar un servidor de OpenAI compatible para ejecutar modelos localmente. (LM Studio, jan.ai, ollama, etc.)

Simplemente ejecute `interpreter` con la URL de base de API de su servidor de inferencia (por defecto, `http://localhost:1234/v1` para LM Studio):

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

O puede utilizar Llamafile sin instalar software adicional simplemente ejecutando:

```shell
interpreter --local
```

Para una gua mas detallada, consulte [este video de Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

**Cmo ejecutar LM Studio en segundo plano.**

1. Descargue [https://lmstudio.ai/](https://lmstudio.ai/) luego ejecutelo.
2. Seleccione un modelo, luego haga clic ** Descargar**.
3. Haga clic en el botn **** en la izquierda (debajo de ).
4. Seleccione su modelo en la parte superior, luego haga clic **Iniciar Servidor**.

Una vez que el servidor est funcionando, puede empezar su conversacin con Intrprete Abierto.

> **Nota:** El modo local establece su `context_window` en 3000 y su `max_tokens` en 1000. Si su modelo tiene requisitos diferentes, ajuste estos parmetros manualmente (ver a continuacin).

#### Python

Nuestro paquete de Python le da ms control sobre cada ajuste. Para replicar y conectarse a LM Studio, utilice estos ajustes:

```python
from interpreter import interpreter

interpreter.offline = True # Desactiva las caractersticas en lnea como Procedimientos Abiertos
interpreter.llm.model = "openai/x" # Indica a OI que enve mensajes en el formato de OpenAI
interpreter.llm.api_key = "fake_key" # LiteLLM, que utilizamos para hablar con LM Studio, requiere esto
interpreter.llm.api_base = "http://localhost:1234/v1" # Apunta esto a cualquier servidor compatible con OpenAI

interpreter.chat()
```

#### Ventana de Contexto, Tokens Mximos

Puede modificar los `max_tokens` y `context_window` (en tokens) de los modelos locales.

Para el modo local, ventanas de contexto ms cortas utilizarn menos RAM, as que recomendamos intentar una ventana mucho ms corta (~1000) si falla o si es lenta. Asegrese de que `max_tokens` sea menor que `context_window`.

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

### Modo Detallado

Para ayudarle a inspeccionar Intrprete Abierto, tenemos un modo `--verbose` para depuracin.

Puede activar el modo detallado utilizando el parmetro (`interpreter --verbose`), o en plena sesin:

```shell
$ interpreter
...
> %verbose true <- Activa el modo detallado

> %verbose false <- Desactiva el modo verbose
```

### Comandos de Modo Interactivo

En el modo interactivo, puede utilizar los siguientes comandos para mejorar su experiencia. Aqu hay una lista de comandos disponibles:

**Comandos Disponibles:**

- `%verbose [true/false]`: Activa o desactiva el modo detallado. Sin parmetros o con `true` entra en modo detallado.
  Con `false` sale del modo verbose.
- `%reset`: Reinicia la sesin actual de conversacin.
- `%undo`: Elimina el mensaje de usuario previo y la respuesta del AI del historial de mensajes.
- `%tokens [prompt]`: (_Experimental_) Calcula los tokens que se enviarn con el prximo prompt como contexto y estima su costo. Opcionalmente, calcule los tokens y el costo estimado de un `prompt` si se proporciona. Depende de [LiteLLM's `cost_per_token()` method](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token) para costos estimados.
- `%help`: Muestra el mensaje de ayuda.

### Configuracin / Perfiles

Intrprete Abierto permite establecer comportamientos predeterminados utilizando archivos `yaml`.

Esto proporciona una forma flexible de configurar el intrprete sin cambiar los argumentos de lnea de comandos cada vez.

Ejecutar el siguiente comando para abrir el directorio de perfiles:

```
interpreter --profiles
```

Puede agregar archivos `yaml` all. El perfil predeterminado se llama `default.yaml`.

#### Perfiles Mltiples

Intrprete Abierto admite mltiples archivos `yaml`, lo que permite cambiar fcilmente entre configuraciones:

```
interpreter --profile my_profile.yaml
```

## Servidor de FastAPI de ejemplo

El generador actualiza permite controlar Intrprete Abierto a travs de puntos de conexin HTTP REST:

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

Puede iniciar un servidor idntico al anterior simplemente ejecutando `interpreter.server()`.

## Android

La gua paso a paso para instalar Intrprete Abierto en su dispositivo Android se encuentra en el [repo de open-interpreter-termux](https://github.com/MikeBirdTech/open-interpreter-termux).

## Aviso de Seguridad

Ya que el cdigo generado se ejecuta en su entorno local, puede interactuar con sus archivos y configuraciones del sistema, lo que puede llevar a resultados inesperados como prdida de datos o riesgos de seguridad.

** Intrprete Abierto le pedir que apruebe el cdigo antes de ejecutarlo.**

Puede ejecutar `interpreter -y` o establecer `interpreter.auto_run = True` para evitar esta confirmacin, en cuyo caso:

- Sea cuidadoso al solicitar comandos que modifican archivos o configuraciones del sistema.
- Vigile Intrprete Abierto como si fuera un coche autnomo y est preparado para terminar el proceso cerrando su terminal.
- Considere ejecutar Intrprete Abierto en un entorno restringido como Google Colab o Replit. Estos entornos son ms aislados, reduciendo los riesgos de ejecutar cdigo arbitrario.

Hay soporte **experimental** para un [modo seguro](docs/SAFE_MODE.md) para ayudar a mitigar algunos riesgos.

## Cmo Funciona?

Intrprete Abierto equipa un [modelo de lenguaje de llamada a funciones](https://platform.openai.com/docs/guides/gpt/function-calling) con una funcin `exec()`, que acepta un `lenguaje` (como "Python" o "JavaScript") y `cdigo` para ejecutar.

Luego, transmite los mensajes del modelo, el cdigo y las salidas del sistema a la terminal como Markdown.

# Acceso a la Documentacin Offline

La documentacin completa est disponible en lnea sin necesidad de conexin a Internet.

[Node](https://nodejs.org/en) es un requisito previo:

- Versin 18.17.0 o cualquier versin posterior 18.x.x.
- Versin 20.3.0 o cualquier versin posterior 20.x.x.
- Cualquier versin a partir de 21.0.0 sin lmite superior especificado.

Instale [Mintlify](https://mintlify.com/):

```bash
npm i -g mintlify@latest
```

Cambia a la carpeta de documentos y ejecuta el comando apropiado:

```bash
# Suponiendo que ests en la carpeta raz del proyecto
cd ./docs

# Ejecute el servidor de documentacin
mintlify dev
```

Una nueva ventana del navegador debera abrirse. La documentacin estar disponible en [http://localhost:3000](http://localhost:3000) mientras el servidor de documentacin est funcionando.

# Contribuyendo

Gracias por su inters en contribuir! Damos la bienvenida a la implicacin de la comunidad.

Por favor, consulte nuestras [directrices de contribucin](docs/CONTRIBUTING.md) para obtener ms detalles sobre cmo involucrarse.

# Roadmap

Visite [nuestro roadmap](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md) para ver el futuro de Intrprete Abierto.

**Nota:** Este software no est afiliado con OpenAI.

![thumbnail-ncu](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/1b19a5db-b486-41fd-a7a1-fe2028031686)

> Tener acceso a un programador junior trabajando a la velocidad de su dedos... puede hacer que los nuevos flujos de trabajo sean sencillos y eficientes, adems de abrir los beneficios de la programacin a nuevas audiencias.
>
>  _Lanzamiento del intrprete de cdigo de OpenAI_

<br>

================
File: README_IN.md
================
<h1 align="center"> Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/>
    </a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Espaol-white.svg" alt="ES doc"/></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/--white.svg" alt="JA doc"/></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/--white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/-white.svg" alt="UK doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br><br>
    <b>          </b><br>
         -,    <br>
    <br><a href="https://openinterpreter.com">             </a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

** **   (, , ,  )                  `$ interpreter`       -        

    -          :

- , ,      
-          
-     ,     
- ...

**  :         **

<br>

## 

[![  ](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

##  

```shell
pip install open-interpreter
```

### 

  ,  `interpreter` :

```shell
interpreter
```

### 

```python
from interpreter import interpreter

interpreter.chat("AAPL  META       ") #      
interpreter.chat() #      
```

## ChatGPT      

  [ ](https://openai.com/blog/chatgpt-plugins#code-interpreter)   GPT-4           ChatGPT             

,       , -     

     ,               :

-     
- [        ](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/)  
- 100        
- 120.0      
-     ,       (        )

---

                      ,          ,           

 GPT-4                

## Commands

### Interactive Chat

To start an interactive chat in your terminal, either run `interpreter` from the command line:

```shell
interpreter
```

Or `interpreter.chat()` from a .py file:

```python
interpreter.chat()
```

## 

###  

        ,      `interpreter` :

```shell
interpreter
```

  .py   `interpreter.chat()` :

````python
interpreter.chat()

###  

    ,   `.chat(message)`      :

```python
interpreter.chat("     /videos ")

# ...       ,     ...

interpreter.chat("              ?")

# ...
````

###    

Python ,                ,       :

```python
interpreter.messages = []
```

###     

```python
messages = interpreter.chat("   ") #   'messages'  

interpreter.messages = messages # 'messages'        (""   )
```

###    

                    ,      ,       

```python
interpreter.system_message += """
       , -y     
"""
print(interpreter.system_message)
```

###  

`gpt-3.5-turbo`       :

```shell
interpreter --fast
```

Python ,         :

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

###       

```shell
interpreter --local
```

####   

        `max_tokens`  `context_window` ( )      

        RAM   ,   GPU                

```shell
interpreter --max_tokens 2000 --context_window 16000
```

###  

           , `--verbose`     

      (`interpreter --verbose`)            :

```shell
$ interpreter
...
> %verbose true <-     

> %verbose false <-     
```

###   

  ,                   :

** :**
 `%verbose [true/false]`:          'true'  ,        'false'  ,       
 `%reset`:      
 `%undo`:            
 `%save_message []`:     JSON            ,      'messages.json'   
 `%load_message []`:   JSON              ,      'messages.json'   
 `%help`:   

            !

##  

          ,             ,            

** Open Interpreter            **

 `interpreter -y`     ... ... `interpreter.auto_run = True`          ,  :

-             
-     -                  
- Google Colab  Replit                            

##     ?

Open Interpreter  [-  ](https://platform.openai.com/docs/guides/gpt/function-calling)   `exec()`      ,   `language` ( "Python"  "JavaScript")  `code`       

    ,                

# 

        !        

      [ ](CONTRIBUTING.md) 

## 

Open Interpreter MIT           , , , ,      

** **:   OpenAI    

>              ...         ,   ...          
>
>  _OpenAI's Code Interpreter Release_

<br>

================
File: README_JA.md
================
<h1 align="center"> Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Espaol-white.svg" alt="ES doc"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/--white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/-white.svg" alt="UK doc"/></a>
    <a href="README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <b></b><br>
    OpenAI Code Interpreter<br>
    <br><a href="https://openinterpreter.com"></a>  |  <a href="https://docs.openinterpreter.com/"></a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

**Update:**  0.1.12  `interpreter --vision` ([](https://docs.openinterpreter.com/usage/terminal/vision))

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter**PythonJavascriptShell `$ interpreter`  ChatGPT Open Interpreter 



- PDF 
- Chrome 
- 
- 

** : **

<br>

## 

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Google Colab :

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

####  (_Her_ ):

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

## 

```shell
pip install open-interpreter
```

### 

`interpreter` :

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("AAPLMETA") # 
interpreter.chat() # 
```

## ChatGPT  Code Interpreter 

GPT-4  OpenAI  [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter)  ChatGPT 

OpenAI :

- 
- [](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/)
-  100MB 120 
- 

---

Open Interpreter 

Open Interpter GPT-4 Code Interpreter 

## 

**:** (0.1.5):

```python
message = ""

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### 

 `interpreter` 

```shell
interpreter
```

.py  `interpreter.chat()` 

```python
interpreter.chat()
```

** chunk :**

```python
message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### 

`.chat(message)`

```python
interpreter.chat("/videos ")

# ...  ...

interpreter.chat("")

# ...
```

### 

 Open Interpreter :

```python
interpreter.messages = []
```

### 

`interpreter.chat()` , `interpreter.messages = messages` :

```python
messages = interpreter.chat("") # 'messages'
interpreter.messages = [] # ""

interpreter.messages = messages # 'messages'""
```

### 

Open Interpreter 

```python
interpreter.system_message += """
 '-y' 
"""
print(interpreter.system_message)
```

### 

Open Interpreter  [LiteLLM](https://docs.litellm.ai/docs/providers/) 

model :

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

Python :

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[ "model" ](https://docs.litellm.ai/docs/providers/)

### 

Open Interpreter OpenAI  (LM Studiojan.aiollam )

 api_base URL interpreter (LM Studio http://localhost:1234/v1)

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

 Llamafile 

```shell
interpreter --local
```

[Mike Bird ](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H) 

**LM Studio **

1. [https://lmstudio.ai/](https://lmstudio.ai/)
2. ** ** 
3.  ****  
4. **** 

Open Interpreter 

> **:** `context_window`  3000 `max_tokens`  1000 

#### 

 `max_tokens`  `context_window`

 RAM 1000`max_tokens`  `context_window` 

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

### 

 Open Interpreter `--verbose` 

`interpreter --verbose`:

```shell
$ interpreter
...
> %verbose true # <- 

> %verbose false # <- 
```

### 

:

**:**

- `%verbose [true/false]`:  `true` `false` 
- `%reset`: 
- `%undo`:  AI 
- `%save_message [path]`:  JSON  `messages.json` 
- `%load_message [path]`:  JSON  `messages.json` 
- `%tokens [prompt]`: (__) `prompt`  [LiteLLM  `cost_per_token()` ](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token)
- `%help`: 

### 

Open Interpreter `config.yaml` 



:

```
interpreter --config
```

#### 

Open Interpreter  `config.yaml` `--config_file` 

****: `--config_file` 

:

```
interpreter --config --config_file $config_path
```

 Open Interpreter :

```
interpreter --config_file $config_path
```

****: `$config_path` 

##### 

1.  `config.turbo.yaml` 
   ```
   interpreter --config --config_file config.turbo.yaml
   ```
2. `config.turbo.yaml` `model`  `gpt-3.5-turbo` 
3. `config.turbo.yaml` Open Interpreter 
   ```
   interpreter --config_file config.turbo.yaml
   ```

##### Python 

Python  Open Interpreter :

```python
import os
from interpreter import interpreter

currentPath = os.path.dirname(os.path.abspath(__file__))
config_path=os.path.join(currentPath, './config.test.yaml')

interpreter.extend_config(config_path=config_path)

message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

## FastAPI 

 Open Interpreter HTTP REST :

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

## 



** Open Interpreter **

`interpreter -y` `interpreter.auto_run = True` :

- 
- Open Interpreter 
- Google Colab  Replit  Open Interpreter 

[](docs/SAFE_MODE.md) **** 

## Open Interpreter 

Open Interpreter [](https://platform.openai.com/docs/guides/gpt/function-calling) `exec()` "python""javascript"

 Markdown 

# 



[](CONTRIBUTING.md)

# 

Open Interpreter [](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md)

****:  OpenAI 

>  
>
>  _OpenAI Code Interpreter _

<br>

================
File: README_UK.md
================
<h1 align="center"> Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/Hvz9Axh84z">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/--white.svg" alt="JA doc"/></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/--white.svg" alt="ZH doc"/></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Espaol-white.svg" alt="ES doc"/></a>
    <a href="README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <br><a href="https://0ggfznkwh4j.typeform.com/to/G21i9lJ2">     </a>  |  <a href="https://docs.openinterpreter.com/"></a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>
<p align="center">
<strong> ' </strong>  <strong><code>--os</code></strong>   <strong>Computer API</strong>. <a href="https://changes.openinterpreter.com/log/the-new-computer-update">  </a>
</p>
<br>

```shell
pip install open-interpreter
```

>  ?   [  ](https://docs.openinterpreter.com/getting-started/setup).

```shell
interpreter
```

<br>

**Open Interpreter**  LLM    (Python, Javascript, Shell ).     Open Interpreter  ,   ChatGPT,   ,  `$ interpreter`  .

          :

-    , , PDF- .
-   Chrome   
- ,      
- ... ..

** :        .**

<br>

## Demo

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

####      Google Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

####     ,  _Her_:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

##  

```shell
pip install open-interpreter
```

### 

    `interpreter`:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Plot AAPL and META's normalized stock prices") #   
interpreter.chat() #   
```

### GitHub Codespaces

  `,`   GitHub  ,   Codespace.         ,     .               ,     .

##     ChatGPT

 OpenAI [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter)  GPT-4         ChatGPT.

  OpenAI  ,       :

-    .
- [    ](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
-    - 100 ,    - 120,0 .
-   (  -    ),   .

---

Open Interpreter   ,     .      ,      ,    -   .

     GPT-4      .

## 

**:**  Generator (0.1.5)   :

```python
message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

###  

      ,  `interpreter`   :

```shell
interpreter
```

 `interpreter.chat()`   .py:

```python
interpreter.chat()
```

**     :**

```python
message = "     ?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

###  

          `.chat(message)`:

```python
interpreter.chat("      /videos.")

# ...     ,   ...

interpreter.chat(" ,      ?")

# ...
```

###   

 Python, Open Interpreter   .     ,    :

```python
interpreter.messages = []
```

###    

`interpreter.chat()`   ,         `interpreter.messages = messages`:

```python
messages = interpreter.chat("  .") #    "messages"
interpreter.messages = [] #   (""  )

interpreter.messages = messages #    "messages" ("" )
```

###   

       Open Interpreter,    ,       .

```python
interpreter.system_message += """
    -y,       .
"""
print(interpreter.system_message)
```

###    

Open Interpreter  [LiteLLM](https://docs.litellm.ai/docs/providers/)      .

   ,   :

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

 Python    :

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[   model     .](https://docs.litellm.ai/docs/providers/)

###  Open Interpreter 

#### 

Open Interpreter   OpenAI-     . (LM Studio, jan.ai, ollama )

  `interpreter`  URL- api_base   interference ( LM Studio  `http://localhost:1234/v1`  ):

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

 ,    Llamafile     ,   

```shell
interpreter --local
```

for a more detailed guide check out [this video by Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

**  LM Studio   .**

1.  [https://lmstudio.ai/](https://lmstudio.ai/),    .
2.     ** **.
3.   ****  ( ).
4.    ,    ** **.

  ,       Open Interpreter.

> **.**     `context_window`  3000,  `max_tokens`  1000.      ,     (. ).

#### Python

  Python       .      LM Studio   :

```python
from interpreter import interpreter

interpreter.offline = True #   -,  Open Procedures
interpreter.llm.model = "openai/x" #  AI     OpenAI
interpreter.llm.api_key = "fake_key" # LiteLLM,       LM Studio,  api-
interpreter.llm.api_base = "http://localhost:1234/v1" #    - ,   OpenAI

interpreter.chat()
```

####  ,   

   `max_tokens`  `context_window` ( )   .

         ,        (~1000),       . ,  `max_tokens`   `context_window`.

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

###  "verbose"

    Open Interpreter,     `--verbose`  .

    "verbose",    (`interpreter --verbose`)    :

```shell
$ interpreter
...
> %verbose true <-   verbose

> %verbose false <-   verbose
```

###   

        ,    .    :
** :**

- `%verbose [true/false]`:   verbose.     `true`.
     .  `false`     .
- `%reset`:    .
- `% undo`:          .
- `%tokens [prompt]`: (__)  ,        ,    .       ,   .   [ `cost_per_token()` LiteLLM](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token)   .
- `%help`:   .

###  / 

Open Interpreter         `yaml`.

     ,      .

  ,    :

```
interpreter --profiles
```

    `yaml`.      `default.yaml`.

####  

Open Interpreter    `yaml`,      :

```
interpreter --profile my_profile.yaml
```

##   FastAPI

    Open Interpreter    HTTP REST:

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

    ,   ,   `interpreter.server()`.

## Android

    Open Interpreter    Android    [ open-interpreter-termux](https://github.com/MikeBirdTech/open-interpreter-termux).

##   

       ,         ,     , -      .

** Open Interpreter      .**

   `interpreter -y`   `interpreter.auto_run = True`,    ,   :

-  ,  ,      .
-   Open Interpreter         ,  .
-   Open Interpreter   ,  Google Colab  Replit.    ,      .

 ****  [ ](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/SAFE_MODE.md),    .

##   ?

Open Interpreter  [   ](https://platform.openai.com/docs/guides/gpt/function-calling)  `exec()`,   `` ( "Python"  "JavaScript")  `code`  .

    ,         Markdown.

#      

 [](https://docs.openinterpreter.com/)       .

[Node](https://nodejs.org/en)   :

-  18.17.0  -   18.x.x.
-  20.3.0  -   20.x.x.
- - ,   21.0.0  ,    .

 [Mintlify](https://mintlify.com/):

```bash
npm i -g mintlify@latest
```

       :

```bash
#       
cd ./docs

#   
mintlify dev
```

    .      [http://localhost:3000](http://localhost:3000),    .

# 

     !    .

    ,   ,    [   ](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md).

#  

 [  ](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md),    Open Interpreter.

****:       OpenAI.

![thumbnail-ncu](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/1b19a5db-b486-41fd-a7a1-fe2028031686)

>    ,       ...         ,       .
>
>  _OpenAI's Code Interpreter Release_

<br>

================
File: README_VN.md
================
<h1 align="center"> Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Espaol-white.svg" alt="ES doc"/></a>
    <a href="docs/README_JA.md"><img src="https://img.shields.io/badge/--white.svg" alt="JA doc"/></a>
    <a href="docs/README_ZH.md"><img src="https://img.shields.io/badge/--white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/-white.svg" alt="UK doc"/></a>
    <a href="docs/README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <b>chy m hnh ngn ng tr tu nhn to trn my tnh ca bn.</b><br>
    M ngun m v ng dng pht trin da trn code ca OpenAI.<br>
    <br><a href="https://openinterpreter.com">Quyn truy cp sm dnh cho my tnh c nhn</a>  |  <b><a href="https://docs.openinterpreter.com/">Ti liu c tham kho</a></b><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter** Chy LLMs trn my tnh cc b (C th s dng ngn ng Python, Javascript, Shell, v nhiu hn th). Bn c th ni chuyn vi Open Interpreter thng qua giao din ging vi ChatGPT ngay trn terminal ca bn bng cch chy lnh `$ interpreter` sau khi ti thnh cng.

Cc tnh nng chung giao din ngn ng mang lli

- To v chnh sa nh, videos, PDF, vn vn...
- iu khin trnh duyt Chrome  tin hnh nghin cu
- V, lm sch v phn tch cc tp d liu ln (large datasets)
- ...vn vn.

** Lu : Bn s c yu cu ph duyt m trc khi chy.**

<br>

## Th nghim

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Bn th nghim c sn trn Google Colab:

[![M trong Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

#### i km vi ng dng mu qua tng tc ging ni (Ly cm hng t _C y_ (Ging n)):

[![M trong Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

## Hng dn khi dng ngn

```shell
pip install open-interpreter
```

### Terminal

Sau khi ci t, chy dng lnh `interpreter`:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("V gi c phiu  bnh ho ca AAPL v META ") # Chy trn 1 dng lnh
interpreter.chat() # Khi ng chat c kh nng tng tc
```

## So snh Code Interpreter ca ChatGPT

Bn pht hnh ca OpenAI [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter) s dng GPT-4 tng kh nng hon thin vn  thc tin vi ChatGPT.

Tuy nhin, dch v ca OpenAI c lu tr, m ngun ng, v rt hn ch:

- Khng c truy cp Internet.
- [S lng gi ci t h tr c sn gii hn](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
- tc  ti ti a 100 MB , thi gian chy gii hn 120.0 giy .
- Trng thi tin nhn b xo km vi cc tp v lin kt c to trc  khi ng mi trng li.

---

Open Interpreter khc phc nhng hn ch ny bng cch chy cc b trob mi trng my tnh ca bn. N c ton quyn truy cp vo Internet, khng b hn ch v thi gian hoc kch thc tp v c th s dng bt k gi hoc th vin no.

y l s kt hp sc mnh ca m ngun ca GPT-4 vi tnh linh hot ca mi trng pht trin cc b ca bn.

## Dng lnh

**Update:** Cp nht trnh to lnh (0.1.5) gii thiu tnh nng trc tuyn:

```python
message = "Chng ta ang  trn h iu hnh no?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### Tr chuyn tng tc

 to mt cuc tr chuyn tng tc t terminal ca bn, chy `interpreter` bng dng lnh:

```shell
interpreter
```

hoc `interpreter.chat()` t file c ui .py :

```python
interpreter.chat()
```

**Bn cng c th pht trc tuyn tng on:**

```python
message = "Chng ta ang chy trn h iu hnh no?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### Tr chuyn lp trnh c

 kim sot tt hn, bn chuyn tin nhn qua `.chat(message)`:

```python
interpreter.chat("Truyn ph  ti tt c videos vo /videos.")

# ... Truyn u ra n thit b u cui ca bn (terminal) hon thnh tc v ...

interpreter.chat("Nhn p y nhng bn c th lm cho ph  ln hn c khng?")

# ...
```

### To mt cuc tr chuyn mi:

Trong Python, Open Interpreter ghi nh lch s hi thoi, nu mun bt u li t u, bn c th ci th:

```python
interpreter.messages = []
```

### Lu v khi phc cuc tr chuyn

`interpreter.chat()` tr v danh sch tin nhn, c th c s dng  tip tc cuc tr chuyn vi `interpreter.messages = messages`:

```python
messages = interpreter.chat("Tn ca ti l Killian.") # Lu tin nhn ti 'messages'
interpreter.messages = [] # Khi ng li trnh phin dch ("Killian" s b lng qun)

interpreter.messages = messages # Tip tc cuc tr chuyn t 'messages' ("Killian" s c ghi nh)
```

### C nhn ho tin nhn t h thng

Bn c th kim tra v iu chnh tin nhn h thng t Opt Interpreter  m rng chc nng ca n, thay i quyn, hoc a cho n nhiu ng cnh hn.

```python
interpreter.system_message += """
Chy shell commands vi -y  ngi dng khng phi xc nhn chng.
"""
print(interpreter.system_message)
```

### Thay i m hnh ngn ng

Open Interpreter s dng m hnh [LiteLLM](https://docs.litellm.ai/docs/providers/)  kt ni ti cc m hnh ngn ng c lu tr trc .

Bn c th thay i m hnh ngn ng bng cch thay i tham s m hnh:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

 trong Python, i model bng cch thay i i tng:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Tm tn chui "m hnh" ph hp cho m hnh ngn ng ca bn  y.](https://docs.litellm.ai/docs/providers/)

### Chy Open Interpreter trn my cc b

Open Interpreter c th s dng my ch tng thch vi OpenAI  chy cc m hnh cc b. (LM Studio, jan.ai, ollama, v.v.)

Ch cn chy `interpreter` vi URL api_base ca my ch suy lun ca bn (i vi LM studio, n l `http://localhost:1234/v1` theo mc nh):

```v
trnh thng dch --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

Ngoi ra, bn c th s dng Llamafile m khng cn ci t bt k phn mm bn th ba no ch bng cch chy

```v
thng dch vin --local
```

 bit hng dn chi tit hn, hy xem [video ny ca Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

** chy LM Studio  ch  nn.**

1. Ti [https://lmstudio.ai/](https://lmstudio.ai/) v khi ng.
2. Chn mt m hnh ri nhn ** Download**.
3. Nhn vo nt ****  bn tri (di ).
4. Chn m hnh ca bn  pha trn, ri nhn chy **Start Server**.

Mt khi server chy, bn c th bt u tr chuyn vi Open Interpreter.

> **Lu :** Ch  cc b chnh `context_window` ca bn ti 3000, v `max_tokens` ca bn ti 600. Nu m hnh ca bn c cc yu cu khc, th hy chnh cc tham s th cng (xem bn di).

#### Ca s ng cnh (Context Window), (Max Tokens)

Bn c th thay i `max_tokens` v `context_window` ( trong cc) of locally running models.

 ch  cc b, cc ca s ng cnh s tiu t RAM hn, vy nn chng ti khuyn khch dng ca s nh hn (~1000) nu nh n chy khng n nh / hoc nu n chm. m bo rng `max_tokens` t hn `context_window`.

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

### Ch  sa li

 gip ng gp kim tra Open Interpreter, th ch  `--verbose` hi di dng.

Bn c th khi ng ch  sa li bng cch s dng c (`interpreter --verbose`), hoc mid-chat:

```shell
$ interpreter
...
> %verbose true <- Khi ng ch  g li

> %verbose false <- Tt ch  g li
```

### Lnh ch  tng tc

Trong ch  tng tc, bn c th s dng nhng dng lnh sau  ci thin tri nghim ca mnh. y l danh sch cc dng lnh c sn:

**Cc lnh c sn:**

- `%verbose [true/false]`: Bt ch  g li. C hay khng c `true` u khi ng ch  g li. Vi `false` th n tt ch  g li.
- `%reset`: Khi ng li ton b phin tr chuyn hin ti.
- `%undo`: Xa tin nhn ca ngi dng trc  v phn hi ca AI khi lch s tin nhn.
- `%save_message [path]`: Lu tin nhn vo mt ng dn JSON c xc nh t trc. Nu khng c ng dn no c cung cp, n s mc nh l `messages.json`.
- `%load_message [path]`: Ti tin nhn t mt ng dn JSON c ch nh. Nu khng c ng dn no c cung cp, n s mc nh l `messages.json`.
- `%tokens [prompt]`: (_Experimental_) Tnh ton cc token s c gi cng vi li nhc tip theo di dng ng cnh v hao tn. Ty chn tnh ton token v hao tn c tnh ca mt `prompt` nu c cung cp. Da vo [hm `cost_per_token()` ca m hnh LiteLLM](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token)  tnh ton hao tn.
- `%help`: Hin ln tr gip cho cuc tr chuyn.

### Cu hnh ci

Open Interpreter cho php bn thit lp cc tc v mc nh bng cch s dng file `config.yaml`.

iu ny cung cp mt cch linh hot  nh cu hnh trnh thng dch m khng cn thay i i s dng lnh mi ln

Chy lnh sau  m tp cu hnh:

```
interpreter --config
```

#### Cu hnh cho nhiu tp

Open Interpreter h tr nhiu file `config.yaml`, cho php bn d dng chuyn i gia cc cu hnh thng qua lnh `--config_file`.

**Ch **: `--config_file` chp nhn tn tp hoc ng dn tp. Tn tp s s dng th mc cu hnh mc nh, trong khi ng dn tp s s dng ng dn  ch nh.

 to hoc chnh sa cu hnh mi, hy chy:

```
interpreter --config --config_file $config_path
```

 yu cu Open Interpreter chy mt tp cu hnh c th, hy chy:

```
interpreter --config_file $config_path
```

**Ch **: Thay i `$config_path` vi tn hoc ng dn n tp cu hnh ca bn.

##### V d CLI

1. To mi mt file `config.turbo.yaml`
   ```
   interpreter --config --config_file config.turbo.yaml
   ```
2. Chy file `config.turbo.yaml` t li `model` thnh `gpt-3.5-turbo`
3. Chy Open Interpreter vi cu hnh `config.turbo.yaml
   ```
   interpreter --config_file config.turbo.yaml
   ```

##### V d Python

Bn cng c th ti cc tp cu hnh khi gi Open Interpreter t tp lnh Python:

```python
import os
from interpreter import interpreter

currentPath = os.path.dirname(os.path.abspath(__file__))
config_path=os.path.join(currentPath, './config.test.yaml')

interpreter.extend_config(config_path=config_path)

message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

## My ch FastAPI mu

Bn cp nht trnh to cho php iu khin Trnh thng dch m thng qua cc im cui HTTP REST:

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

## Hng dn an ton

V m c to c thc thi trong mi trng cc b ca bn nn n c th tng tc vi cc tp v ci t h thng ca bn, c kh nng dn n cc kt qu khng mong mun nh mt d liu hoc ri ro bo mt.

** Open Interpreter s yu cu xc nhn ca ngi dng trc khi chy code.**

Bn c th chy `interpreter -y` hoc t `interpreter.auto_run = True`  b qua xc nhn ny, trong trng hp :

- Hy thn trng khi yu cu cc lnh sa i tp hoc ci t h thng.
- Theo di Open Interpreter ging nh mt chic  t t li v sn sng kt thc qu trnh bng cch ng terminal ca bn.
- Cn nhc vic chy Open Interpreter trong mi trng b hn ch nh Google Colab hoc Replit. Nhng mi trng ny bit lp hn, gim thiu ri ro khi chy code ty .

y l h tr **th nghim** cho [ch  an ton](docs/SAFE_MODE.md) gip gim thiu ri ro.

## N hot ng th no?

Open Interpreter trang b [m hnh ngn ng gi hm](https://platform.openai.com/docs/guides/gpt/function-calling) vi mt hm `exec()`, chp nhn mt `language` (nh "Python" hoc "JavaScript") v `code`  chy.

Sau , chng ti truyn trc tuyn thng bo, m ca m hnh v kt qu u ra ca h thng ca bn n terminal di dng Markdown.

# ng gp

Cm n bn  quan tm ng gp! Chng ti hoan nghnh s tham gia ca cng ng.

Vui lng xem [Hng dn ng gp](CONTRIBUTING.md)  bit thm chi tit cch tham gia.

## Giy php

Open Interpreter c cp php theo Giy php MIT. Bn c php s dng, sao chp, sa i, phn phi, cp php li v bn cc bn sao ca phn mm.

**Lu **: Phn mm ny khng lin kt vi OpenAI.

> C quyn truy cp vo mt lp trnh vin cp di lm vic nhanh chng trong tm tay bn ... c th khin quy trnh lm vic mi tr nn d dng v hiu qu, cng nh m ra nhng li ch ca vic lp trnh cho ngi mi.
>
>  _Pht hnh trnh thng dch m ca OpenAI_

<br>

================
File: README_ZH.md
================
<h1 align="center"> Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm"><img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/--white.svg" alt="JA doc"></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Espaol-white.svg" alt="ES doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/-white.svg" alt="UK doc"/></a>
    <a href="README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <b></b><br>
    OpenAI<br>
    <br><a href="https://openinterpreter.com">Open Interpreter</a>  |  <b><a href="https://docs.openinterpreter.com/"></a></b><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter** LLMs PythonJavaScriptShell  `$ interpreter`  ChatGPT  Open Interpreter 



- PDF 
-  Chrome 
- 
- ...

** **

<br>

## 

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Google Colab 

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

## 

```shell
pip install open-interpreter
```

### 

 `interpreter`

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Plot AAPL and META's normalized stock prices") # 
interpreter.chat() # 
```

##  ChatGPT 

OpenAI  [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter)  GPT-4  ChatGPT 

OpenAI 

- 
- [](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/)
-  100 MB 120.0 
- 

---

Open Interpreter

 GPT-4 

## 

### 

 `interpreter`

```shell
interpreter
```

.py  `interpreter.chat()`

```python
interpreter.chat()
```

### 

 `.chat(message)`  

```python
interpreter.chat("Add subtitles to all videos in /videos.")

# ... Streams output to your terminal, completes task ...

interpreter.chat("These look great but can you make the subtitles bigger?")

# ...
```

### 

 Python Open Interpreter 

```python
interpreter.messages = []
```

### 

```python
messages = interpreter.chat("My name is Killian.") #  'messages'
interpreter.messages = [] #  ("Killian" )

interpreter.messages = messages #  'messages'  ("Killian" )
```

### 

 Open Interpreter 

```python
interpreter.system_message += """
 -y  shell 
"""
print(interpreter.system_message)
```

### 

Open Interpreter [LiteLLM](https://docs.litellm.ai/docs/providers/)



```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

 Python 

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

###  Open Interpreter

```shell
interpreter --local
```

### 

 Open Interpreter`--verbose` 

 `interpreter --verbose` 

```shell
$ interpreter
...
> %verbose true <- 

> %verbose false <- 
```

## 



** Open Interpreter **

 `interpreter -y`  `interpreter.auto_run = True` 

- 
-  Open Interpreter
-  Google Colab  Replit  Open Interpreter 

## 

Open Interpreter [](https://platform.openai.com/docs/guides/gpt/function-calling) `exec()`  `` "Python " "JavaScript" ``

 Markdown 

# 



 [](CONTRIBUTING.md)

## 

 Open Interpreter [](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md) 

**** OpenAI 

![thumbnail-ncu](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/1b19a5db-b486-41fd-a7a1-fe2028031686)

================
File: ROADMAP.md
================
# Roadmap

## Documentation
- [ ] Work with Mintlify to translate docs. How does Mintlify let us translate our documentation automatically? I know there's a way.
- [ ] Better comments throughout the package (they're like docs for contributors)
- [ ] Show how to replace interpreter.llm so you can use a custom llm

## New features
- [ ] Figure out how to get OI to answer to user input requests like python's `input()`. Do we somehow detect a delay in the output..? Is there some universal flag that TUIs emit when they expect user input? Should we do this semantically with embeddings, then ask OI to review it and respond..?
- [ ] Placeholder text that gives a compelling example OI request. Probably use `textual`
- [ ] Everything else `textual` offers, like could we make it easier to select text? Copy paste in and out? Code editing interface?
- [x] Let people turn off the active line highlighting
- [ ] Add a --plain flag which doesn't use rich, just prints stuff in plain text
- [ ] Use iPython stuff to track the active line, instead of inserting print statements, which makes debugging weird (From ChatGPT: For deeper insights into what's happening behind the scenes, including which line of code is being executed, you can increase the logging level of the IPython kernel. You can configure the kernel's logger to a more verbose setting, which logs each execution request. However, this requires modifying the kernel's startup settings, which might involve changing logging configurations in the IPython kernel source or when launching the kernel.)
- [ ] Let people edit the code OI writes. Could just open it in the user's preferred editor. Simple. [Full description of how to implement this here.](https://github.com/OpenInterpreter/open-interpreter/pull/830#issuecomment-1854989795)
- [ ] Display images in the terminal interface
- [ ] There should be a function that just renders messages to the terminal, so we can revive conversation navigator, and let people look at their conversations
- [ ] ^ This function should also render the last like 5 messages once input() is about to be run, so we don't get those weird stuttering `rich` artifacts
- [ ] Let OI use OI, add `interpreter.chat(async=True)` bool. OI can use this to open OI on a new thread
  - [ ] Also add `interpreter.await()` which waits for `interpreter.running` (?) to = False, and `interpreter.result()` which returns the last assistant messages content.
- [ ] Allow for limited functions (`interpreter.functions`) using regex
  - [ ] If `interpreter.functions != []`:
    - [ ] set `interpreter.computer.languages` to only use Python
    - [ ] Use regex to ensure the output of code blocks conforms to just using those functions + other python basics
- [ ] (Maybe) Allow for a custom embedding function (`interpreter.embed` or `computer.ai.embed`) which will let us do semantic search
- [ ] (Maybe) if a git is detected, switch to a mode that's good for developers, like showing nested file structure in dynamic system message, searching for relevant functions (use computer.files.search)
- [x] Allow for integrations somehow (you can replace interpreter.llm.completions with a wrapped completions endpoint for any kind of logging. need to document this tho)
  - [ ] Document this^
- [ ] Expand "safe mode" to have proper, simple Docker support, or maybe Cosmopolitan LibC
- [ ] Make it so core can be run elsewhere from terminal package  perhaps split over HTTP (this would make docker easier too)
- [ ] For OS mode, experiment with screenshot just returning active window, experiment with it just showing the changes, or showing changes in addition to the whole thing, etc. GAIA should be your guide

## Future-proofing

- [ ] Really good tests / optimization framework, to be run less frequently than Github actions tests
  - [x] Figure out how to run us on [GAIA](https://huggingface.co/gaia-benchmark)
    - [x] How do we just get the questions out of this thing?
    - [x] How do we assess whether or not OI has solved the task?
  - [ ] Loop over GAIA, use a different language model every time (use Replicate, then ask LiteLLM how they made their "mega key" to many different LLM providers)
  - [ ] Loop over that  using a different prompt each time. Which prompt is best across all LLMs?
  - [ ] (For the NCU) might be good to use a Google VM with a display
  - [ ] (Future future) Use GPT-4 to assess each result, explaining each failure. Summarize. Send it all to GPT-4 + our prompt. Let it redesign the prompt, given the failures, rinse and repeat
- [ ] Stateless (as in, doesn't use the application directory) core python package. All `appdir` or `platformdirs` stuff should be only for the TUI
  - [ ] `interpreter.__dict__` = a dict derived from config is how the python package should be set, and this should be from the TUI. `interpreter` should not know about the config
  - [ ] Move conversation storage out of the core and into the TUI. When we exit or error, save messages same as core currently does
- [ ] Further split TUI from core (some utils still reach across)
- [ ] Better storage of different model keys in TUI / config file. All keys, to multiple providers, should be stored in there. Easy switching
  - [ ] Automatically migrate users from old config to new config, display a message of this
- [ ] On update, check for new system message and ask user to overwrite theirs, or only let users pass in "custom instructions" which adds to our system message
  - [ ] I think we could have a config that's like... system_message_version. If system_message_version is below the current version, ask the user if we can overwrite it with the default config system message of that version. (This somewhat exists now but needs to be robust)

# What's in our scope?

Open Interpreter contains two projects which support each other, whose scopes are as follows:

1. `core`, which is dedicated to figuring out how to get LLMs to safely control a computer. Right now, this means creating a real-time code execution environment that language models can operate.
2. `terminal_interface`, a text-only way for users to direct the code-running LLM running inside `core`. This includes functions for connecting the `core` to various local and hosted LLMs (which the `core` itself should not know about).

# What's not in our scope?

Our guiding philosophy is minimalism, so we have also decided to explicitly consider the following as **out of scope**:

1. Additional functions in `core` beyond running code.
2. More complex interactions with the LLM in `terminal_interface` beyond text (but file paths to more complex inputs, like images or video, can be included in that text).

---

This roadmap gets pretty rough from here. More like working notes.

# Working Notes

## \* Roughly, how to build `computer.browser`:

First I think we should have a part, like `computer.browser.ask(query)` which just hits up [perplexity](https://www.perplexity.ai/) for fast answers to questions.

Then we want these sorts of things:

- `browser.open(url)`
- `browser.screenshot()`
- `browser.click()`

It should actually be based closely on Selenium. Copy their API so the LLM knows it.

Other than that, basically should be = to the computer module itself, at least the IO / keyboard and mouse parts.

However, for non vision models, `browser.screenshot()` can return the accessibility tree, not an image. And for `browser.click(some text)` we can use the HTML to find that text.

**Here's how GPT suggests we implement the first steps of this:**

Creating a Python script that automates the opening of Chrome with the necessary flags and then interacts with it to navigate to a URL and retrieve the accessibility tree involves a few steps. Here's a comprehensive approach:

1. **Script to Launch Chrome with Remote Debugging**:

   - This script will start Chrome with the `--remote-debugging-port=9222` flag.
   - It will handle different platforms (Windows, macOS, Linux).

2. **Python Script for Automation**:
   - This script uses `pychrome` to connect to the Chrome instance, navigate to a URL, and retrieve the accessibility tree.

### Step 1: Launching Chrome with Remote Debugging

You'll need a script to launch Chrome. This script varies based on the operating system. Below is an example for Windows. You can adapt it for macOS or Linux by changing the path and command to start Chrome.

```python
import subprocess
import sys
import os

def launch_chrome():
    chrome_path = "C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe"  # Update this path for your system
    url = "http://localhost:9222/json/version"
    subprocess.Popen([chrome_path, '--remote-debugging-port=9222'], shell=True)
    print("Chrome launched with remote debugging on port 9222.")

if __name__ == "__main__":
    launch_chrome()
```

### Step 2: Python Script to Navigate and Retrieve Accessibility Tree

Next, you'll use `pychrome` to connect to this Chrome instance. Ensure you've installed `pychrome`:

```bash
pip install pychrome
```

Here's the Python script:

```python
import pychrome
import time

def get_accessibility_tree(tab):
    # Enable the Accessibility domain
    tab.call_method("Accessibility.enable")

    # Get the accessibility tree
    tree = tab.call_method("Accessibility.getFullAXTree")
    return tree

def main():
    # Create a browser instance
    browser = pychrome.Browser(url="http://127.0.0.1:9222")

    # Create a new tab
    tab = browser.new_tab()

    # Start the tab
    tab.start()

    # Navigate to a URL
    tab.set_url("https://www.example.com")
    time.sleep(3)  # Wait for page to load

    # Retrieve the accessibility tree
    accessibility_tree = get_accessibility_tree(tab)
    print(accessibility_tree)

    # Stop the tab (closes it)
    tab.stop()

    # Close the browser
    browser.close()

if __name__ == "__main__":
    main()
```

This script will launch Chrome, connect to it, navigate to "https://www.example.com", and then print the accessibility tree to the console.

**Note**: The script to launch Chrome assumes a typical installation path on Windows. You will need to modify this path according to your Chrome installation location and operating system. Additionally, handling different operating systems requires conditional checks and respective commands for each OS.

================
File: SAFE_MODE.md
================
# Safe Mode

** Safe mode is experimental and does not provide any guarantees of safety or security.**

Open Interpreter is working on providing an experimental safety toolkit to help you feel more confident running the code generated by Open Interpreter.

Install Open Interpreter with the safety toolkit dependencies as part of the bundle:

```shell
pip install open-interpreter[safe]
```

Alternatively, you can install the safety toolkit dependencies separately in your virtual environment:

```shell
pip install semgrep
```

## Features

- **No Auto Run**: Safe mode disables the ability to automatically execute code
- **Code Scanning**: Scan generated code for vulnerabilities with [`semgrep`](https://semgrep.dev/)

## Enabling Safe Mode

You can enable safe mode by passing the `--safe` flag when invoking `interpreter` or by configuring `safe_mode` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration).

The safe mode setting has three options:

- `off`: disables the safety toolkit (_default_)
- `ask`: prompts you to confirm that you want to scan code
- `auto`: automatically scans code

### Example Config:

```yaml
model: gpt-4
temperature: 0
verbose: false
safe_mode: ask
```

## Roadmap

Some upcoming features that enable even more safety:

- [Execute code in containers](https://github.com/OpenInterpreter/open-interpreter/pull/459)

## Tips & Tricks

You can adjust the `system_message` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration) to include instructions for the model to scan packages with [`guarddog`]() before installing them.

```yaml
model: gpt-4
verbose: false
safe_mode: ask
system_message: |
  # normal system message here
  BEFORE INSTALLING ANY PACKAGES WITH pip OR npm YOU MUST SCAN THEM WITH `guarddog` FIRST. Run `guarddog pypi scan $package` for pip packages and `guarddog npm scan $package` for npm packages. `guarddog` only accepts one package name at a time.
```

================
File: SECURITY.md
================
# Open Interpreter Security Policy

We take security seriously. Responsible reporting and disclosure of security
vulnerabilities is important for the protection and privacy of our users. If you
discover any security vulnerabilities, please follow these guidelines.

Published security advisories are available on our [GitHub Security Advisories]
page.

To report a vulnerability, please draft a [new security advisory on GitHub]. Any
fields that you are unsure of or don't understand can be left at their default
values. The important part is that the vulnerability is reported. Once the
security advisory draft has been created, we will validate the vulnerability and
coordinate with you to fix it, release a patch, and responsibly disclose the
vulnerability to the public. Read GitHub's documentation on [privately reporting
a security vulnerability] for details.

Please do not report undisclosed vulnerabilities on public sites or forums,
including GitHub issues and pull requests. Reporting vulnerabilities to the
public could allow attackers to exploit vulnerable applications before we have
been able to release a patch and before applications have had time to install
the patch. Once we have released a patch and sufficient time has passed for
applications to install the patch, we will disclose the vulnerability to the
public, at which time you will be free to publish details of the vulnerability
on public sites and forums.

If you have a fix for a security vulnerability, please do not submit a GitHub
pull request. Instead, report the vulnerability as described in this policy.
Once we have verified the vulnerability, we can create a [temporary private
fork] to collaborate on a patch.

We appreciate your cooperation in helping keep our users safe by following this
policy.

[github security advisories]: https://github.com/OpenInterpreter/open-interpreter/security/advisories
[new security advisory on github]: https://github.com/OpenInterpreter/open-interpreter/security/advisories/new
[privately reporting a security vulnerability]: https://docs.github.com/en/code-security/security-advisories/guidance-on-reporting-and-writing/privately-reporting-a-security-vulnerability
[temporary private fork]: https://docs.github.com/en/code-security/security-advisories/repository-security-advisories/collaborating-in-a-temporary-private-fork-to-resolve-a-repository-security-vulnerability

================
File: style.css
================
.rounded-lg {
    border-radius: 0;
}

/*

.rounded-sm, .rounded-md, .rounded-lg, .rounded-xl, .rounded-2xl, .rounded-3xl {
    border-radius: 0.125rem;
}

.rounded-full {
    border-radius: 0.125rem;
}

*/

.font-extrabold {
    font-weight: 600;
}

.h1, .h2, .h3, .h4, .h5, .h6 {
    font-weight: 600;
}

.body {
    font-weight: normal;
}
