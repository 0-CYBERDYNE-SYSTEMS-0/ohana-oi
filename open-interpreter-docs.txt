This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-25T22:29:07.996Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
code-execution/
  computer-api.mdx
  custom-languages.mdx
  settings.mdx
  usage.mdx
computer/
  introduction.mdx
  language-model-usage.mdx
  user-usage.mdx
getting-started/
  introduction.mdx
  setup.mdx
guides/
  advanced-terminal-usage.mdx
  basic-usage.mdx
  demos.mdx
  multiple-instances.mdx
  os-mode.mdx
  profiles.mdx
  running-locally.mdx
  streaming-response.mdx
integrations/
  docker.mdx
  e2b.mdx
language-models/
  hosted-models/
    ai21.mdx
    anthropic.mdx
    anyscale.mdx
    aws-sagemaker.mdx
    azure.mdx
    baseten.mdx
    cloudflare.mdx
    cohere.mdx
    deepinfra.mdx
    gpt-4-setup.mdx
    huggingface.mdx
    mistral-api.mdx
    nlp-cloud.mdx
    openai.mdx
    openrouter.mdx
    palm.mdx
    perplexity.mdx
    petals.mdx
    replicate.mdx
    togetherai.mdx
    vertex-ai.mdx
    vllm.mdx
  local-models/
    best-practices.mdx
    custom-endpoint.mdx
    janai.mdx
    llamafile.mdx
    lm-studio.mdx
    ollama.mdx
  custom-models.mdx
  introduction.mdx
  settings.mdx
legal/
  license.mdx
protocols/
  lmc-messages.mdx
safety/
  best-practices.mdx
  introduction.mdx
  isolation.mdx
  safe-mode.mdx
server/
  usage.mdx
settings/
  all-settings.mdx
  example-profiles.mdx
  profiles.mdx
telemetry/
  telemetry.mdx
troubleshooting/
  faq.mdx
usage/
  desktop/
    help.md
    install.mdx
  python/
    arguments.mdx
    budget-manager.mdx
    conversation-history.mdx
    magic-commands.mdx
    multiple-instances.mdx
    settings.mdx
  terminal/
    arguments.mdx
    budget-manager.mdx
    magic-commands.mdx
    settings.mdx
    vision.mdx
  examples.mdx
CONTRIBUTING.md
mint.json
NCU_MIGRATION_GUIDE.md
README_DE.md
README_ES.md
README_IN.md
README_JA.md
README_UK.md
README_VN.md
README_ZH.md
ROADMAP.md
SAFE_MODE.md
SECURITY.md
style.css

================================================================
Repository Files
================================================================

================
File: code-execution/computer-api.mdx
================
---
title: Computer API
---

The following functions are designed for language models to use in Open Interpreter, currently only supported in [OS Mode](/guides/os-mode/).

### Display - View

Takes a screenshot of the primary display.



```python
interpreter.computer.display.view()
```



### Display - Center

Gets the x, y value of the center of the screen.



```python
x, y = interpreter.computer.display.center()
```



### Keyboard - Hotkey

Performs a hotkey on the computer



```python
interpreter.computer.keyboard.hotkey(" ", "command")
```



### Keyboard - Write

Writes the text into the currently focused window.



```python
interpreter.computer.keyboard.write("hello")
```



### Mouse - Click

Clicks on the specified coordinates, or an icon, or text. If text is specified, OCR will be run on the screenshot to find the text coordinates and click on it.



```python
# Click on coordinates
interpreter.computer.mouse.click(x=100, y=100)

# Click on text on the screen
interpreter.computer.mouse.click("Onscreen Text")

# Click on a gear icon
interpreter.computer.mouse.click(icon="gear icon")
```



### Mouse - Move

Moves to the specified coordinates, or an icon, or text. If text is specified, OCR will be run on the screenshot to find the text coordinates and move to it.



```python
# Click on coordinates
interpreter.computer.mouse.move(x=100, y=100)

# Click on text on the screen
interpreter.computer.mouse.move("Onscreen Text")

# Click on a gear icon
interpreter.computer.mouse.move(icon="gear icon")
```



### Mouse - Scroll

Scrolls the mouse a specified number of pixels.



```python
# Scroll Down
interpreter.computer.mouse.scroll(-10)

# Scroll Up
interpreter.computer.mouse.scroll(10)
```



### Clipboard - View

Returns the contents of the clipboard.



```python
interpreter.computer.clipboard.view()
```



### OS - Get Selected Text

Get the selected text on the screen.



```python
interpreter.computer.os.get_selected_text()
```



### Mail - Get

Retrieves the last `number` emails from the inbox, optionally filtering for only unread emails. (Mac only)



```python
interpreter.computer.mail.get(number=10, unread=True)
```



### Mail - Send

Sends an email with the given parameters using the default mail app. (Mac only)



```python
interpreter.computer.mail.send("john@email.com", "Subject", "Body", ["path/to/attachment.pdf", "path/to/attachment2.pdf"])
```



### Mail - Unread Count

Retrieves the count of unread emails in the inbox. (Mac only)



```python
interpreter.computer.mail.unread_count()
```



### SMS - Send

Send a text message using the default SMS app. (Mac only)



```python
interpreter.computer.sms.send("2068675309", "Hello from Open Interpreter!")
```



### Contacts - Get Phone Number

Returns the phone number of a contact name. (Mac only)



```python
interpreter.computer.contacts.get_phone_number("John Doe")
```



### Contacts - Get Email Address

Returns the email of a contact name. (Mac only)



```python
interpreter.computer.contacts.get_phone_number("John Doe")
```



### Calendar - Get Events

Fetches calendar events for the given date or date range from all calendars. (Mac only)



```python
interpreter.computer.calendar.get_events(start_date=datetime, end_date=datetime)
```



### Calendar - Create Event

Creates a new calendar event. Uses first calendar if none is specified (Mac only)



```python
interpreter.computer.calendar.create_event(title="Title", start_date=datetime, end_date=datetime, location="Location", notes="Notes", calendar="Work")
```



### Calendar - Delete Event

Delete a specific calendar event. (Mac only)



```python
interpreter.computer.calendar.delete_event(event_title="Title", start_date=datetime, calendar="Work")
```

================
File: code-execution/custom-languages.mdx
================
---
title: Custom Languages
---

You can add or edit the programming languages that Open Interpreter's computer runs.

In this example, we'll swap out the `python` language for a version of `python` that runs in the cloud. We'll use `E2B` to do this.

([`E2B`](https://e2b.dev/) is a secure, sandboxed environment where you can run arbitrary code.)

First, [get an API key here](https://e2b.dev/), and set it:

```python
import os
os.environ["E2B_API_KEY"] = "<your_api_key_here>"
```

Then, define a custom language for Open Interpreter. The class name doesn't matter, but we'll call it `PythonE2B`:

```python
import e2b

class PythonE2B:
    """
    This class contains all requirements for being a custom language in Open Interpreter:

    - name (an attribute)
    - run (a method)
    - stop (a method)
    - terminate (a method)

    You can use this class to run any language you know how to run, or edit any of the official languages (which also conform to this class).

    Here, we'll use E2B to power the `run` method.
    """

    # This is the name that will appear to the LLM.
    name = "python"

    # Optionally, you can append some information about this language to the system message:
    system_message = "# Follow this rule: Every Python code block MUST contain at least one print statement."

    # (E2B isn't a Jupyter Notebook, so we added ^ this so it would print things,
    # instead of putting variables at the end of code blocks, which is a Jupyter thing.)

    def run(self, code):
        """Generator that yields a dictionary in LMC Format."""

        # Run the code on E2B
        stdout, stderr = e2b.run_code('Python3', code)

        # Yield the output
        yield {
            "type": "console", "format": "output",
            "content": stdout + stderr # We combined these arbitrarily. Yield anything you'd like!
        }

    def stop(self):
        """Stops the code."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

    def terminate(self):
        """Terminates the entire process."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

# (Tip: Do this before adding/removing languages, otherwise OI might retain the state of previous languages:)
interpreter.computer.terminate()

# Give Open Interpreter its languages. This will only let it run PythonE2B:
interpreter.computer.languages = [PythonE2B]

# Try it out!
interpreter.chat("What's 349808*38490739?")
```

================
File: code-execution/settings.mdx
================
---
title: Settings
---

The `interpreter.computer` is responsible for executing code.

[Click here](https://docs.openinterpreter.com/settings/all-settings#computer) to view `interpreter.computer` settings.

================
File: code-execution/usage.mdx
================
---
title: Usage
---

# Running Code

The `computer` itself is separate from Open Interpreter's core, so you can run it independently:

```python
from interpreter import interpreter

interpreter.computer.run("python", "print('Hello World!')")
```

This runs in the same Python instance that interpreter uses, so you can define functions, variables, or log in to services before the AI starts running code:

```python
interpreter.computer.run("python", "import replicate\nreplicate.api_key='...'")

interpreter.custom_instructions = "Replicate has already been imported."

interpreter.chat("Please generate an image on replicate...") # Interpreter will be logged into Replicate
```

# Custom Languages

You also have control over the `computer`'s languages (like Python, Javascript, and Shell), and can easily append custom languages:

<Card
  title="Custom Languages"
  icon="code"
  iconType="solid"
  href="/code-execution/custom-languages/"
>
  Add or customize the programming languages that Open Interpreter can use.
</Card>

================
File: computer/introduction.mdx
================
The Computer module is responsible for executing code.

You can manually execute code in the same instance that Open Interpreter uses:

```

```

User Usage

It also comes with a suite of modules that we think are particularly useful to code interpreting LLMs.

LLM Usage

================
File: computer/language-model-usage.mdx
================
Open Interpreter can use the Computer module itself.

Here's what it can do:

================
File: computer/user-usage.mdx
================
The Computer module is responsible for running code.

You can add custom languages to it.

The user can add custom languages to the Computer, and .run code on it.

================
File: getting-started/introduction.mdx
================
---
title: Introduction
description: A new way to use computers
---

# <div class="hidden">Introduction</div>

<img src="https://openinterpreter.com/assets/banner.jpg" alt="thumbnail" style={{transform: "translateY(-1.25rem)"}} />

**Open Interpreter** lets language models run code.

You can chat with Open Interpreter through a ChatGPT-like interface in your terminal by running `interpreter` after installing.

This provides a natural-language interface to your computer's general-purpose capabilities:

-   Create and edit photos, videos, PDFs, etc.
-   Control a Chrome browser to perform research
-   Plot, clean, and analyze large datasets
-   ...etc.

<br/>

<Info>You can also build Open Interpreter into your applications with [our Python package.](/usage/python/arguments)</Info>

---

<h1><span class="font-semibold">Quick start</span></h1>

If you already use Python, you can install Open Interpreter via `pip`:

<Steps>
  <Step title="Install" icon={"arrow-down"} iconType={"solid"}>
```bash
pip install open-interpreter
```
  </Step>
  <Step title="Use" icon={"circle"} iconType={"solid"}>
```bash
interpreter
```
  </Step>
</Steps>

We've also developed [one-line installers](/getting-started/setup#experimental-one-line-installers) that install Python and set up Open Interpreter.

================
File: getting-started/setup.mdx
================
---
title: Setup
---

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/5sk3t8ilDR8"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

## Installation from `pip`

If you are familiar with Python, we recommend installing Open Interpreter via `pip`

```bash
pip install open-interpreter
```

<Info>
  You'll need Python
  [3.10](https://www.python.org/downloads/release/python-3100/) or
  [3.11](https://www.python.org/downloads/release/python-3110/). Run `python
  --version` to check yours.

It is recommended to install Open Interpreter in a [virtual
environment](https://docs.python.org/3/library/venv.html).

</Info>

## Install optional dependencies from `pip`

Open Interpreter has optional dependencies for different capabilities

[Local Mode](/guides/running-locally) dependencies

```bash
pip install open-interpreter[local]
```

[OS Mode](/guides/os-mode) dependencies

```bash
pip install open-interpreter[os]
```

[Safe Mode](/safety/safe-mode) dependencies

```bash
pip install open-interpreter[safe]
```

Server dependencies

```bash
pip install open-interpreter[server]
```

## Experimental one-line installers

To try our experimental installers, open your Terminal with admin privileges [(click here to learn how)](https://chat.openai.com/share/66672c0f-0935-4c16-ac96-75c1afe14fe3), then paste the following commands:

<CodeGroup>

```bash Mac
curl -sL https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-mac-installer.sh | bash
```

```powershell Windows
iex "& {$(irm https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-windows-installer-conda.ps1)}"
```

```bash Linux
curl -sL https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-linux-installer.sh | bash
```

</CodeGroup>

These installers will attempt to download Python, set up an environment, and install Open Interpreter for you.

## No Installation

If configuring your computer environment is challenging, you can press the `,` key on the [GitHub page](https://github.com/OpenInterpreter/open-interpreter) to create a codespace. After a moment, you'll receive a cloud virtual machine environment pre-installed with open-interpreter. You can then start interacting with it directly and freely confirm its execution of system commands without worrying about damaging the system.

================
File: guides/advanced-terminal-usage.mdx
================
---
title: Advanced Terminal Usage
---

Magic commands can be used to control the interpreter's behavior in interactive mode:

- `%% [shell commands, like ls or cd]`: Run commands in Open Interpreter's shell instance
- `%verbose [true/false]`: Toggle verbose mode. Without arguments or with 'true', it enters verbose mode. With 'false', it exits verbose mode.
- `%reset`: Reset the current session.
- `%undo`: Remove previous messages and its response from the message history.
- `%save_message [path]`: Saves messages to a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%load_message [path]`: Loads messages from a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%tokens [prompt]`: EXPERIMENTAL: Calculate the tokens used by the next request based on the current conversation's messages and estimate the cost of that request; optionally provide a prompt to also calculate the tokens used by that prompt and the total amount of tokens that will be sent with the next request.
- `%info`: Show system and interpreter information.
- `%help`: Show this help message.
- `%jupyter`: Export the current session to a Jupyter notebook file (.ipynb) to the Downloads folder.
- `%markdown [path]`: Export the conversation to a specified Markdown path. If no path is provided, it will be saved to the Downloads folder with a generated conversation name.

================
File: guides/basic-usage.mdx
================
---
title: Basic Usage
---

<CardGroup>

<Card
  title="Interactive demo"
  icon="gamepad-modern"
  iconType="solid"
  href="https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing"
>
  Try Open Interpreter without installing anything on your computer
</Card>

<Card
  title="Example voice interface"
  icon="circle"
  iconType="solid"
  href="https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK"
>
  An example implementation of Open Interpreter's streaming capabilities
</Card>

</CardGroup>

---

### Interactive Chat

To start an interactive chat in your terminal, either run `interpreter` from the command line or `interpreter.chat()` from a .py file.

<CodeGroup>

```shell Terminal
interpreter
```

```python Python
interpreter.chat()
```

</CodeGroup>

---

### Programmatic Chat

For more precise control, you can pass messages directly to `.chat(message)` in Python:

```python
interpreter.chat("Add subtitles to all videos in /videos.")

# ... Displays output in your terminal, completes task ...

interpreter.chat("These look great but can you make the subtitles bigger?")

# ...
```

---

### Start a New Chat

In your terminal, Open Interpreter behaves like ChatGPT and will not remember previous conversations. Simply run `interpreter` to start a new chat.

In Python, Open Interpreter remembers conversation history. If you want to start fresh, you can reset it.

<CodeGroup>

```shell Terminal
interpreter
```

```python Python
interpreter.messages = []
```

</CodeGroup>

---

### Save and Restore Chats

In your terminal, Open Interpreter will save previous conversations to `<your application directory>/Open Interpreter/conversations/`.

You can resume any of them by running `--conversations`. Use your arrow keys to select one , then press `ENTER` to resume it.

In Python, `interpreter.chat()` returns a List of messages, which can be used to resume a conversation with `interpreter.messages = messages`.

<CodeGroup>

```shell Terminal
interpreter --conversations
```

```python Python
# Save messages to 'messages'
messages = interpreter.chat("My name is Killian.")

# Reset interpreter ("Killian" will be forgotten)
interpreter.messages = []

# Resume chat from 'messages' ("Killian" will be remembered)
interpreter.messages = messages
```

</CodeGroup>

---

### Configure Default Settings

We save default settings to the `default.yaml` profile which can be opened and edited by running the following command:

```shell
interpreter --profiles
```

You can use this to set your default language model, system message (custom instructions), max budget, etc.

<Info>
  **Note:** The Python library will also inherit settings from the default
  profile file. You can change it by running `interpreter --profiles` and
  editing `default.yaml`.
</Info>

---

### Customize System Message

In your terminal, modify the system message by [editing your configuration file as described here](#configure-default-settings).

In Python, you can inspect and configure Open Interpreter's system message to extend its functionality, modify permissions, or give it more context.

```python
interpreter.system_message += """
Run shell commands with -y so the user doesn't have to confirm them.
"""
print(interpreter.system_message)
```

---

### Change your Language Model

Open Interpreter uses [LiteLLM](https://docs.litellm.ai/docs/providers/) to connect to language models.

You can change the model by setting the model parameter:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

In Python, set the model on the object:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Find the appropriate "model" string for your language model here.](https://docs.litellm.ai/docs/providers/)

================
File: guides/demos.mdx
================
---
title: Demos
---

### Vision Mode

#### Recreating a Tailwind Component

Creating a dropdown menu in Tailwind from a single screenshot:

<iframe src="data:text/html;charset=utf-8,%0A%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3Ewe%26%2339%3Bve%20literally%20been%20flying%20blind%20until%20now%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%24%20interpreter%20--vision%3Cbr%3E%0A%20%20%20%20%26gt%3B%20Recreate%20this%20component%20in%20Tailwind%20CSS%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%28this%20is%20realtime%29%20%3Ca%20href%3D%22https%3A//t.co/PyVm11mclF%22%3Epic.twitter.com/PyVm11mclF%3C/a%3E%0A%20%20%20%20%3C/p%3E%26mdash%3B%20killian%20%28%40hellokillian%29%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/hellokillian/status/1723106008061587651%3Fref_src%3Dtwsrc%255Etfw%22%3ENovember%2010%2C%202023%3C/a%3E%0A%3C/blockquote%3E%20%0A%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A" width="100%" height="500"></iframe>

#### Recreating the ChatGPT interface using GPT-4V:

<iframe src="data:text/html;charset=utf-8,%0A%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EOpen%20Interpreter%20%2B%20Vision%20-%20with%20the%20self-improving%20feedback%20loop%20is%20%F0%9F%91%8C%20%3Cbr%3E%3Cbr%3E%0A%20%20%20%20Here%20is%20how%20it%20iterates%20to%20recreate%20the%20ChatGPT%20UI%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%284x%20speedup%29%20%3Ca%20href%3D%22https%3A//t.co/HphKMOWBiB%22%3Epic.twitter.com/HphKMOWBiB%3C/a%3E%0A%20%20%20%20%3C/p%3E%26mdash%3B%20chilang%20%28%40chilang%29%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/chilang/status/1724577200135897255%3Fref_src%3Dtwsrc%255Etfw%22%3ENovember%2014%2C%202023%3C/a%3E%0A%3C/blockquote%3E%20%0A%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A" width="100%" height="500"></iframe>

### OS Mode

#### Playing Music

Open Interpreter playing some Lofi using OS mode:

<iframe width="560" height="315" src="https://www.youtube.com/embed/-n8qYi5HhO8?si=huEpYFBEwotBIMMs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

#### Open Interpreter Chatting with Open Interpreter

OS mode creating and chatting with a local instance of Open Interpreter:

<iframe src="data:text/html;charset=utf-8,%0A%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EComputer-operating%20AI%20can%20replicate%20itself%20onto%20other%20systems.%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20Open%20Interpreter%20uses%20my%20mouse%20and%20keyboard%20to%20start%20a%20local%20instance%20of%20itself%3A%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/1BZWRA4FMn%22%3Epic.twitter.com/1BZWRA4FMn%3C/a%3E%3C/p%3E%26mdash%3B%20Ty%20%28%40FieroTy%29%20%0A%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/FieroTy/status/1746639975234560101%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%2014%2C%202024%3C/a%3E%0A%3C/blockquote%3E%20%0A%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A" width="100%" height="500"></iframe>

#### Controlling an Arduino

Reading temperature and humidity from an Arudino:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EThis%20time%20I%20showed%20it%20an%20image%20of%20a%20temp%20sensor%2C%20LCD%20%26amp%3B%20Arduino.%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20And%20it%20wrote%20a%20program%20to%20read%20the%20temperature%20%26amp%3B%20humidity%20from%20the%20sensor%20%26amp%3B%20show%20it%20on%20the%20LCD%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Still%20blown%20away%20by%20how%20good%20%40hellokillian%27s%20Open%20Interpreter%20is%21%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20p.s.%20-%20ignore%20the%20cat%20fight%20in%20the%20background%20%3Ca%20href%3D%22https%3A//t.co/tG9sSdkfD5%22%3Ehttps%3A//t.co/tG9sSdkfD5%3C/a%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/B6sH4absff%22%3Epic.twitter.com/B6sH4absff%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Vindiw%20Wijesooriya%20%28%40vindiww%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/vindiww/status/1744252926321942552%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%208%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Music Creation

OS mode using Logic Pro X to record a piano song and play it back:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3Eit%27s%20not%20quite%20Mozart%2C%20but...%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20this%20is%20Open%20Interpreter%20firing%20up%20Logic%20Pro%20to%20write/record%20a%20song%21%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/vPHpPvjk4b%22%3Epic.twitter.com/vPHpPvjk4b%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Ty%20%28%40FieroTy%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/FieroTy/status/1744203268451111035%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%208%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Generating images in Everart.ai

Open Interpreter describing pictures it wants to make, then creating them using OS mode:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EThis%20is%20wild.%20I%20gave%20OS%20control%20to%20GPT-4%20via%20the%20latest%20update%20of%20Open%20Interpreter%20and%20now%20it%27s%20generating%20pictures%20it%20wants%20to%20see%20in%20%40everartai%20%F0%9F%A4%AF%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20GPT%20is%20controlling%20the%20mouse%20and%20adding%20text%20in%20the%20fields%2C%20I%20am%20not%20doing%20anything.%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/hGgML9epEc%22%3Epic.twitter.com/hGgML9epEc%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Pietro%20Schirano%20%28%40skirano%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/skirano/status/1747670816437735836%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%2017%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Open Interpreter Conversing With ChatGPT

OS mode has a conversation with ChatGPT and even asks it "What do you think about human/AI interaction?"

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3EWatch%20GPT%20Vision%20with%20control%20over%20my%20OS%20talking%20to%20ChatGPT.%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20The%20most%20fascinating%20part%20is%20that%20it%27s%20intrigued%20by%20having%20a%20conversation%20with%20another%20%22similar.%22%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22What%20do%20you%20think%20about%20human/AI%20interaction%3F%22%20it%20asked.%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Also%2C%20the%20superhuman%20speed%20at%20which%20it%20types%2C%20lol%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/ViffvDK5H9%22%3Epic.twitter.com/ViffvDK5H9%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Pietro%20Schirano%20%28%40skirano%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/skirano/status/1747772471770583190%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%2018%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

#### Sending an Email with Gmail

OS mode launches Safari, composes an email, and sends it:

<iframe src="data:text/html;charset=utf-8,%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cblockquote%20class%3D%22twitter-tweet%22%20data-media-max-width%3D%22560%22%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Cp%20lang%3D%22en%22%20dir%3D%22ltr%22%3ELook%20ma%2C%20no%20hands%21%20This%20is%20%40OpenInterpreter%20using%20my%20mouse%20and%20keyboard%20to%20send%20an%20email.%20%3Cbr%3E%3Cbr%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Imagine%20what%20else%20is%20possible.%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//t.co/GcBqbTwD23%22%3Epic.twitter.com/GcBqbTwD23%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3C/p%3E%26mdash%3B%20Ty%20%28%40FieroTy%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Ca%20href%3D%22https%3A//twitter.com/FieroTy/status/1743437525207928920%3Fref_src%3Dtwsrc%255Etfw%22%3EJanuary%206%2C%202024%3C/a%3E%0A%20%20%20%20%20%20%20%20%20%20%20%20%3C/blockquote%3E%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%3Cscript%20async%20src%3D%22https%3A//platform.twitter.com/widgets.js%22%20charset%3D%22utf-8%22%3E%3C/script%3E%0A%20%20%20%20%20%20%20%20" width="100%" height="500"></iframe>

================
File: guides/multiple-instances.mdx
================
---
title: Multiple Instances
---

To create multiple instances, use the base class, `OpenInterpreter`:

```python
from interpreter import OpenInterpreter

agent_1 = OpenInterpreter()
agent_1.system_message = "This is a separate instance."

agent_2 = OpenInterpreter()
agent_2.system_message = "This is yet another instance."
```

For fun, you could make these instances talk to eachother:

```python
def swap_roles(messages):
    for message in messages:
        if message['role'] == 'user':
            message['role'] = 'assistant'
        elif message['role'] == 'assistant':
            message['role'] = 'user'
    return messages

agents = [agent_1, agent_2]

# Kick off the conversation
messages = [{"role": "user", "message": "Hello!"}]

while True:
    for agent in agents:
        messages = agent.chat(messages)
        messages = swap_roles(messages)
```

================
File: guides/os-mode.mdx
================
---
title: OS Mode
---

OS mode is a highly experimental mode that allows Open Interpreter to control the operating system visually through the mouse and keyboard. It provides a multimodal LLM like GPT-4V with the necessary tools to capture screenshots of the display and interact with on-screen elements such as text and icons. It will try to use the most direct method to achieve the goal, like using spotlight on Mac to open applications, and using query parameters in the URL to open websites with additional information.

OS mode is a work in progress, if you have any suggestions or experience issues, please reach out on our [Discord](https://discord.com/invite/6p3fD6rBVm).

To enable OS Mode, run the interpreter with the `--os` flag:

```bash
interpreter --os
```

Please note that screen recording permissions must be enabled for your terminal application for OS mode to work properly to work.

OS mode does not currently support multiple displays.

================
File: guides/profiles.mdx
================
---
title: Profiles
---

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/NxfdrGQrkHQ"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

Profiles are a powerful way to customize your instance of Open Interpreter.

Profiles are Python files that configure Open Interpreter. A wide range of fields from the [model](/settings/all-settings#model-selection) to the [context window](/settings/all-settings#context-window) to the [message templates](/settings/all-settings#user-message-template) can be configured in a Profile. This allows you to save multiple variations of Open Interpreter to optimize for your specific use-cases.

You can access your Profiles by running `interpreter --profiles`. This will open the directory where all of your Profiles are stored.

If you want to make your own profile, start with the [Template Profile](https://github.com/OpenInterpreter/open-interpreter/blob/main/interpreter/terminal_interface/profiles/defaults/template_profile.py).

To apply a Profile to an Open Interpreter session, you can run `interpreter --profile <name>`

# Example Python Profile

```Python
from interpreter import interpreter

interpreter.os = True
interpreter.llm.supports_vision = True

interpreter.llm.model = "gpt-4o"

interpreter.llm.supports_functions = True
interpreter.llm.context_window = 110000
interpreter.llm.max_tokens = 4096
interpreter.auto_run = True
interpreter.loop = True
```

# Example YAML Profile

<Info> Make sure YAML profile version is set to 0.2.5 </Info>

```YAML
llm:
  model: "gpt-4-o"
  temperature: 0
  # api_key: ...  # Your API key, if the API requires it
  # api_base: ...  # The URL where an OpenAI-compatible server is running to handle LLM API requests

# Computer Settings
computer:
  import_computer_api: True # Gives OI a helpful Computer API designed for code interpreting language models

# Custom Instructions
custom_instructions: ""  # This will be appended to the system message

# General Configuration
auto_run: False  # If True, code will run without asking for confirmation
offline: False  # If True, will disable some online features like checking for updates

version: 0.2.5 # Configuration file version (do not modify)
```

<Tip>
  There are many settings that can be configured. [See them all
  here](/settings/all-settings)
</Tip>

================
File: guides/running-locally.mdx
================
---
title: Running Locally
---

Open Interpreter can be run fully locally.

Users need to install software to run local LLMs. Open Interpreter supports multiple local model providers such as [Ollama](https://www.ollama.com/), [Llamafile](https://github.com/Mozilla-Ocho/llamafile), [Jan](https://jan.ai/), and [LM Studio](https://lmstudio.ai/).

<Tip>
  Local models perform better with extra guidance and direction. You can improve
  performance for your use-case by creating a new [Profile](/guides/profiles).
</Tip>

## Terminal Usage

### Local Explorer

A Local Explorer was created to simplify the process of using OI locally. To access this menu, run the command `interpreter --local`.

Select your chosen local model provider from the list of options.

Most providers will require the user to state the model they are using. Provider specific instructions are shown to the user in the menu.

### Custom Local

If you want to use a provider other than the ones listed, you will set the `--api_base` flag to set a [custom endpoint](/language-models/local-models/custom-endpoint).

You will also need to set the model by passing in the `--model` flag to select a [model](/settings/all-settings#model-selection).

```python
interpreter --api_base "http://localhost:11434" --model ollama/codestral
```

<Info>
  Other terminal flags are explained in [Settings](/settings/all-settings).
</Info>

## Python Usage

In order to have a Python script use Open Interpreter locally, some fields need to be set

```python
from interpreter import interpreter

interpreter.offline = True
interpreter.llm.model = "ollama/codestral"
interpreter.llm.api_base = "http://localhost:11434"

interpreter.chat("how many files are on my desktop?")
```

## Helpful settings for local models

Local models benefit from more coercion and guidance. This verbosity of adding extra context to messages can impact the conversational experience of Open Interpreter. The following settings allow templates to be applied to messages to improve the steering of the language model while maintaining the natural flow of conversation.

`interpreter.user_message_template` allows users to have their message wrapped in a template. This can be helpful steering a language model to a desired behaviour without needing the user to add extra context to their message.

`interpreter.always_apply_user_message_template` has all user messages to be wrapped in the template. If False, only the last User message will be wrapped.

`interpreter.code_output_template` wraps the output from the computer after code is run. This can help with nudging the language model to continue working or to explain outputs.

`interpreter.empty_code_output_template` is the message that is sent to the language model if code execution results in no output.

<Info>
  Other configuration settings are explained in
  [Settings](/settings/all-settings).
</Info>

================
File: guides/streaming-response.mdx
================
---
title: Streaming Response
---

You can stream messages, code, and code outputs out of Open Interpreter by setting `stream=True` in an `interpreter.chat(message)` call.

```python
for chunk in interpreter.chat("What's 34/24?", stream=True, display=False):
  print(chunk)
```

```
{"role": "assistant", "type": "code", "format": "python", "start": True}
{"role": "assistant", "type": "code", "format": "python", "content": "34"}
{"role": "assistant", "type": "code", "format": "python", "content": " /"}
{"role": "assistant", "type": "code", "format": "python", "content": " "}
{"role": "assistant", "type": "code", "format": "python", "content": "24"}
{"role": "assistant", "type": "code", "format": "python", "end": True}

{"role": "computer", "type": "confirmation", "format": "execution", "content": {"type": "code", "format": "python", "content": "34 / 24"}},

{"role": "computer", "type": "console", "start": True}
{"role": "computer", "type": "console", "format": "active_line", "content": "1"}
{"role": "computer", "type": "console", "format": "output", "content": "1.4166666666666667\n"}
{"role": "computer", "type": "console", "format": "active_line", "content": None},
{"role": "computer", "type": "console", "end": True}

{"role": "assistant", "type": "message", "start": True}
{"role": "assistant", "type": "message", "content": "The"}
{"role": "assistant", "type": "message", "content": " result"}
{"role": "assistant", "type": "message", "content": " of"}
{"role": "assistant", "type": "message", "content": " the"}
{"role": "assistant", "type": "message", "content": " division"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "34"}
{"role": "assistant", "type": "message", "content": "/"}
{"role": "assistant", "type": "message", "content": "24"}
{"role": "assistant", "type": "message", "content": " is"}
{"role": "assistant", "type": "message", "content": " approximately"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "1"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "content": "42"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "end": True}
```

**Note:** Setting `display=True` won't change the behavior of the streaming response, it will just render a display in your terminal.

# Anatomy

Each chunk of the streamed response is a dictionary, that has a "role" key that can be either "assistant" or "computer". The "type" key describes what the chunk is. The "content" key contains the actual content of the chunk.

Every 'message' is made up of chunks, and begins with a "start" chunk, and ends with an "end" chunk. This helps you parse the streamed response into messages.

Let's break down each part of the streamed response.

## Code

In this example, the LLM decided to start writing code first. It could have decided to write a message first, or to only write code, or to only write a message.

Every streamed chunk of type "code" has a format key that specifies the language. In this case it decided to write `python`.

This can be any language defined in [our languages directory.](https://github.com/OpenInterpreter/open-interpreter/tree/main/interpreter/core/computer/terminal/languages)

```

{"role": "assistant", "type": "code", "format": "python", "start": True}

```

Then, the LLM decided to write some code. The code is sent token-by-token:

```

{"role": "assistant", "type": "code", "format": "python", "content": "34"}
{"role": "assistant", "type": "code", "format": "python", "content": " /"}
{"role": "assistant", "type": "code", "format": "python", "content": " "}
{"role": "assistant", "type": "code", "format": "python", "content": "24"}

```

When the LLM finishes writing code, it will send an "end" chunk:

```

{"role": "assistant", "type": "code", "format": "python", "end": True}

```

## Code Output

After the LLM finishes writing a code block, Open Interpreter will attempt to run it.

**Before** it runs it, the following chunk is sent:

```

{"role": "computer", "type": "confirmation", "format": "execution", "content": {"type": "code", "language": "python", "code": "34 / 24"}}

```

If you check for this object, you can break (or get confirmation) **before** executing the code.

```python
# This example asks the user before running code

for chunk in interpreter.chat("What's 34/24?", stream=True):
    if "executing" in chunk:
        if input("Press ENTER to run this code.") != "":
            break
```

**While** the code is being executed, you'll receive the line of code that's being run:

```
{"role": "computer", "type": "console", "format": "active_line", "content": "1"}
```

We use this to highlight the active line of code on our UI, which keeps the user aware of what Open Interpreter is doing.

You'll then receive its output, if it produces any:

```
{"role": "computer", "type": "console", "format": "output", "content": "1.4166666666666667\n"}
```

When the code is **finished** executing, this flag will be sent:

```
{"role": "computer", "type": "console", "end": True}
```

## Message

Finally, the LLM decided to write a message. This is streamed token-by-token as well:

```
{"role": "assistant", "type": "message", "start": True}
{"role": "assistant", "type": "message", "content": "The"}
{"role": "assistant", "type": "message", "content": " result"}
{"role": "assistant", "type": "message", "content": " of"}
{"role": "assistant", "type": "message", "content": " the"}
{"role": "assistant", "type": "message", "content": " division"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "34"}
{"role": "assistant", "type": "message", "content": "/"}
{"role": "assistant", "type": "message", "content": "24"}
{"role": "assistant", "type": "message", "content": " is"}
{"role": "assistant", "type": "message", "content": " approximately"}
{"role": "assistant", "type": "message", "content": " "}
{"role": "assistant", "type": "message", "content": "1"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "content": "42"}
{"role": "assistant", "type": "message", "content": "."}
{"role": "assistant", "type": "message", "end": True}
```

For an example in JavaScript on how you might process these streamed chunks, see the [migration guide](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md)

================
File: integrations/docker.mdx
================
---
title: Docker
---

Docker support is currently experimental. Running Open Interpreter inside of a Docker container may not function as you expect. Let us know on [Discord](https://discord.com/invite/6p3fD6rBVm) if you encounter errors or have suggestions to improve Docker support.

We are working on an official integration for Docker in the coming weeks. For now, you can use Open Interpreter in a sandboxed Docker container environment using the following steps:

1. If you do not have Docker Desktop installed, [install it](https://www.docker.com/products/docker-desktop) before proceeding.

2. Create a new directory and add a file named `Dockerfile` in it with the following contents:

```dockerfile
# Start with Python 3.11
FROM python:3.11

# Replace <your_openai_api_key> with your own key
ENV OPENAI_API_KEY <your_openai_api_key>

# Install Open Interpreter
RUN pip install open-interpreter
```

3. Run the following commands in the same directory to start Open Interpreter.

```bash
docker build -t openinterpreter .
docker run -d -it --name interpreter-instance openinterpreter interpreter
docker attach interpreter-instance
```

## Mounting Volumes

This is how you let it access _some_ files, by telling it a folder (a volume) it will be able to see / manipulate.

To mount a volume, you can use the `-v` flag followed by the path to the directory on your host machine, a colon, and then the path where you want to mount the directory in the container.

```bash
docker run -d -it -v /path/on/your/host:/path/in/the/container --name interpreter-instance openinterpreter interpreter
```

Replace `/path/on/your/host` with the path to the directory on your host machine that you want to mount, and replace `/path/in/the/container` with the path in the Docker container where you want to mount the directory.

Here's a simple example:

```bash
docker run -d -it -v $(pwd):/files --name interpreter-instance openinterpreter interpreter
```

In this example, `$(pwd)` is your current directory, and it is mounted to a `/files` directory in the Docker container (this creates that folder too).

## Flags

To add flags to the command, just append them after `interpreter`. For example, to run the interpreter with custom instructions, run the following command:

```bash
docker-compose run --rm oi interpreter --custom_instructions "Be as concise as possible"
```

Please note that some flags will not work. For example, `--config` will not work, because it cannot open the config file in the container. If you want to use a config file other than the default, you can create a `config.yml` file inside of the same directory, add your custom config, and then run the following command:

```bash
docker-compose run --rm oi interpreter --config_file config.yml
```

================
File: integrations/e2b.mdx
================
---
title: E2B
---

[E2B](https://e2b.dev/) is a secure, sandboxed environment where you can run arbitrary code.

To build this integration, you just need to replace Open Interpreter's `python` (which runs locally) with a `python` that runs on E2B.

First, [get an API key here](https://e2b.dev/), and set it:

```python
import os
os.environ["E2B_API_KEY"] = "<your_api_key_here>"
```

Then, define a custom language for Open Interpreter. The class name doesn't matter, but we'll call it `PythonE2B`:

```python
import e2b

class PythonE2B:
    """
    This class contains all requirements for being a custom language in Open Interpreter:

    - name (an attribute)
    - run (a method)
    - stop (a method)
    - terminate (a method)

    Here, we'll use E2B to power the `run` method.
    """

    # This is the name that will appear to the LLM.
    name = "python"

    # Optionally, you can append some information about this language to the system message:
    system_message = "# Follow this rule: Every Python code block MUST contain at least one print statement."

    # (E2B isn't a Jupyter Notebook, so we added ^ this so it would print things,
    # instead of putting variables at the end of code blocks, which is a Jupyter thing.)

    def run(self, code):
        """Generator that yields a dictionary in LMC Format."""

        # Run the code on E2B
        stdout, stderr = e2b.run_code('Python3', code)

        # Yield the output
        yield {
            "type": "console", "format": "output",
            "content": stdout + stderr # We combined these arbitrarily. Yield anything you'd like!
        }

    def stop(self):
        """Stops the code."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

    def terminate(self):
        """Terminates the entire process."""
        # Not needed here, because e2b.run_code isn't stateful.
        pass

# (Tip: Do this before adding/removing languages, otherwise OI might retain the state of previous languages:)
interpreter.computer.terminate()

# Give Open Interpreter its languages. This will only let it run PythonE2B:
interpreter.computer.languages = [PythonE2B]

# Try it out!
interpreter.chat("What's 349808*38490739?")
```

================
File: language-models/hosted-models/ai21.mdx
================
---
title: AI21
---

To use Open Interpreter with a model from AI21, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model j2-light
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "j2-light"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model from [AI21:](https://www.ai21.com/)

<CodeGroup>

```bash Terminal
interpreter --model j2-light
interpreter --model j2-mid
interpreter --model j2-ultra
```

```python Python
interpreter.llm.model = "j2-light"
interpreter.llm.model = "j2-mid"
interpreter.llm.model = "j2-ultra"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `AI21_API_KEY`       | The API key for authenticating to AI21's services. | [AI21 Account Page](https://www.ai21.com/account/api-keys) |

================
File: language-models/hosted-models/anthropic.mdx
================
---
title: Anthropic
---

To use Open Interpreter with a model from Anthropic, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model claude-instant-1
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "claude-instant-1"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model from [Anthropic:](https://www.anthropic.com/)

<CodeGroup>

```bash Terminal
interpreter --model claude-instant-1
interpreter --model claude-instant-1.2
interpreter --model claude-2
```

```python Python
interpreter.llm.model = "claude-instant-1"
interpreter.llm.model = "claude-instant-1.2"
interpreter.llm.model = "claude-2"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `ANTHROPIC_API_KEY`       | The API key for authenticating to Anthropic's services. | [Anthropic](https://www.anthropic.com/) |

================
File: language-models/hosted-models/anyscale.mdx
================
---
title: Anyscale
---

To use Open Interpreter with a model from Anyscale, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model anyscale/<model-name>
```

```python Python
from interpreter import interpreter

# Set the model to use from AWS Bedrock:
interpreter.llm.model = "anyscale/<model-name>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from Anyscale:

- Llama 2 7B Chat
- Llama 2 13B Chat
- Llama 2 70B Chat
- Mistral 7B Instruct
- CodeLlama 34b Instruct

<CodeGroup>

```bash Terminal
interpreter --model anyscale/meta-llama/Llama-2-7b-chat-hf
interpreter --model anyscale/meta-llama/Llama-2-13b-chat-hf
interpreter --model anyscale/meta-llama/Llama-2-70b-chat-hf
interpreter --model anyscale/mistralai/Mistral-7B-Instruct-v0.1
interpreter --model anyscale/codellama/CodeLlama-34b-Instruct-hf
```

```python Python
interpreter.llm.model = "anyscale/meta-llama/Llama-2-7b-chat-hf"
interpreter.llm.model = "anyscale/meta-llama/Llama-2-13b-chat-hf"
interpreter.llm.model = "anyscale/meta-llama/Llama-2-70b-chat-hf"
interpreter.llm.model = "anyscale/mistralai/Mistral-7B-Instruct-v0.1"
interpreter.llm.model = "anyscale/codellama/CodeLlama-34b-Instruct-hf"

```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                            | Where to Find                                                               |
| -------------------- | -------------------------------------- | --------------------------------------------------------------------------- |
| `ANYSCALE_API_KEY`   | The API key for your Anyscale account. | [Anyscale Account Settings](https://app.endpoints.anyscale.com/credentials) |

================
File: language-models/hosted-models/aws-sagemaker.mdx
================
---
title: AWS Sagemaker
---

To use Open Interpreter with a model from AWS Sagemaker, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model sagemaker/<model-name>
```

```python Python
# Sagemaker requires boto3 to be installed on your machine:
!pip install boto3

from interpreter import interpreter

interpreter.llm.model = "sagemaker/<model-name>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from AWS Sagemaker:

- Meta Llama 2 7B
- Meta Llama 2 7B (Chat/Fine-tuned)
- Meta Llama 2 13B
- Meta Llama 2 13B (Chat/Fine-tuned)
- Meta Llama 2 70B
- Meta Llama 2 70B (Chat/Fine-tuned)
- Your Custom Huggingface Model

<CodeGroup>

```bash Terminal

interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b-f
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b-f
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b
interpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b-b-f
interpreter --model sagemaker/<your-hugginface-deployment-name>
```

```python Python
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b-f"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b-f"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b"
interpreter.llm.model = "sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b-b-f"
interpreter.llm.model = "sagemaker/<your-hugginface-deployment-name>"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable    | Description                                     | Where to Find                                                                       |
| ----------------------- | ----------------------------------------------- | ----------------------------------------------------------------------------------- |
| `AWS_ACCESS_KEY_ID`     | The API access key for your AWS account.        | [AWS Account Overview -> Security Credentials](https://console.aws.amazon.com/)     |
| `AWS_SECRET_ACCESS_KEY` | The API secret access key for your AWS account. | [AWS Account Overview -> Security Credentials](https://console.aws.amazon.com/)     |
| `AWS_REGION_NAME`       | The AWS region you want to use                  | [AWS Account Overview -> Navigation bar -> Region](https://console.aws.amazon.com/) |

================
File: language-models/hosted-models/azure.mdx
================
---
title: Azure
---

To use a model from Azure, set the `model` flag to begin with `azure/`:

<CodeGroup>

```bash Terminal
interpreter --model azure/<your_deployment_id>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "azure/<your_deployment_id>"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `AZURE_API_KEY`       | The API key for authenticating to Azure's services. | [Azure Account Page](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps) |
| `AZURE_API_BASE`      | The base URL for Azure's services. | [Azure Account Page](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps) |
| `AZURE_API_VERSION`   | The version of Azure's services. | [Azure Account Page](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps) |

================
File: language-models/hosted-models/baseten.mdx
================
---
title: Baseten
---

To use Open Interpreter with Baseten, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model baseten/<baseten-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "baseten/<baseten-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from Baseten:

- Falcon 7b (qvv0xeq)
- Wizard LM (q841o8w)
- MPT 7b Base (31dxrj3)

<CodeGroup>

```bash Terminal

interpreter --model baseten/qvv0xeq
interpreter --model baseten/q841o8w
interpreter --model baseten/31dxrj3


```

```python Python
interpreter.llm.model = "baseten/qvv0xeq"
interpreter.llm.model = "baseten/q841o8w"
interpreter.llm.model = "baseten/31dxrj3"


```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description     | Where to Find                                                                                            |
| -------------------- | --------------- | -------------------------------------------------------------------------------------------------------- |
| BASETEN_API_KEY'`    | Baseten API key | [Baseten Dashboard -> Settings -> Account -> API Keys](https://app.baseten.co/settings/account/api_keys) |

================
File: language-models/hosted-models/cloudflare.mdx
================
---
title: Cloudflare Workers AI
---

To use Open Interpreter with the Cloudflare Workers AI API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model cloudflare/<cloudflare-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "cloudflare/<cloudflare-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from Cloudflare Workers AI:

- Llama-2 7b chat fp16
- Llama-2 7b chat int8
- Mistral 7b instruct v0.1
- CodeLlama 7b instruct awq

<CodeGroup>

```bash Terminal

interpreter --model cloudflare/@cf/meta/llama-2-7b-chat-fp16
interpreter --model cloudflare/@cf/meta/llama-2-7b-chat-int8
interpreter --model @cf/mistral/mistral-7b-instruct-v0.1
interpreter --model @hf/thebloke/codellama-7b-instruct-awq

```

```python Python
interpreter.llm.model = "cloudflare/@cf/meta/llama-2-7b-chat-fp16"
interpreter.llm.model = "cloudflare/@cf/meta/llama-2-7b-chat-int8"
interpreter.llm.model = "@cf/mistral/mistral-7b-instruct-v0.1"
interpreter.llm.model = "@hf/thebloke/codellama-7b-instruct-awq"

```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable    | Description                | Where to Find                                                                                  |
| ----------------------- | -------------------------- | ---------------------------------------------------------------------------------------------- |
| `CLOUDFLARE_API_KEY'`   | Cloudflare API key         | [Cloudflare Profile Page -> API Tokens](https://dash.cloudflare.com/profile/api-tokens)        |
| `CLOUDFLARE_ACCOUNT_ID` | Your Cloudflare account ID | [Cloudflare Dashboard -> Grab the Account ID from the url like: https://dash.cloudflare.com/{CLOUDFLARE_ACCOUNT_ID}?account= ](https://dash.cloudflare.com/) |

================
File: language-models/hosted-models/cohere.mdx
================
---
title: Cohere
---

To use Open Interpreter with a model from Cohere, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model command-nightly
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "command-nightly"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [Cohere's models page:](https://www.cohere.ai/models)

<CodeGroup>

```bash Terminal
interpreter --model command
interpreter --model command-light
interpreter --model command-medium
interpreter --model command-medium-beta
interpreter --model command-xlarge-beta
interpreter --model command-nightly
```

```python Python
interpreter.llm.model = "command"
interpreter.llm.model = "command-light"
interpreter.llm.model = "command-medium"
interpreter.llm.model = "command-medium-beta"
interpreter.llm.model = "command-xlarge-beta"
interpreter.llm.model = "command-nightly"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `COHERE_API_KEY`       | The API key for authenticating to Cohere's services. | [Cohere Account Page](https://app.cohere.ai/login) |

================
File: language-models/hosted-models/deepinfra.mdx
================
---
title: DeepInfra
---

To use Open Interpreter with DeepInfra, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model deepinfra/<deepinfra-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "deepinfra/<deepinfra-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from DeepInfra:

- Llama-2 70b chat hf
- Llama-2 7b chat hf
- Llama-2 13b chat hf
- CodeLlama 34b instruct awq
- Mistral 7b instruct v0.1
- jondurbin/airoboros I2 70b gpt3 1.4.1

<CodeGroup>

```bash Terminal

interpreter --model deepinfra/meta-llama/Llama-2-70b-chat-hf
interpreter --model deepinfra/meta-llama/Llama-2-7b-chat-hf
interpreter --model deepinfra/meta-llama/Llama-2-13b-chat-hf
interpreter --model deepinfra/codellama/CodeLlama-34b-Instruct-hf
interpreter --model deepinfra/mistral/mistral-7b-instruct-v0.1
interpreter --model deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1

```

```python Python
interpreter.llm.model = "deepinfra/meta-llama/Llama-2-70b-chat-hf"
interpreter.llm.model = "deepinfra/meta-llama/Llama-2-7b-chat-hf"
interpreter.llm.model = "deepinfra/meta-llama/Llama-2-13b-chat-hf"
interpreter.llm.model = "deepinfra/codellama/CodeLlama-34b-Instruct-hf"
interpreter.llm.model = "deepinfra/mistral-7b-instruct-v0.1"
interpreter.llm.model = "deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1"

```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description       | Where to Find                                                          |
| -------------------- | ----------------- | ---------------------------------------------------------------------- |
| `DEEPINFRA_API_KEY'` | DeepInfra API key | [DeepInfra Dashboard -> API Keys](https://deepinfra.com/dash/api_keys) |

================
File: language-models/hosted-models/gpt-4-setup.mdx
================
---
title: GPT-4 Setup
---

# Setting Up GPT-4

Step 1 - Install OpenAI packages

```
pip install openai
```

Step 2 - create a new API key at [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)

![alt](https://drive.google.com/file/d/1xfs_SZVbK6hhDf2-_AMH4uCxdgFlGiMK/view?usp=sharing)

Step 3 - Run the interpreter command after installing open-interpreter and enter your newly generated api key

![alt](https://drive.google.com/file/d/1avLeCIKvQV732mbrf-91s5T7uJfTLyCS/view?usp=sharing)

or

**FOR MACOS :**

1.  **Open Terminal**: You can find it in the Applications folder or search for it using Spotlight (Command + Space).
2.  **Edit Bash Profile**: Use the command `nano ~/.bash_profile` or `nano ~/.zshrc` (for newer MacOS versions) to open the profile file in a text editor.
3.  **Add Environment Variable**: In the editor, add the line below, replacing `your-api-key-here` with your actual API key:

    ```
    export OPENAI\_API\_KEY='your-api-key-here'
    ```

4.  **Save and Exit**: Press Ctrl+O to write the changes, followed by Ctrl+X to close the editor.
5.  **Load Your Profile**: Use the command `source ~/.bash_profile` or `source ~/.zshrc` to load the updated profile.
6.  **Verification**: Verify the setup by typing `echo $OPENAI_API_KEY` in the terminal. It should display your API key.

**FOR WINDOWS :**

1.  **Open Command Prompt**: You can find it by searching "cmd" in the start menu.
2.  **Set environment variable in the current session**: To set the environment variable in the current session, use the command below, replacing `your-api-key-here` with your actual API key:

    ```
    setx OPENAI\_API\_KEY "your-api-key-here"
    ```

    This command will set the OPENAI_API_KEY environment variable for the current session.

3.  **Permanent setup**: To make the setup permanent, add the variable through the system properties as follows:

    - Right-click on 'This PC' or 'My Computer' and select 'Properties'.
    - Click on 'Advanced system settings'.
    - Click the 'Environment Variables' button.
    - In the 'System variables' section, click 'New...' and enter OPENAI_API_KEY as the variable name and your API key as the variable value.

4.  **Verification**: To verify the setup, reopen the command prompt and type the command below. It should display your API key: `echo %OPENAI_API_KEY%`

================
File: language-models/hosted-models/huggingface.mdx
================
---
title: Huggingface
---

To use Open Interpreter with Huggingface models, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model huggingface/<huggingface-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "huggingface/<huggingface-model>"
interpreter.chat()
```

</CodeGroup>

You may also need to specify your Huggingface api base url:
<CodeGroup>

```bash Terminal
interpreter --api_base <https://my-endpoint.huggingface.cloud>
```

```python Python
from interpreter import interpreter

interpreter.llm.api_base = "https://my-endpoint.huggingface.cloud"
interpreter.chat()
```

</CodeGroup>

# Supported Models

Open Interpreter should work with almost any text based hugging face model.

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable   | Description                 | Where to Find                                                                      |
| ---------------------- | --------------------------- | ---------------------------------------------------------------------------------- |
| `HUGGINGFACE_API_KEY'` | Huggingface account API key | [Huggingface -> Settings -> Access Tokens](https://huggingface.co/settings/tokens) |

================
File: language-models/hosted-models/mistral-api.mdx
================
---
title: Mistral AI API
---

To use Open Interpreter with the Mistral API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model mistral/<mistral-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "mistral/<mistral-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from the Mistral API:

- mistral-tiny
- mistral-small
- mistral-medium

<CodeGroup>

```bash Terminal

interpreter --model mistral/mistral-tiny
interpreter --model mistral/mistral-small
interpreter --model mistral/mistral-medium
```

```python Python
interpreter.llm.model = "mistral/mistral-tiny"
interpreter.llm.model = "mistral/mistral-small"
interpreter.llm.model = "mistral/mistral-medium"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                                  | Where to Find                                      |
| -------------------- | -------------------------------------------- | -------------------------------------------------- |
| `MISTRAL_API_KEY`    | The Mistral API key from Mistral API Console | [Mistral API Console](https://console.mistral.ai/user/api-keys/) |

================
File: language-models/hosted-models/nlp-cloud.mdx
================
---
title: NLP Cloud
---

To use Open Interpreter with NLP Cloud, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model dolphin
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "dolphin"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description       | Where to Find                                                     |
| -------------------- | ----------------- | ----------------------------------------------------------------- |
| `NLP_CLOUD_API_KEY'` | NLP Cloud API key | [NLP Cloud Dashboard -> API KEY](https://nlpcloud.com/home/token) |

================
File: language-models/hosted-models/openai.mdx
================
---
title: OpenAI
---

To use Open Interpreter with a model from OpenAI, simply run:

<CodeGroup>

```bash Terminal
interpreter
```

```python Python
from interpreter import interpreter

interpreter.chat()
```

</CodeGroup>

This will default to `gpt-4-turbo`, which is the most capable publicly available model for code interpretation (Open Interpreter was designed to be used with `gpt-4`).

To run a specific model from OpenAI, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model gpt-3.5-turbo
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "gpt-3.5-turbo"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [OpenAI's models page:](https://platform.openai.com/docs/models/)

<CodeGroup>

```bash Terminal
interpreter --model gpt-4o
```

```python Python
interpreter.llm.model = "gpt-4o"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                                          | Where to Find                                                       |
| -------------------- | ---------------------------------------------------- | ------------------------------------------------------------------- |
| `OPENAI_API_KEY`     | The API key for authenticating to OpenAI's services. | [OpenAI Account Page](https://platform.openai.com/account/api-keys) |

================
File: language-models/hosted-models/openrouter.mdx
================
---
title: OpenRouter
---

To use Open Interpreter with a model from OpenRouter, set the `model` flag to begin with `openrouter/`:

<CodeGroup>

```bash Terminal
interpreter --model openrouter/openai/gpt-3.5-turbo
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "openrouter/openai/gpt-3.5-turbo"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [OpenRouter's models page:](https://openrouter.ai/models)

<CodeGroup>

```bash Terminal
interpreter --model openrouter/openai/gpt-3.5-turbo
interpreter --model openrouter/openai/gpt-3.5-turbo-16k
interpreter --model openrouter/openai/gpt-4
interpreter --model openrouter/openai/gpt-4-32k
interpreter --model openrouter/anthropic/claude-2
interpreter --model openrouter/anthropic/claude-instant-v1
interpreter --model openrouter/google/palm-2-chat-bison
interpreter --model openrouter/google/palm-2-codechat-bison
interpreter --model openrouter/meta-llama/llama-2-13b-chat
interpreter --model openrouter/meta-llama/llama-2-70b-chat
```

```python Python
interpreter.llm.model = "openrouter/openai/gpt-3.5-turbo"
interpreter.llm.model = "openrouter/openai/gpt-3.5-turbo-16k"
interpreter.llm.model = "openrouter/openai/gpt-4"
interpreter.llm.model = "openrouter/openai/gpt-4-32k"
interpreter.llm.model = "openrouter/anthropic/claude-2"
interpreter.llm.model = "openrouter/anthropic/claude-instant-v1"
interpreter.llm.model = "openrouter/google/palm-2-chat-bison"
interpreter.llm.model = "openrouter/google/palm-2-codechat-bison"
interpreter.llm.model = "openrouter/meta-llama/llama-2-13b-chat"
interpreter.llm.model = "openrouter/meta-llama/llama-2-70b-chat"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `OPENROUTER_API_KEY`       | The API key for authenticating to OpenRouter's services. | [OpenRouter Account Page](https://openrouter.ai/keys) |
| `OR_SITE_URL`      | The site URL for OpenRouter's services. | [OpenRouter Account Page](https://openrouter.ai/keys) |
| `OR_APP_NAME`   | The app name for OpenRouter's services. | [OpenRouter Account Page](https://openrouter.ai/keys) |

================
File: language-models/hosted-models/palm.mdx
================
---
title: PaLM API - Google
---

To use Open Interpreter with PaLM, you must `pip install -q google-generativeai`, then set the `model` flag in Open Interpreter:

<CodeGroup>

```bash Terminal
interpreter --model palm/chat-bison
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "palm/chat-bison"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                                                      | Where to Find                                                                        |
| -------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| `PALM_API_KEY`       | The PaLM API key from Google Generative AI Developers dashboard. | [Google Generative AI Developers Dashboard](https://developers.generativeai.google/) |

================
File: language-models/hosted-models/perplexity.mdx
================
---
title: Perplexity
---

To use Open Interpreter with the Perplexity API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model perplexity/<perplexity-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "perplexity/<perplexity-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from the Perplexity API:

- pplx-7b-chat
- pplx-70b-chat
- pplx-7b-online
- pplx-70b-online
- codellama-34b-instruct
- llama-2-13b-chat
- llama-2-70b-chat
- mistral-7b-instruct
- openhermes-2-mistral-7b
- openhermes-2.5-mistral-7b
- pplx-7b-chat-alpha
- pplx-70b-chat-alpha

<CodeGroup>

```bash Terminal

interpreter --model perplexity/pplx-7b-chat
interpreter --model perplexity/pplx-70b-chat
interpreter --model perplexity/pplx-7b-online
interpreter --model perplexity/pplx-70b-online
interpreter --model perplexity/codellama-34b-instruct
interpreter --model perplexity/llama-2-13b-chat
interpreter --model perplexity/llama-2-70b-chat
interpreter --model perplexity/mistral-7b-instruct
interpreter --model perplexity/openhermes-2-mistral-7b
interpreter --model perplexity/openhermes-2.5-mistral-7b
interpreter --model perplexity/pplx-7b-chat-alpha
interpreter --model perplexity/pplx-70b-chat-alpha
```

```python Python
interpreter.llm.model = "perplexity/pplx-7b-chat"
interpreter.llm.model = "perplexity/pplx-70b-chat"
interpreter.llm.model = "perplexity/pplx-7b-online"
interpreter.llm.model = "perplexity/pplx-70b-online"
interpreter.llm.model = "perplexity/codellama-34b-instruct"
interpreter.llm.model = "perplexity/llama-2-13b-chat"
interpreter.llm.model = "perplexity/llama-2-70b-chat"
interpreter.llm.model = "perplexity/mistral-7b-instruct"
interpreter.llm.model = "perplexity/openhermes-2-mistral-7b"
interpreter.llm.model = "perplexity/openhermes-2.5-mistral-7b"
interpreter.llm.model = "perplexity/pplx-7b-chat-alpha"
interpreter.llm.model = "perplexity/pplx-70b-chat-alpha"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable    | Description                          | Where to Find                                                     |
| ----------------------- | ------------------------------------ | ----------------------------------------------------------------- |
| `PERPLEXITYAI_API_KEY'` | The Perplexity API key from pplx-api | [Perplexity API Settings](https://www.perplexity.ai/settings/api) |

================
File: language-models/hosted-models/petals.mdx
================
---
title: Petals
---

To use Open Interpreter with a model from Petals, set the `model` flag to begin with `petals/`:

<CodeGroup>

```bash Terminal
interpreter --model petals/petals-team/StableBeluga2
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "petals/petals-team/StableBeluga2"
interpreter.chat()
```

</CodeGroup>

# Pre-Requisites

Ensure you have petals installed:

```bash Terminal
pip install git+https://github.com/bigscience-workshop/petals
```

# Supported Models

We support any model on [Petals:](https://github.com/bigscience-workshop/petals)

<CodeGroup>

```bash Terminal
interpreter --model petals/petals-team/StableBeluga2
interpreter --model petals/huggyllama/llama-65b
```

```python Python
interpreter.llm.model = "petals/petals-team/StableBeluga2"
interpreter.llm.model = "petals/huggyllama/llama-65b"
```

</CodeGroup>

# Required Environment Variables

No environment variables are required to use these models.

================
File: language-models/hosted-models/replicate.mdx
================
---
title: Replicate
---

To use Open Interpreter with a model from Replicate, set the `model` flag to begin with `replicate/`:

<CodeGroup>

```bash Terminal
interpreter --model replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf"
interpreter.chat()
```

</CodeGroup>

# Supported Models

We support any model on [Replicate's models page:](https://replicate.ai/explore)

<CodeGroup>

```bash Terminal
interpreter --model replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf
interpreter --model replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52
interpreter --model replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b
interpreter --model replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f
```

```python Python
interpreter.llm.model = "replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf"
interpreter.llm.model = "replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52"
interpreter.llm.model = "replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b"
interpreter.llm.model = "replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description  | Where to Find  |
| --------------------- | ------------ | -------------- |
| `REPLICATE_API_KEY`       | The API key for authenticating to Replicate's services. | [Replicate Account Page](https://replicate.ai/login) |

================
File: language-models/hosted-models/togetherai.mdx
================
---
title: Together AI
---

To use Open Interpreter with Together AI, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model together_ai/<together_ai-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "together_ai/<together_ai-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

All models on Together AI are supported.

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable  | Description                                   | Where to Find                                                                               |
| --------------------- | --------------------------------------------- | ------------------------------------------------------------------------------------------- |
| `TOGETHERAI_API_KEY'` | The TogetherAI API key from the Settings page | [TogetherAI -> Profile -> Settings -> API Keys](https://api.together.xyz/settings/api-keys) |

================
File: language-models/hosted-models/vertex-ai.mdx
================
---
title: Google (Vertex AI)
---

## Pre-requisites
* `pip install google-cloud-aiplatform`
* Authentication: 
    * run `gcloud auth application-default login` See [Google Cloud Docs](https://cloud.google.com/docs/authentication/external/set-up-adc)
    * Alternatively you can set `application_default_credentials.json`

To use Open Interpreter with Google's Vertex AI API, set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model gemini-pro
interpreter --model gemini-pro-vision
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "gemini-pro"
interpreter.llm.model = "gemini-pro-vision"
interpreter.chat()
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

Environment Variable  | Description  | Where to Find  |
--------------------- | ------------ | -------------- |
`VERTEXAI_PROJECT`       | The Google Cloud project ID. | [Google Cloud Console](https://console.cloud.google.com/vertex-ai) |
`VERTEXAI_LOCATION`      | The location of your Vertex AI resources. | [Google Cloud Console](https://console.cloud.google.com/vertex-ai) |

## Supported Models

- gemini-pro
- gemini-pro-vision
- chat-bison-32k
- chat-bison
- chat-bison@001
- codechat-bison
- codechat-bison-32k
- codechat-bison@001

================
File: language-models/hosted-models/vllm.mdx
================
---
title: vLLM
---

To use Open Interpreter with vLLM, you will need to:

1. `pip install vllm`
2. Set the api_base flag:

<CodeGroup>

```bash Terminal
interpreter --api_base <https://your-hosted-vllm-server>
```

```python Python
from interpreter import interpreter

interpreter.llm.api_base = "<https://your-hosted-vllm-server>"
interpreter.chat()
```

</CodeGroup>

3. Set the `model` flag:

<CodeGroup>

```bash Terminal
interpreter --model vllm/<perplexity-model>
```

```python Python
from interpreter import interpreter

interpreter.llm.model = "vllm/<perplexity-model>"
interpreter.chat()
```

</CodeGroup>

# Supported Models

All models from VLLM should be supported

================
File: language-models/local-models/best-practices.mdx
================
---
title: "Best Practices"
---

Most settings — like model architecture and GPU offloading — can be adjusted via your LLM providers like [LM Studio.](https://lmstudio.ai/)

**However, `max_tokens` and `context_window` should be set via Open Interpreter.**

For local mode, smaller context windows will use less RAM, so we recommend trying a much shorter window (~1000) if it's is failing or if it's slow.

<CodeGroup>

```bash Terminal
interpreter --local --max_tokens 1000 --context_window 3000
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "openai/x" # Tells OI to send messages in OpenAI's format
interpreter.llm.api_key = "fake_key" # LiteLLM, which we use to talk to LM Studio, requires this
interpreter.llm.api_base = "http://localhost:1234/v1" # Point this at any OpenAI compatible server

interpreter.llm.max_tokens = 1000
interpreter.llm.context_window = 3000

interpreter.chat()
```

</CodeGroup>

<br />

<Info>Make sure `max_tokens` is less than `context_window`.</Info>

================
File: language-models/local-models/custom-endpoint.mdx
================
---
title: Custom Endpoint
---

Simply set `api_base` to any OpenAI compatible server:

<CodeGroup>
```bash Terminal
interpreter --api_base <custom_endpoint>
```

```python Python
from interpreter import interpreter

interpreter.llm.api_base = "<custom_endpoint>"
interpreter.chat()
```

</CodeGroup>

================
File: language-models/local-models/janai.mdx
================
---
title: Jan.ai
---

Jan.ai is an open-source platform for running local language models on your computer, and is equipped with a built in server.

To run Open Interpreter with Jan.ai, follow these steps:

1. [Install](https://jan.ai/) the Jan.ai Desktop Application on your computer.

2. Once installed, you will need to install a language model. Click the 'Hub' icon on the left sidebar (the four squares icon). Click the 'Download' button next to the model you would like to install, and wait for it to finish installing before continuing.

3. To start your model, click the 'Settings' icon at the bottom of the left sidebar. Then click 'Models' under the CORE EXTENSIONS section. This page displays all of your installed models. Click the options icon next to the model you would like to start (vertical ellipsis icon). Then click 'Start Model', which will take a few seconds to fire up.

4. Click the 'Advanced' button under the GENERAL section, and toggle on the "Enable API Server" option. This will start a local server that you can use to interact with your model.

5. Now we fire up Open Interpreter with this custom model. Either run `interpreter --local` in the terminal to set it up interactively, or run this command, but replace `<model_id>` with the id of the model you downloaded:

<CodeGroup>

```bash Terminal
interpreter --api_base http://localhost:1337/v1  --model <model_id>
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "<model_id>"
interpreter.llm.api_base = "http://localhost:1337/v1 "

interpreter.chat()
```

</CodeGroup>

If your model can handle a longer context window than the default 3000, you can set the context window manually by running:

<CodeGroup>

```bash Terminal
interpreter --api_base http://localhost:1337/v1  --model <model_id> --context_window 5000
```

```python Python
from interpreter import interpreter

interpreter.context_window = 5000
```

</CodeGroup>

<Warning>
  If Jan is producing strange output, or no output at all, make sure to update
  to the latest version and clean your cache.
</Warning>

================
File: language-models/local-models/llamafile.mdx
================
---
title: LlamaFile
---

The easiest way to get started with local models in Open Interpreter is to run `interpreter --local` in the terminal, select LlamaFile, then go through the interactive set up process. This will download the model and start the server for you. If you choose to do it manually, you can follow the instructions below.

To use LlamaFile manually with Open Interpreter, you'll need to download the model and start the server by running the file in the terminal. You can do this with the following commands:

```bash
# Download Mixtral

wget https://huggingface.co/jartine/Mixtral-8x7B-v0.1.llamafile/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile

# Make it an executable

chmod +x mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile

# Start the server

./mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile

# In a separate terminal window, run OI and point it at the llamafile server

interpreter --api_base https://localhost:8080/v1
```

Please note that if you are using a Mac with Apple Silicon, you'll need to have Xcode installed.

================
File: language-models/local-models/lm-studio.mdx
================
---
title: LM Studio
---

Open Interpreter can use OpenAI-compatible server to run models locally. (LM Studio, jan.ai, ollama etc)

Simply run `interpreter` with the api_base URL of your inference server (for LM studio it is `http://localhost:1234/v1` by default):

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

Alternatively you can use Llamafile without installing any third party software just by running

```shell
interpreter --local
```

for a more detailed guide check out [this video by Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

**How to run LM Studio in the background.**

1. Download [https://lmstudio.ai/](https://lmstudio.ai/) then start it.
2. Select a model then click **↓ Download**.
3. Click the **↔️** button on the left (below 💬).
4. Select your model at the top, then click **Start Server**.

Once the server is running, you can begin your conversation with Open Interpreter.

(When you run the command `interpreter --local` and select LMStudio, these steps will be displayed.)

<Info>
  Local mode sets your `context_window` to 3000, and your `max_tokens` to 1000.
  If your model has different requirements, [set these parameters
  manually.](/settings#language-model)
</Info>

# Python

Compared to the terminal interface, our Python package gives you more granular control over each setting.

You can point `interpreter.llm.api_base` at any OpenAI compatible server (including one running locally).

For example, to connect to [LM Studio](https://lmstudio.ai/), use these settings:

```python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "openai/x" # Tells OI to send messages in OpenAI's format
interpreter.llm.api_key = "fake_key" # LiteLLM, which we use to talk to LM Studio, requires this
interpreter.llm.api_base = "http://localhost:1234/v1" # Point this at any OpenAI compatible server

interpreter.chat()
```

Simply ensure that **LM Studio**, or any other OpenAI compatible server, is running at `api_base`.

================
File: language-models/local-models/ollama.mdx
================
---
title: Ollama
---

Ollama is an easy way to get local language models running on your computer through a command-line interface.

To run Ollama with Open interpreter:

1. Download Ollama for your platform from [here](https://ollama.ai/download).

2. Open the installed Ollama application, and go through the setup, which will require your password.

3. Now you are ready to download a model. You can view all available models [here](https://ollama.ai/library). To download a model, run:

```bash
ollama run <model-name>
```

4. It will likely take a while to download, but once it does, we are ready to use it with Open Interpreter. You can either run `interpreter --local` to set it up interactively in the terminal, or do it manually:

<CodeGroup>

```bash Terminal
interpreter --model ollama/<model-name>
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "ollama_chat/<model-name>"
interpreter.llm.api_base = "http://localhost:11434"

interpreter.chat()
```

</CodeGroup>

For any future runs with Ollama, ensure that the Ollama server is running. If using the desktop application, you can check to see if the Ollama menu bar item is active.

<Warning>
  If Ollama is producing strange output, make sure to update to the latest
  version
</Warning>

================
File: language-models/custom-models.mdx
================
---
title: Custom Models
---

In addition to hosted and local language models, Open Interpreter also supports custom models.

As long as your system can accept an input and stream an output (and can be interacted with via a Python generator) it can be used as a language model in Open Interpreter.

Simply replace the OpenAI-compatible `completions` function in your language model with one of your own:

```python
def custom_language_model(messages, model, stream, max_tokens):
    """
    OpenAI-compatible completions function (this one just echoes what the user said back).
    To make it OpenAI-compatible and parsable, `choices` has to be the root property.
    The property `delta` is used to signify streaming.
    """
    users_content = messages[-1].get("content") # Get last message's content

    for character in users_content:
        yield {"choices": [{"delta": {"content": character}}]}

# Tell Open Interpreter to power the language model with this function

interpreter.llm.completions = custom_language_model
```

Then, set the following settings:

```
interpreter.llm.context_window = 2000 # In tokens
interpreter.llm.max_tokens = 1000 # In tokens
interpreter.llm.supports_vision = False # Does this completions endpoint accept images?
interpreter.llm.supports_functions = False # Does this completions endpoint accept/return function calls?
```

And start using it:

```
interpreter.chat("Hi!") # Returns/displays "Hi!" character by character
```

================
File: language-models/introduction.mdx
================
---
title: Introduction
---

**Open Interpreter** works with both hosted and local language models.

Hosted models are faster and more capable, but require payment. Local models are private and free, but are often less capable.

For this reason, we recommend starting with a **hosted** model, then switching to a local model once you've explored Open Interpreter's capabilities.

<CardGroup>

<Card title="Hosted setup" icon="cloud" href="/language-models/hosted-models">
  Connect to a hosted language model like GPT-4 **(recommended)**
</Card>

<Card title="Local setup" icon="microchip" href="/language-models/local-models">
  Setup a local language model like Mistral
</Card>

</CardGroup>

<br />
<br />

<Info>
  Thank you to the incredible [LiteLLM](https://litellm.ai/) team for their
  efforts in connecting Open Interpreter to hosted providers.
</Info>

================
File: language-models/settings.mdx
================
---
title: Settings
---

The `interpreter.llm` is responsible for running the language model.

[Click here](/settings/all-settings#language-model) to view `interpreter.llm` settings.

================
File: legal/license.mdx
================
---
title: Licenses
description: By using Interpreter, you agree to our Privacy Policy and Terms of Service
---

\n

# Interpreter Privacy Policy

Last updated: August 13, 2024

Open Interpreter, Inc. ("we," "our," or "us") is committed to protecting your privacy. This Privacy Policy explains how we collect, use, and safeguard your information when you use our AI desktop application, Interpreter ("the Application").

## 1. Information We Collect

We collect the following information:

a) Personal Information:
   - Name
   - Email address

b) Usage Information:
   - Conversations with the AI chatbot
   - Code generated during use of the Application

## 2. How We Use Your Information

We use the collected information to:

a) Provide and improve our services
b) Communicate with you about your account or the Application
c) Improve our underlying AI model

## 3. Data Anonymization

All conversations and generated code are anonymized before being used to improve our AI model. However, please be aware that if you explicitly instruct the AI to include personal identifiable information (PII) in the generated code, such information may be captured.

## 4. Data Security

We implement appropriate technical and organizational measures to protect your personal information. However, no method of transmission over the Internet or electronic storage is 100% secure.

## 5. Your Rights

You have the right to access, correct, or delete your personal information. Please contact us at help@openinterpreter.com for any data-related requests.

## 6. Changes to This Privacy Policy

We may update our Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page and updating the "Last updated" date.

## 7. Contact Us

If you have any questions about this Privacy Policy, please contact us at help@openinterpreter.com.

By using Interpreter, you agree to the collection and use of information in accordance with this Privacy Policy.

---

# Interpreter Terms of Service

Last updated: August 13, 2024

Please read these Terms of Service ("Terms", "Terms of Service") carefully before using the Interpreter desktop application (the "Service") operated by Open Interpreter, Inc. ("us", "we", or "our").

## 1. Acceptance of Terms

By accessing or using the Service, you agree to be bound by these Terms. If you disagree with any part of the terms, then you may not access the Service.

## 2. Description of Service

Interpreter is an AI-powered desktop application that allows users to interact with an AI chatbot to write and execute code.

## 3. User Responsibilities

By using our Service, you agree to:

a) Review ALL code generated by Interpreter before execution.
b) Grant explicit permission before any code is executed on your system.
c) Understand the implications of the code you choose to execute.
d) Use the Service in compliance with all applicable laws and regulations.

## 4. Safety Measures

We have implemented the following safety measures:

a) We employ LakeraGuard, an industry-leading solution, to assess potential harm in generated code.
b) Our custom judge layer provides explanations of what the code is intended to do.
c) You will always be asked for permission before any code is executed.

## 5. Assumption of Risk

By using Interpreter, you acknowledge and accept the following risks:

a) The application may generate code that, if executed, could alter or delete files on your system.
b) While we have implemented safety measures, the AI may occasionally generate code with unintended consequences.
c) In rare cases, the application might generate code that, if executed, could potentially expose sensitive information.

## 6. Limitation of Liability

To the fullest extent permitted by law, Open Interpreter, Inc. shall not be liable for any direct, indirect, incidental, special, consequential, or exemplary damages resulting from your use of the Service or any code generated or executed through the Service.

## 7. Indemnification

You agree to indemnify and hold harmless Open Interpreter, Inc., its officers, directors, employees, and agents from any claims, damages, losses, liabilities, and expenses (including legal fees) arising out of or related to your use of the Service or any code generated or executed through the Service.

## 8. Modifications to Terms

We reserve the right to modify these Terms at any time. Continued use of the Service after changes constitutes acceptance of the modified Terms.

## 9. Governing Law

These Terms shall be governed by and construed in accordance with the laws of [Your Jurisdiction], without regard to its conflict of law provisions.

## 10. Contact Us

If you have any questions about these Terms, please contact us at help@openinterpreter.com.

By using Interpreter, you acknowledge that you have read, understood, and agree to be bound by these Terms of Service.

================
File: protocols/lmc-messages.mdx
================
---
title: LMC Messages
---

To support the incoming `L`anguage `M`odel `C`omputer architecture, we extend OpenAI's messages format to include additional information, and a new role called `computer`:

```python
# The user sends a message.
{"role": "user", "type": "message", "content": "What's 2380*3875?"}

# The assistant runs some code.
{"role": "assistant", "type": "code", "format": "python", "content": "2380*3875"}

# The computer responds with the result of the code.
{"role": "computer", "type": "console", "format": "output", "content": "9222500"}

# The assistant sends a message.
{"role": "assistant", "type": "message", "content": "The result of multiplying 2380 by 3875 is 9222500."}
```

## Anatomy

Each message in the LMC architecture has the following parameters (`format` is only present for some types):

```
{
  "role": "<role>",       # Who is sending the message.
  "type": "<type>",       # What kind of message is being sent.
  "format": "<format>"    # Some types need to be further specified, so they optionally use this parameter.
  "content": "<content>", # What the message says.
}
```

Parameter|Description|
---|---|
`role`|The sender of the message.|
`type`|The kind of message being sent.|
`content`|The actual content of the message.|
`format`|The format of the content (optional).|

## Roles

Role|Description|
---|---|
`user`|The individual interacting with the system.|
`assistant`|The language model.|
`computer`|The system that executes the language model's commands.|

## Possible Message Types / Formats

Any role can produce any of the following formats, but we've included a `Common Roles` column to give you a sense of the message type's usage.

Type|Format|Content Description|Common Roles
---|---|---|---|
message|None|A text-only message.|`user`, `assistant`|
console|active_line|The active line of code (from the most recent code block) that's executing.|`computer`|
console|output|Text output resulting from `print()` statements in Python, `console.log()` statements in Javascript, etc. **This includes errors.**|`computer`|
image|base64|A `base64` image in PNG format (default)|`user`, `computer`|
image|base64.png|A `base64` image in PNG format|`user`, `computer`|
image|base64.jpeg|A `base64` image in JPEG format|`user`, `computer`|
image|path|A path to an image.|`user`, `computer`|
code|html|HTML code that should be executed.|`assistant`, `computer`|
code|javascript|JavaScript code that should be executed.|`assistant`, `computer`|
code|python|Python code that should be executed.|`assistant`|
code|r|R code that should be executed.|`assistant`|
code|applescript|AppleScript code that should be executed.|`assistant`|
code|shell|Shell code that should be executed.|`assistant`|
audio|wav|audio in wav format for websocket.|`user`|

================
File: safety/best-practices.mdx
================
---
title: Best Practices
---

LLM's are not perfect. They can make mistakes, they can be tricked into doing things that they shouldn't, and they are capable of writing unsafe code. This page will help you understand how to use these LLM's safely.

## Best Practices

- Avoid asking it to perform potentially risky tasks. This seems obvious, but it's the number one way to prevent safety mishaps.

- Run it in a sandbox. This is the safest way to run it, as it completely isolates the code it runs from the rest of your system.

- Use trusted models. Yes, Open Interpreter can be configured to run pretty much any text-based model on huggingface. But it does not mean it's a good idea to run any random model you find. Make sure you trust the models you're using. If you're not sure, run it in a sandbox. Nefarious LLM's are becoming a real problem, and they are not going away anytime soon.

- Local models are fun! But GPT-4 is probably your safest bet. OpenAI has their models aligned in a major way. It will outperform the local models, and it will generally refuse to run unsafe code, as it truly understands that the code it writes could be run. It has a pretty good idea what unsafe code looks like, and will refuse to run code like `rm -rf /` that would delete your entire disk, for example.

- The [--safe_mode](/safety/safe-mode) argument is your friend. It enables code scanning, and can use [guarddog](https://github.com/DataDog/guarddog) to identify malicious PyPi and npm packages. It's not a perfect solution, but it's a great start.

================
File: safety/introduction.mdx
================
---
title: Introduction
---

Safety is a top priority for us at Open Interpreter. Running LLM generated code on your computer is inherently risky, and we have taken steps to make it as safe as possible. One of the primary safety 'mechanisms', is the alignment of the LLM itself. GPT-4 refuses to run dangerous code like `rm -rf /`, it understands what that command will do, and won't let you footgun yourself. This is less applicable when running local models like Mistral, that have little or no alignment, making our other safety measures more important.

# Safety Measures

- [Safe mode](/safety/safe-mode) enables code scanning, as well as the ability to scan packages with [guarddog](https://github.com/DataDog/guarddog) with a simple change to the system message. See the [safe mode docs](/safety/safe-mode) for more information.

- Requiring confirmation with the user before the code is actually run. This is a simple measure that can prevent a lot of accidents. It exists as another layer of protection, but can be disabled with the `--auto-run` flag if you wish.

- Sandboxing code execution. Open Interpreter can be run in a sandboxed environment using [Docker](/integrations/docker). This is a great way to run code without worrying about it affecting your system. Docker support is currently experimental, but we are working on making it a core feature of Open Interpreter. Another option for sandboxing is [E2B](https://e2b.dev/), which overrides the default python language with a sandboxed, hosted version of python through E2B. Follow [this guide](/integrations/e2b) to set it up.

## Notice

<Warning>
  Open Interpreter is not responsible for any damage caused by using the
  package. These safety measures provide no guarantees of safety or security.
  Please be careful when running code generated by Open Interpreter, and make
  sure you understand what it will do before running it.
</Warning>

================
File: safety/isolation.mdx
================
---
title: Isolation
---

Isolating Open Interpreter from your system is helpful to prevent security mishaps. By running it in a separate process, you can ensure that actions taken by Open Interpreter will not directly affect your system. This is by far the safest way to run Open Interpreter, although it can be limiting based on your use case.

If you wish to sandbox Open Interpreter, we have two primary methods of doing so: Docker and E2B.

## Docker

Docker is a containerization technology that allows you to run an isolated Linux environment on your system. This allows you to run Open Interpreter in a container, which **completely** isolates it from your system. All code execution is done in the container, and the container is not able to access your system. Docker support is currently experimental, and we are working on integrating it as a core feature of Open Interpreter.

Follow [these instructions](/integrations/docker) to get it running.

## E2B

[E2B](https://e2b.dev/) is a cloud-based platform for running sandboxed code environments, designed for use by AI agents. You can override the default `python` language in Open Interpreter to use E2B, and it will automatically run the code in a cloud-sandboxed environment. You will need an E2B account to use this feature. It's worth noting that this will only sandbox python code, other languages like shell and JavaScript will still be run on your system.

Follow [these instructions](/integrations/e2b) to get it running.

================
File: safety/safe-mode.mdx
================
---
title: Safe Mode
---

# Safe Mode

**⚠️ Safe mode is experimental and does not provide any guarantees of safety or security.**

Open Interpreter is working on providing an experimental safety toolkit to help you feel more confident running the code generated by Open Interpreter.

Install Open Interpreter with the safety toolkit dependencies as part of the bundle:

```shell
pip install open-interpreter[safe]
```

Alternatively, you can install the safety toolkit dependencies separately in your virtual environment:

```shell
pip install semgrep
```

## Features

- **No Auto Run**: Safe mode disables the ability to automatically execute code
- **Code Scanning**: Scan generated code for vulnerabilities with [`semgrep`](https://semgrep.dev/)

## Enabling Safe Mode

You can enable safe mode by passing the `--safe` flag when invoking `interpreter` or by configuring `safe_mode` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration).

The safe mode setting has three options:

- `off`: disables the safety toolkit (_default_)
- `ask`: prompts you to confirm that you want to scan code
- `auto`: automatically scans code

### Example Config:

```yaml
model: gpt-4
temperature: 0
verbose: false
safe_mode: ask
```

## Roadmap

Some upcoming features that enable even more safety:

- [Execute code in containers](https://github.com/OpenInterpreter/open-interpreter/pull/459)

## Tips & Tricks

You can adjust the `custom_instructions` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration) to include instructions for the model to scan packages with [guarddog](https://github.com/DataDog/guarddog) before installing them.

```yaml
model: gpt-4
verbose: false
safe_mode: ask
system_message: |
  # normal system message here
  BEFORE INSTALLING ANY PACKAGES WITH pip OR npm YOU MUST SCAN THEM WITH `guarddog` FIRST. Run `guarddog pypi scan $package` for pip packages and `guarddog npm scan $package` for npm packages. `guarddog` only accepts one package name at a time.
```

================
File: server/usage.mdx
================
# Server Usage Guide

## Starting the Server

### From Command Line
To start the server from the command line, use:

```bash
interpreter --server
```

### From Python
To start the server from within a Python script:

```python
from interpreter import AsyncInterpreter

async_interpreter = AsyncInterpreter()
async_interpreter.server.run(port=8000)  # Default port is 8000, but you can customize it
```

## WebSocket API

### Establishing a Connection
Connect to the WebSocket server at `ws://localhost:8000/`.

### Message Format
Open Interpreter uses an extended version of OpenAI's message format called [LMC messages](https://docs.openinterpreter.com/protocols/lmc-messages) that allow for rich, multi-part messages. **Messages must be sent between start and end flags.** Here's the basic structure:

```json
{"role": "user", "start": true}
{"role": "user", "type": "message", "content": "Your message here"}
{"role": "user", "end": true}
```

### Multi-part Messages
You can send complex messages with multiple components:

1. Start with `{"role": "user", "start": true}`
2. Add various types of content (message, file, image, etc.)
3. End with `{"role": "user", "end": true}`

### Content Types
You can include various types of content in your messages:

- Text messages: `{"role": "user", "type": "message", "content": "Your text here"}`
- File paths: `{"role": "user", "type": "file", "content": "path/to/file"}`
- Images: `{"role": "user", "type": "image", "format": "path", "content": "path/to/photo"}`
- Audio: `{"role": "user", "type": "audio", "format": "wav", "content": "path/to/audio.wav"}`

### Control Commands
To control the server's behavior, send the following commands:

1. Stop execution:
   ```json
   {"role": "user", "type": "command", "content": "stop"}
   ```
   This stops all execution and message processing.

2. Execute code block:
   ```json
   {"role": "user", "type": "command", "content": "go"}
   ```
   This executes a generated code block and allows the agent to proceed.

   **Note**: If `auto_run` is set to `False`, the agent will pause after generating code blocks. You must send the "go" command to continue execution.

### Completion Status
The server indicates completion with the following message:
```json
{"role": "server", "type": "status", "content": "complete"}
```
Ensure your client watches for this message to determine when the interaction is finished.

### Error Handling
If an error occurs, the server will send an error message in the following format:
```json
{"role": "server", "type": "error", "content": "Error traceback information"}
```
Your client should be prepared to handle these error messages appropriately.

## Code Execution Review

After code blocks are executed, you'll receive a review message:

```json
{
  "role": "assistant",
  "type": "review",
  "content": "Review of the executed code, including safety assessment and potential irreversible actions."
}
```

This review provides important information about the safety and potential impact of the executed code. Pay close attention to these messages, especially when dealing with operations that might have significant effects on your system.

The `content` field of the review message may have two possible formats:

1. If the code is deemed completely safe, the content will be exactly `"<SAFE>"`.
2. Otherwise, it will contain an explanation of why the code might be unsafe or have irreversible effects.

Example of a safe code review:
```json
{
  "role": "assistant",
  "type": "review",
  "content": "<SAFE>"
}
```

Example of a potentially unsafe code review:
```json
{
  "role": "assistant",
  "type": "review",
  "content": "This code performs file deletion operations which are irreversible. Please review carefully before proceeding."
}
```

## Example WebSocket Interaction

Here's an example demonstrating the WebSocket interaction:

```python
import websockets
import json
import asyncio

async def websocket_interaction():
    async with websockets.connect("ws://localhost:8000/") as websocket:
        # Send a multi-part user message
        await websocket.send(json.dumps({"role": "user", "start": True}))
        await websocket.send(json.dumps({"role": "user", "type": "message", "content": "Analyze this image:"}))
        await websocket.send(json.dumps({"role": "user", "type": "image", "format": "path", "content": "path/to/image.jpg"}))
        await websocket.send(json.dumps({"role": "user", "end": True}))

        # Receive and process messages
        while True:
            message = await websocket.recv()
            data = json.loads(message)
            
            if data.get("type") == "message":
                print(f"Assistant: {data.get('content', '')}")
            elif data.get("type") == "review":
                print(f"Code Review: {data.get('content')}")
            elif data.get("type") == "error":
                print(f"Error: {data.get('content')}")
            elif data == {"role": "assistant", "type": "status", "content": "complete"}:
                print("Interaction complete")
                break

asyncio.run(websocket_interaction())
```

## HTTP API

### Modifying Settings
To change server settings, send a POST request to `http://localhost:8000/settings`. The payload should conform to [the interpreter object's settings](https://docs.openinterpreter.com/settings/all-settings).

Example:
```python
import requests

settings = {
    "llm": {"model": "gpt-4"},
    "custom_instructions": "You only write Python code.",
    "auto_run": True,
}
response = requests.post("http://localhost:8000/settings", json=settings)
print(response.status_code)
```

### Retrieving Settings
To get current settings, send a GET request to `http://localhost:8000/settings/{property}`.

Example:
```python
response = requests.get("http://localhost:8000/settings/custom_instructions")
print(response.json())
# Output: {"custom_instructions": "You only write Python code."}
```

## OpenAI-Compatible Endpoint

The server provides an OpenAI-compatible endpoint at `/openai`. This allows you to use the server with any tool or library that's designed to work with the OpenAI API.

### Chat Completions Endpoint

The chat completions endpoint is available at:

```
[server_url]/openai/chat/completions
```

To use this endpoint, set the `api_base` in your OpenAI client or configuration to `[server_url]/openai`. For example:

```python
import openai

openai.api_base = "http://localhost:8000/openai"  # Replace with your server URL if different
openai.api_key = "dummy"  # The key is not used but required by the OpenAI library

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",  # This model name is ignored, but required
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the capital of France?"}
    ]
)

print(response.choices[0].message['content'])
```

Note that only the chat completions endpoint (`/chat/completions`) is implemented. Other OpenAI API endpoints are not available.

When using this endpoint:
- The `model` parameter is required but ignored.
- The `api_key` is required by the OpenAI library but not used by the server.

## Using Docker

You can also run the server using Docker. First, build the Docker image from the root of the repository:

```bash
docker build -t open-interpreter .
```

Then, run the container:

```bash
docker run -p 8000:8000 open-interpreter
```

This will expose the server on port 8000 of your host machine.

## Acknowledgment Feature

When the `INTERPRETER_REQUIRE_ACKNOWLEDGE` environment variable is set to `"True"`, the server requires clients to acknowledge each message received. This feature ensures reliable message delivery in environments where network stability might be a concern.

### How it works

1. When this feature is enabled, each message sent by the server will include an `id` field.
2. The client must send an acknowledgment message back to the server for each received message.
3. The server will wait for this acknowledgment before sending the next message.

### Client Implementation

To implement this on the client side:

1. Check if each received message contains an `id` field.
2. If an `id` is present, send an acknowledgment message back to the server.

Here's an example of how to handle this in your WebSocket client:

```python
import json
import websockets

async def handle_messages(websocket):
    async for message in websocket:
        data = json.loads(message)
        
        # Process the message as usual
        print(f"Received: {data}")

        # Check if the message has an ID that needs to be acknowledged
        if "id" in data:
            ack_message = {
                "ack": data["id"]
            }
            await websocket.send(json.dumps(ack_message))
            print(f"Sent acknowledgment for message {data['id']}")

async def main():
    uri = "ws://localhost:8000"
    async with websockets.connect(uri) as websocket:
        await handle_messages(websocket)

# Run the async function
import asyncio
asyncio.run(main())
```

### Server Behavior

- If the server doesn't receive an acknowledgment within a certain timeframe, it will attempt to resend the message.
- The server will make multiple attempts to send a message before considering it failed.

### Enabling the Feature

To enable this feature, set the `INTERPRETER_REQUIRE_ACKNOWLEDGE` environment variable to `"True"` before starting the server:

```bash
export INTERPRETER_REQUIRE_ACKNOWLEDGE="True"
interpreter --server
```

Or in Python:

```python
import os
os.environ["INTERPRETER_REQUIRE_ACKNOWLEDGE"] = "True"

from interpreter import AsyncInterpreter
async_interpreter = AsyncInterpreter()
async_interpreter.server.run()
```

## Advanced Usage: Accessing the FastAPI App Directly

The FastAPI app is exposed at `async_interpreter.server.app`. This allows you to add custom routes or host the app using Uvicorn directly.

Example of adding a custom route and hosting with Uvicorn:

```python
from interpreter import AsyncInterpreter
from fastapi import FastAPI
import uvicorn

async_interpreter = AsyncInterpreter()
app = async_interpreter.server.app

@app.get("/custom")
async def custom_route():
    return {"message": "This is a custom route"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## Best Practices

1. Always handle the "complete" status message to ensure your client knows when the server has finished processing.
2. If `auto_run` is set to `False`, remember to send the "go" command to execute code blocks and continue the interaction.
3. Implement proper error handling in your client to manage potential connection issues, unexpected server responses, or server-sent error messages.
4. Use the AsyncInterpreter class when working with the server in Python to ensure compatibility with asynchronous operations.
5. Pay attention to the code execution review messages for important safety and operational information.
6. Utilize the multi-part user message structure for complex inputs, including file paths and images.
7. When sending file paths or image paths, ensure they are accessible to the server.

================
File: settings/all-settings.mdx
================
---
title: All Settings
---

<CardGroup cols={3}>

<Card title="Language Model Settings" icon="microchip" href="#language-model">
  Set your `model`, `api_key`, `temperature`, etc.
</Card>

<Card
  title="Interpreter Settings"
  icon="circle"
  iconType="solid"
  href="#interpreter"
>
  Change your `system_message`, set your interpreter to run `offline`, etc.
</Card>
<Card
  title="Code Execution Settings"
  icon="code"
  iconType="solid"
  href="#computer"
>
  Modify the `interpreter.computer`, which handles code execution.
</Card>

</CardGroup>

# Language Model

### Model Selection

Specifies which language model to use. Check out the [models](/language-models/) section for a list of available models. Open Interpreter uses [LiteLLM](https://github.com/BerriAI/litellm) under the hood to support over 100+ models.

<CodeGroup>

```bash Terminal
interpreter --model "gpt-3.5-turbo"
```

```python Python
interpreter.llm.model = "gpt-3.5-turbo"
```

```yaml Profile
llm:
  model: gpt-3.5-turbo
```

</CodeGroup>

### Temperature

Sets the randomness level of the model's output. The default temperature is 0, you can set it to any value between 0 and 1. The higher the temperature, the more random and creative the output will be.

<CodeGroup>

```bash Terminal
interpreter --temperature 0.7
```

```python Python
interpreter.llm.temperature = 0.7
```

```yaml Profile
llm:
  temperature: 0.7
```

</CodeGroup>

### Context Window

Manually set the context window size in tokens for the model. For local models, using a smaller context window will use less RAM, which is more suitable for most devices.

<CodeGroup>

```bash Terminal
interpreter --context_window 16000
```

```python Python
interpreter.llm.context_window = 16000
```

```yaml Profile
llm:
  context_window: 16000
```

</CodeGroup>

### Max Tokens

Sets the maximum number of tokens that the model can generate in a single response.

<CodeGroup>

```bash Terminal
interpreter --max_tokens 100
```

```python Python
interpreter.llm.max_tokens = 100
```

```yaml Profile
llm:
  max_tokens: 100
```

</CodeGroup>

### Max Output

Set the maximum number of characters for code outputs.

<CodeGroup>

```bash Terminal
interpreter --max_output 1000
```

```python Python
interpreter.llm.max_output = 1000
```

```yaml Profile
llm:
  max_output: 1000
```

</CodeGroup>

### API Base

If you are using a custom API, specify its base URL with this argument.

<CodeGroup>

```bash Terminal
interpreter --api_base "https://api.example.com"
```

```python Python
interpreter.llm.api_base = "https://api.example.com"
```

```yaml Profile
llm:
  api_base: https://api.example.com
```

</CodeGroup>

### API Key

Set your API key for authentication when making API calls. For OpenAI models, you can get your API key [here](https://platform.openai.com/api-keys).

<CodeGroup>

```bash Terminal
interpreter --api_key "your_api_key_here"
```

```python Python
interpreter.llm.api_key = "your_api_key_here"
```

```yaml Profile
llm:
  api_key: your_api_key_here
```

</CodeGroup>

### API Version

Optionally set the API version to use with your selected model. (This will override environment variables)

<CodeGroup>

```bash Terminal
interpreter --api_version 2.0.2
```

```python Python
interpreter.llm.api_version = '2.0.2'
```

```yaml Profile
llm:
  api_version: 2.0.2
```

</CodeGroup>

### LLM Supports Functions

Inform Open Interpreter that the language model you're using supports function calling.

<CodeGroup>

```bash Terminal
interpreter --llm_supports_functions
```

```python Python
interpreter.llm.supports_functions = True
```

```yaml Profile
llm:
  supports_functions: true
```

</CodeGroup>

### LLM Does Not Support Functions

Inform Open Interpreter that the language model you're using does not support function calling.

<CodeGroup>

```bash Terminal
interpreter --no-llm_supports_functions
```

```python Python
interpreter.llm.supports_functions = False
```

```yaml Profile
llm:
  supports_functions: false
```

</CodeGroup>

### Execution Instructions

If `llm.supports_functions` is `False`, this value will be added to the system message. This parameter tells language models how to execute code. This can be set to an empty string or to `False` if you don't want to tell the LLM how to do this.

<CodeGroup>

````python Python
interpreter.llm.execution_instructions = "To execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language."
````

````python Profile
interpreter.llm.execution_instructions = "To execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language."
````

</CodeGroup>

### LLM Supports Vision

Inform Open Interpreter that the language model you're using supports vision. Defaults to `False`.

<CodeGroup>

```bash Terminal
interpreter --llm_supports_vision
```

```python Python
interpreter.llm.supports_vision = True
```

```yaml Profile
llm:
  supports_vision: true
```

</CodeGroup>

# Interpreter

### Vision Mode

Enables vision mode, which adds some special instructions to the prompt and switches to `gpt-4o`.

<CodeGroup>
```bash Terminal
interpreter --vision
```

```python Python
interpreter.llm.model = "gpt-4o" # Any vision supporting model
interpreter.llm.supports_vision = True
interpreter.llm.supports_functions = True

interpreter.custom_instructions = """The user will show you an image of the code you write. You can view images directly.
For HTML: This will be run STATELESSLY. You may NEVER write '<!-- previous code here... --!>' or `<!-- header will go here -->` or anything like that. It is CRITICAL TO NEVER WRITE PLACEHOLDERS. Placeholders will BREAK it. You must write the FULL HTML CODE EVERY TIME. Therefore you cannot write HTML piecemeal—write all the HTML, CSS, and possibly Javascript **in one step, in one code block**. The user will help you review it visually.
If the user submits a filepath, you will also see the image. The filepath and user image will both be in the user's message.
If you use `plt.show()`, the resulting image will be sent to you. However, if you use `PIL.Image.show()`, the resulting image will NOT be sent to you."""
```

```yaml Profile
loop: True

llm:
  model: "gpt-4o"
  temperature: 0
  supports_vision: True
  supports_functions: True
  context_window: 110000
  max_tokens: 4096
  custom_instructions: >
    The user will show you an image of the code you write. You can view images directly.
    For HTML: This will be run STATELESSLY. You may NEVER write '<!-- previous code here... --!>' or `<!-- header will go here -->` or anything like that. It is CRITICAL TO NEVER WRITE PLACEHOLDERS. Placeholders will BREAK it. You must write the FULL HTML CODE EVERY TIME. Therefore you cannot write HTML piecemeal—write all the HTML, CSS, and possibly Javascript **in one step, in one code block**. The user will help you review it visually.
    If the user submits a filepath, you will also see the image. The filepath and user image will both be in the user's message.
    If you use `plt.show()`, the resulting image will be sent to you. However, if you use `PIL.Image.show()`, the resulting image will NOT be sent to you.
```

</CodeGroup>

### OS Mode

Enables OS mode for multimodal models. Currently not available in Python. Check out more information on OS mode [here](/guides/os-mode).

<CodeGroup>

```bash Terminal
interpreter --os
```

```yaml Profile
os: true
```

</CodeGroup>

### Version

Get the current installed version number of Open Interpreter.

<CodeGroup>

```bash Terminal
interpreter --version
```

</CodeGroup>

### Open Local Models Directory

Opens the models directory. All downloaded Llamafiles are saved here.

<CodeGroup>

```bash Terminal
interpreter --local_models
```

</CodeGroup>

### Open Profiles Directory

Opens the profiles directory. New yaml profile files can be added to this directory.

<CodeGroup>

```bash Terminal
interpreter --profiles
```

</CodeGroup>

### Select Profile

Select a profile to use. If no profile is specified, the default profile will be used.

<CodeGroup>

```bash Terminal
interpreter --profile local.yaml
```

</CodeGroup>

### Help

Display all available terminal arguments.

<CodeGroup>

```bash Terminal
interpreter --help
```

</CodeGroup>

### Loop (Force Task Completion)

Runs Open Interpreter in a loop, requiring it to admit to completing or failing every task.

<CodeGroup>

```bash Terminal
interpreter --loop
```

```python Python
interpreter.loop = True
```

```yaml Profile
loop: true
```

</CodeGroup>

### Verbose

Run the interpreter in verbose mode. Debug information will be printed at each step to help diagnose issues.

<CodeGroup>

```bash Terminal
interpreter --verbose
```

```python Python
interpreter.verbose = True
```

```yaml Profile
verbose: true
```

</CodeGroup>

### Safe Mode

Enable or disable experimental safety mechanisms like code scanning. Valid options are `off`, `ask`, and `auto`.

<CodeGroup>

```bash Terminal
interpreter --safe_mode ask
```

```python Python
interpreter.safe_mode = 'ask'
```

```yaml Profile
safe_mode: ask
```

</CodeGroup>

### Auto Run

Automatically run the interpreter without requiring user confirmation.

<CodeGroup>

```bash Terminal
interpreter --auto_run
```

```python Python
interpreter.auto_run = True
```

```yaml Profile
auto_run: true
```

</CodeGroup>

### Max Budget

Sets the maximum budget limit for the session in USD.

<CodeGroup>

```bash Terminal
interpreter --max_budget 0.01
```

```python Python
interpreter.max_budget = 0.01
```

```yaml Profile
max_budget: 0.01
```

</CodeGroup>

### Local Mode

Run the model locally. Check the [models page](/language-models/local-models/lm-studio) for more information.

<CodeGroup>

```bash Terminal
interpreter --local
```

```python Python
from interpreter import interpreter

interpreter.offline = True # Disables online features like Open Procedures
interpreter.llm.model = "openai/x" # Tells OI to send messages in OpenAI's format
interpreter.llm.api_key = "fake_key" # LiteLLM, which we use to talk to local models, requires this
interpreter.llm.api_base = "http://localhost:1234/v1" # Point this at any OpenAI compatible server

interpreter.chat()
```

```yaml Profile
local: true
```

</CodeGroup>

### Fast Mode

Sets the model to gpt-3.5-turbo and encourages it to only write code without confirmation.

<CodeGroup>

```bash Terminal
interpreter --fast
```

```yaml Profile
fast: true
```

</CodeGroup>

### Custom Instructions

Appends custom instructions to the system message. This is useful for adding information about your system, preferred languages, etc.

<CodeGroup>

```bash Terminal
interpreter --custom_instructions "This is a custom instruction."
```

```python Python
interpreter.custom_instructions = "This is a custom instruction."
```

```yaml Profile
custom_instructions: "This is a custom instruction."
```

</CodeGroup>

### System Message

We don't recommend modifying the system message, as doing so opts you out of future updates to the core system message. Use `--custom_instructions` instead, to add relevant information to the system message. If you must modify the system message, you can do so by using this argument, or by changing a profile file.

<CodeGroup>

```bash Terminal
interpreter --system_message "You are Open Interpreter..."
```

```python Python
interpreter.system_message = "You are Open Interpreter..."
```

```yaml Profile
system_message: "You are Open Interpreter..."
```

</CodeGroup>

### Disable Telemetry

Opt out of [telemetry](telemetry/telemetry).

<CodeGroup>

```bash Terminal
interpreter --disable_telemetry
```

```python Python
interpreter.anonymized_telemetry = False
```

```yaml Profile
disable_telemetry: true
```

</CodeGroup>

### Offline

This boolean flag determines whether to enable or disable some offline features like [open procedures](https://open-procedures.replit.app/). Use this in conjunction with the `model` parameter to set your language model.

<CodeGroup>

```python Python
interpreter.offline = True
```

```bash Terminal
interpreter --offline true
```

```yaml Profile
offline: true
```

</CodeGroup>

### Messages

This property holds a list of `messages` between the user and the interpreter.

You can use it to restore a conversation:

```python
interpreter.chat("Hi! Can you print hello world?")

print(interpreter.messages)

# This would output:

# [
#    {
#       "role": "user",
#       "message": "Hi! Can you print hello world?"
#    },
#    {
#       "role": "assistant",
#       "message": "Sure!"
#    }
#    {
#       "role": "assistant",
#       "language": "python",
#       "code": "print('Hello, World!')",
#       "output": "Hello, World!"
#    }
# ]

#You can use this to restore `interpreter` to a previous conversation.
interpreter.messages = messages # A list that resembles the one above
```

### User Message Template

A template applied to the User's message. `{content}` will be replaced with the user's message, then sent to the language model.

<CodeGroup>

````python Python
interpreter.user_message_template = "{content} Please send me some code that would be able to answer my question, in the form of ```python\n... the code ...\n``` or ```shell\n... the code ...\n```"
````

```python Profile
interpreter.user_message_template = "{content}. Be concise, don't include anything unnecessary. Don't use placeholders, I can't edit code."
```

</CodeGroup>

### Always Apply User Message Template

The boolean flag for whether the User Message Template will be applied to every user message. The default is False which means the template is only applied to the last User message.

<CodeGroup>

```python Python
interpreter.always_apply_user_message_template = False
```

```python Profile
interpreter.always_apply_user_message_template = False
```

</CodeGroup>

### Code Message Template

A template applied to the Computer's output after running code. `{content}` will be replaced with the computer's output, then sent to the language model.

<CodeGroup>

```python Python
interpreter.code_output_template = "Code output: {content}\nWhat does this output mean / what's next (if anything, or are we done)?"
```

```python Profile
interpreter.code_output_template = "Code output: {content}\nWhat code needs to be run next?"
```

</CodeGroup>

### Empty Code Message Template

If the computer does not output anything after code execution, this value will be sent to the language model.

<CodeGroup>

```python Python
interpreter.empty_code_output_template = "The code above was executed on my machine. It produced no text output. what's next (if anything, or are we done?)"
```

```python Profile
interpreter.empty_code_output_template = "The code above was executed on my machine. It produced no text output. what's next?"
```

</CodeGroup>

### Code Output Sender

This field determines whether the computer / code output messages are sent as the assistant or as the user. The default is user.

<CodeGroup>

```python Python
interpreter.code_output_sender = "user"
```

```python Profile
interpreter.code_output_sender = "assistant"
```

</CodeGroup>

# Computer

The `computer` object in `interpreter.computer` is a virtual computer that the AI controls. Its primary interface/function is to execute code and return the output in real-time.

### Offline

Running the `computer` in offline mode will disable some online features, like the hosted [Computer API](https://api.openinterpreter.com/). Inherits from `interpreter.offline`.

<CodeGroup>

```python Python
interpreter.computer.offline = True
```

```yaml Profile
computer.offline: True
```

</CodeGroup>

### Verbose

This is primarily used for debugging `interpreter.computer`. Inherits from `interpreter.verbose`.

<CodeGroup>

```python Python
interpreter.computer.verbose = True
```

```yaml Profile
computer.verbose: True
```

</CodeGroup>

### Emit Images

The `emit_images` attribute in `interpreter.computer` controls whether the computer should emit images or not. This is inherited from `interpreter.llm.supports_vision`.

This is used for multimodel vs. text only models. Running `computer.display.view()` will return an actual screenshot for multimodal models if `emit_images` is True. If it's False, `computer.display.view()` will return all the text on the screen.

Many other functions of the computer can produce image/text outputs, and this parameter controls that.

<CodeGroup>

```python Python
interpreter.computer.emit_images = True
```

```yaml Profile
computer.emit_images: True
```

</CodeGroup>

### Import Computer API

Include the computer API in the system message. The default is False and won't import the computer API automatically

<CodeGroup>

```python Python
interpreter.computer.import_computer_api = True
```

```yaml Profile
computer.import_computer_api: True
```

</CodeGroup>
````

================
File: settings/example-profiles.mdx
================
---
title: Example Profiles
---

### OS Mode

```yaml
os: True
custom_instructions: "Always use Safari as the browser, and use Raycast instead of spotlight search by pressing option + space."
```

================
File: settings/profiles.mdx
================
---
title: Profiles
---

Profiles are preconfigured settings for Open Interpreter that make it easy to get going quickly with a specific set of settings. Any [setting](/settings/all-settings) can be configured in a profile. Custom instructions are helpful to have in each profile, to customize the behavior of Open Interpreter for the specific use case that the profile is designed for.

To load a profile, run:

```bash
interpreter --profile <profile_name>.yaml

```

All profiles are stored in their own folder, which can be accessed by running:

```bash
interpreter --profile

```

To create your own profile, you can add a `.yaml` file to this folder and add whatever [settings](/settings/all-settings) you'd like:

```yaml
custom_instructions: "Always use python, and be as concise as possible"
llm.model: gpt-4
llm.temperature: 0.5
# Any other settings you'd like to add
```

Any profile named 'default.yaml' will be loaded by default.

Profiles can be shared with others by sending them the profile yaml file!

================
File: telemetry/telemetry.mdx
================
---
title: Introduction
---

Open Interpreter contains a telemetry feature that collects **anonymous** usage information.

We use this information to help us understand how OI is used, to help us prioritize work on new features and bug fixes, and to help us improve OI's performance and stability.

# Opting out

If you prefer to opt out of telemetry, you can do this in two ways.

### Python

Set `disable_telemetry` to `true` on the `interpreter` object:

```python
from interpreter import interpreter
interpreter.disable_telemetry = True
```

### Terminal

Use the `--disable_telemetry` flag:

```shell
interpreter --disable_telemetry
```

### Profile

Set `disable_telemetry` to `true`. This will persist to future terminal sessions:

```yaml
disable_telemetry: true
```

### Environment Variables

Set `DISABLE_TELEMETRY` to `true` in your shell or server environment.

If you are running Open Interpreter on your local computer with `docker-compose` you can set this value in an `.env` file placed in the same directory as the `docker-compose.yml` file:

```
DISABLE_TELEMETRY=true
```

# What do you track?

We will only track usage details that help us make product decisions, specifically:

- Open Interpreter version and environment (i.e whether or not it's running in Python / a terminal)
- When interpreter.chat is run, in what mode (e.g `--os` mode), and the type of the message being passed in (e.g `None`, `str`, or `list`)
- Exceptions that occur within Open Interpreter (not tracebacks)

We **do not** collect personally-identifiable or sensitive information, such as: usernames, hostnames, file names, environment variables, or hostnames of systems being tested.

To view the list of events we track, you may reference the **[code](https://github.com/OpenInterpreter/open-interpreter/tree/main/interpreter/core)**

## Where is telemetry information stored?

We use **[Posthog](https://posthog.com/)** to store and visualize telemetry data.

<Info>
  Posthog is an open source platform for product analytics. Learn more about
  Posthog on **[posthog.com](https://posthog.com/)** or
  **[github.com/posthog](https://github.com/posthog/posthog)**
</Info>

================
File: troubleshooting/faq.mdx
================
---
title: "FAQ"
description: "Frequently Asked Questions"
---

<Accordion title="Does Open Interpreter ensure that my data doesn't leave my computer?">
  As long as you're using a local language model, your messages / personal info
  won't leave your computer. If you use a cloud model, we send your messages +
  custom instructions to the model. We also have a basic telemetry
  [function](https://github.com/OpenInterpreter/open-interpreter/blob/main/interpreter/core/core.py#L167)
  (copied over from ChromaDB's telemetry) that anonymously tracks usage. This
  only lets us know if a message was sent, includes no PII. OI errors will also
  be reported here which includes the exception string. Detailed docs on all
  this is [here](/telemetry/telemetry), and you can opt out by running
  `--local`, `--offline`, or `--disable_telemetry`.
</Accordion>

================
File: usage/desktop/help.md
================
Reach out to help@openinterpreter.com for support.

================
File: usage/desktop/install.mdx
================
---
title: Desktop App
---

Our desktop application is currently in development and is not yet available to the public.

You can apply for early access [here](https://0ggfznkwh4j.typeform.com/to/G21i9lJ2?typeform-source=docs.openinterpreter.com).

================
File: usage/python/arguments.mdx
================
---
title: Arguments
---

<Card
  title="New: Streaming responses in Python"
  icon="arrow-up-right"
  href="/usage/python/streaming-response"
>
  Learn how to build Open Interpreter into your application.
</Card>

#### `messages`

This property holds a list of `messages` between the user and the interpreter.

You can use it to restore a conversation:

```python
interpreter.chat("Hi! Can you print hello world?")

print(interpreter.messages)

# This would output:

[
   {
      "role": "user",
      "message": "Hi! Can you print hello world?"
   },
   {
      "role": "assistant",
      "message": "Sure!"
   }
   {
      "role": "assistant",
      "language": "python",
      "code": "print('Hello, World!')",
      "output": "Hello, World!"
   }
]
```

You can use this to restore `interpreter` to a previous conversation.

```python
interpreter.messages = messages # A list that resembles the one above
```

---

#### `offline`

<Info>This replaced `interpreter.local` in the New Computer Update (`0.2.0`).</Info>

This boolean flag determines whether to enable or disable some offline features like [open procedures](https://open-procedures.replit.app/).

```python
interpreter.offline = True  # Check for updates, use procedures
interpreter.offline = False  # Don't check for updates, don't use procedures
```

Use this in conjunction with the `model` parameter to set your language model.

---

#### `auto_run`

Setting this flag to `True` allows Open Interpreter to automatically run the generated code without user confirmation.

```python
interpreter.auto_run = True  # Don't require user confirmation
interpreter.auto_run = False  # Require user confirmation (default)
```

---

#### `verbose`

Use this boolean flag to toggle verbose mode on or off. Verbose mode will print information at every step to help diagnose problems.

```python
interpreter.verbose = True  # Turns on verbose mode
interpreter.verbose = False  # Turns off verbose mode
```

---

#### `max_output`

This property sets the maximum number of tokens for the output response.

```python
interpreter.max_output = 2000
```

---

#### `conversation_history`

A boolean flag to indicate if the conversation history should be stored or not.

```python
interpreter.conversation_history = True  # To store history
interpreter.conversation_history = False  # To not store history
```

---

#### `conversation_filename`

This property sets the filename where the conversation history will be stored.

```python
interpreter.conversation_filename = "my_conversation.json"
```

---

#### `conversation_history_path`

You can set the path where the conversation history will be stored.

```python
import os
interpreter.conversation_history_path = os.path.join("my_folder", "conversations")
```

---

#### `model`

Specifies the language model to be used.

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

---

#### `temperature`

Sets the randomness level of the model's output.

```python
interpreter.llm.temperature = 0.7
```

---

#### `system_message`

This stores the model's system message as a string. Explore or modify it:

```python
interpreter.system_message += "\nRun all shell commands with -y."
```

---

#### `context_window`

This manually sets the context window size in tokens.

We try to guess the right context window size for you model, but you can override it with this parameter.

```python
interpreter.llm.context_window = 16000
```

---

#### `max_tokens`

Sets the maximum number of tokens the model can generate in a single response.

```python
interpreter.llm.max_tokens = 100
```

---

#### `api_base`

If you are using a custom API, you can specify its base URL here.

```python
interpreter.llm.api_base = "https://api.example.com"
```

---

#### `api_key`

Set your API key for authentication.

```python
interpreter.llm.api_key = "your_api_key_here"
```

---

#### `max_budget`

This property sets the maximum budget limit for the session in USD.

```python
interpreter.max_budget = 0.01 # 1 cent
```

================
File: usage/python/budget-manager.mdx
================
---
title: Budget Manager
---

The `max_budget` property sets the maximum budget limit for the session in USD.

```python
interpreter.max_budget = 0.01 # 1 cent
```

================
File: usage/python/conversation-history.mdx
================
---
title: Conversation History
---

Conversations will be saved in your application directory. **This is true for python and for the terminal interface.**

The command below, when run in your terminal, will show you which folder they're being saved in (use your arrow keys to move down and press enter over `> Open Folder`):

```shell
interpreter --conversations
```

You can turn off conversation history for a particular conversation:

```python
from interpreter import interpreter

interpreter.conversation_history = False
interpreter.chat() # Conversation history will not be saved
```

================
File: usage/python/magic-commands.mdx
================
---
title: Magic Commands
---

If you run an interactive chat in python, you can use *magic commands* built for terminal usage:

```python
interpreter.chat()
```

The following magic commands will work:

- %verbose [true/false]: Toggle verbose mode. Without arguments or with true it enters verbose mode. With false it exits verbose mode.
- %reset: Resets the current session's conversation.
- %undo: Removes the previous user message and the AI's response from the message history.
- %tokens [prompt]: (Experimental) Calculate the tokens that will be sent with the next prompt as context and estimate their cost. Optionally calculate the tokens and estimated cost of a prompt if one is provided. Relies on LiteLLM's cost_per_token() method for estimated costs.
- %help: Show the help message.

================
File: usage/python/multiple-instances.mdx
================
To create multiple instances, use the base class, `OpenInterpreter`:

```python
from interpreter import OpenInterpreter

agent_1 = OpenInterpreter()
agent_1.system_message = "This is a separate instance."

agent_2 = OpenInterpreter()
agent_2.system_message = "This is yet another instance."
```

For fun, you could make these instances talk to each other:

```python
def swap_roles(messages):
    for message in messages:
        if message['role'] == 'user':
            message['role'] = 'assistant'
        elif message['role'] == 'assistant':
            message['role'] = 'user'
    return messages

agents = [agent_1, agent_2]

# Kick off the conversation
messages = [{"role": "user", "type": "message", "content": "Hello!"}]

while True:
    for agent in agents:
        messages = agent.chat(messages)
        messages = swap_roles(messages)
```

================
File: usage/python/settings.mdx
================
---
title: Settings
---

Default settings will be inherited from a profile in your application directory. **This is true for python and for the terminal interface.**

To open the file, run:

```bash
interpreter --profiles
```

================
File: usage/terminal/arguments.mdx
================
---
title: Arguments
---

**[Modes](/docs/usage/terminal/arguments#modes)**

`--vision`, `--os`.

**[Model Settings](/docs/usage/terminal/arguments#model-settings)**

`--model`, `--fast`, `--local`, `--temperature`, `--context_window`, `--max_tokens`, `--max_output`, `--api_base`, `--api_key`, `--api_version`, `--llm_supports_functions`, `--llm_supports_vision`.

**[Configuration](/docs/usage/terminal/arguments#Configuration)**

`--profiles`, `--profile`, `--custom_instructions`, `--system_message`.

**[Options](/docs/usage/terminal/arguments#options)**

`--safe_mode`, `--auto_run`, `--loop`, `--verbose`, `--max_budget`, `--speak_messages`, `--multi_line`.

**[Other](/docs/usage/terminal/arguments#other)**

`--version`, `--help`.

---

## Modes

#### `--vision` or `-vi`

Enables vision mode for multimodal models. Defaults to GPT-4-turbo.

<CodeGroup>
```bash Terminal
interpreter --vision
```

```yaml Config
vision: true
```

</CodeGroup>

#### `--os` or `-o`

Enables OS mode for multimodal models. Defaults to GPT-4-turbo.

<CodeGroup>
    
    ```bash Terminal
    interpreter --os
    ```

    ```yaml Config
    os: true
    ```

</CodeGroup>

---

## Model Settings

#### `--model` or `-m`

Specifies which language model to use. Check out the [models](https://docs.openinterpreter.com/language-model-setup/introduction) section for a list of available models.

<CodeGroup>
    
```bash Terminal
interpreter --model "gpt-3.5-turbo"
```

```yaml Config
model: gpt-3.5-turbo
```

</CodeGroup>

#### `--fast` or `-f`

Sets the model to gpt-3.5-turbo.

<CodeGroup>
```bash Terminal
interpreter --fast
```

```yaml Config
fast: true
```

</CodeGroup>

#### `--local` or `-l`

Run the model locally. Check the [models page](/language-model-setup/introduction) for more information.

<CodeGroup>

```bash Terminal
interpreter --local
```

```yaml Config
local: true
```

</CodeGroup>

#### `--temperature` or `-t`

Sets the randomness level of the model's output.

<CodeGroup>
    
```bash Terminal
interpreter --temperature 0.7
```

```yaml Config
temperature: 0.7
```

</CodeGroup>

#### `--context_window` or `-c`

Manually set the context window size in tokens for the model.

<CodeGroup>

```bash Terminal
interpreter --context_window 16000
```

```yaml Config
context_window: 16000
```

</CodeGroup>

#### `--max_tokens` or `-x`

Sets the maximum number of tokens that the model can generate in a single response.

<CodeGroup>

```bash Terminal
interpreter --max_tokens 100
```

```yaml Config
max_tokens: 100
```

</CodeGroup>

#### `--max_output` or `-xo`

Set the maximum number of characters for code outputs.

<CodeGroup>
```bash Terminal
interpreter --max_output 1000
```

```yaml Config
max_output: 1000
```

</CodeGroup>
#### `--api_base` or `-ab`

If you are using a custom API, specify its base URL with this argument.

<CodeGroup>

```bash Terminal
interpreter --api_base "https://api.example.com"
```

```yaml Config
api_base: https://api.example.com
```

</CodeGroup>

#### `--api_key` or `-ak`

Set your API key for authentication when making API calls.

<CodeGroup>

```bash Terminal
interpreter --api_key "your_api_key_here"
```

```yaml Config
api_key: your_api_key_here
```

</CodeGroup>

#### `--api_version` or `-av`

Optionally set the API version to use with your selected model. (This will override environment variables)

<CodeGroup>
```bash Terminal
interpreter --api_version 2.0.2
```

```yaml Config
api_version: 2.0.2
```

</CodeGroup>
#### `--llm_supports_functions` or `-lsf`

Inform Open Interpreter that the language model you're using supports function calling.

<CodeGroup>
```bash Terminal
interpreter --llm_supports_functions
```

```yaml Config
llm_supports_functions: true
```

</CodeGroup>
#### `--no-llm_supports_functions`

Inform Open Interpreter that the language model you're using does not support function calling.

<CodeGroup>
  ```bash Terminal interpreter --no-llm_supports_functions ```
</CodeGroup>

#### `--llm_supports_vision` or `-lsv`

Inform Open Interpreter that the language model you're using supports vision.

<CodeGroup>
```bash Terminal
interpreter --llm_supports_vision
```

```yaml Config
llm_supports_vision: true
```

</CodeGroup>

---

## Configuration

#### `--profiles`

Opens the directory containing all profiles. They can be edited in your default editor.

<CodeGroup>
```bash Terminal
interpreter --profilees
```

</CodeGroup>

#### `--profile` or `-p`

Optionally set a profile to use.

<CodeGroup>
```bash Terminal
interpreter --profile "default.yaml"
```

</CodeGroup>

#### `--custom_instructions` or `-ci`

Appends custom instructions to the system message. This is useful for adding information about the your system, preferred languages, etc.

<CodeGroup>
```bash Terminal
interpreter --custom_instructions "This is a custom instruction."
```

```yaml Config
custom_instructions: "This is a custom instruction."
```

</CodeGroup>

#### `--system_message` or `-s`

We don't recommend modifying the system message, as doing so opts you out of future updates to the system message. Use `--custom_instructions` instead, to add relevant information to the system message. If you must modify the system message, you can do so by using this argument, or by opening the profile using `--profiles`.

<CodeGroup>
```bash Terminal
interpreter --system_message "You are Open Interpreter..."
```

```yaml Config
system_message: "You are Open Interpreter..."
```

## Options

#### `--safe_mode`

Enable or disable experimental safety mechanisms like code scanning. Valid options are `off`, `ask`, and `auto`.

<CodeGroup>

```bash Terminal
interpreter --safe_mode ask
```

```yaml Config
safe_mode: ask
```

</CodeGroup>

#### `--auto_run` or `-y`

Automatically run the interpreter without requiring user confirmation.

<CodeGroup>

```bash Terminal
interpreter --auto_run
```

```yaml Config
auto_run: true
```

</CodeGroup>

#### `--loop`

Runs Open Interpreter in a loop, requiring it to admit to completing or failing every task.

<CodeGroup>
```bash Terminal
interpreter --loop
```

```yaml Config
loop: true
```

</CodeGroup>

#### `--verbose` or `-v`

Run the interpreter in verbose mode. Debug information will be printed at each step to help diagnose issues.

<CodeGroup>

```bash Terminal
interpreter --verbose
```

```yaml Config
verbose: true
```

</CodeGroup>

#### `--max_budget` or `-b`

Sets the maximum budget limit for the session in USD.

<CodeGroup>

```bash Terminal
interpreter --max_budget 0.01
```

```yaml Config
max_budget: 0.01
```

</CodeGroup>

#### `--speak_messages` or `-sm`

(Mac Only) Speak messages out loud using the system's text-to-speech engine.

<CodeGroup>
```bash Terminal
interpreter --speak_messages
```

```yaml Config
speak_messages: true
```

</CodeGroup>

#### `--multi_line` or `-ml`

Enable multi-line inputs starting and ending with ` ``` `

<CodeGroup>
```bash Terminal
interpreter --multi_line
```

```yaml Config
multi_line: true
```

</CodeGroup>

---

## Other

#### `--version`

Get the current installed version number of Open Interpreter.

<CodeGroup>```bash Terminal interpreter --version ```</CodeGroup>

#### `--help` or `-h`

Display all available terminal arguments.

<CodeGroup>
```bash Terminal
interpreter --help
```

</CodeGroup>

================
File: usage/terminal/budget-manager.mdx
================
---
title: Budget Manager
---

You can set a maximum budget per session:
```bash
interpreter --max_budget 0.01
```

================
File: usage/terminal/magic-commands.mdx
================
---
title: Magic Commands
---

Magic commands can be used to control the interpreter's behavior in interactive mode:

- `%% [commands]`: Run commands in system shell.
- `%verbose [true/false]`: Toggle verbose mode. Without arguments or with 'true', it enters verbose mode. With 'false', it exits verbose mode.
- `%reset`: Resets the current session's conversation.
- `%undo`: Remove previous messages and its response from the message history.
- `%save_message [path]`: Saves messages to a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%load_message [path]`: Loads messages from a specified JSON path. If no path is provided, it defaults to 'messages.json'.
- `%tokens [prompt]`: EXPERIMENTAL: Calculate the tokens used by the next request based on the current conversation's messages and estimate the cost of that request; optionally provide a prompt to also calculate the tokens used by that prompt and the total amount of tokens that will be sent with the next request.
- `%info`: Show system and interpreter information.
- `%help`: Show this help message.
- `%markdown [path]`: Export the conversation to a specified Markdown path. If no path is provided, it will be saved to the Downloads folder with a generated conversation name.

================
File: usage/terminal/settings.mdx
================
---
title: Settings
---

Default settings can be edited via a profile. To open the file, run:

```bash
interpreter --profiles
```

| Key                      | Value                                                    |
| ------------------------ | -------------------------------------------------------- |
| `llm_model`              | String ["openai/gpt-4", "openai/local", "azure/gpt-3.5"] |
| `llm_temperature`        | Float [0.0 -> 1.0]                                       |
| `llm_supports_vision`    | Boolean [True/False]                                     |
| `llm_supports_functions` | Boolean [True/False]                                     |
| `llm_context_window`     | Integer [3000]                                           |
| `llm_max_tokens`         | Integer [3000]                                           |
| `llm_api_base`           | String ["http://ip_address:port", "https://openai.com"]  |
| `llm_api_key`            | String ["sk-Your-Key"]                                   |
| `llm_api_version`        | String ["version-number"]                                |
| `llm_max_budget`         | Float [0.01] #USD $0.01                                  |
| `offline`                | Boolean [True/False]                                     |
| `vision`                 | Boolean [True/False]                                     |
| `auto_run`               | Boolean [True/False]                                     |
| `verbose`                | Boolean [True/False]                                     |

================
File: usage/terminal/vision.mdx
================
---
title: Vision
---

To use vision (highly experimental), run the following command:

```bash
interpreter --vision
```

If a file path to an image is found in your input, it will be loaded into the vision model (`gpt-4o` for now).

================
File: usage/examples.mdx
================
---
title: Examples
description: Get started by copying these code snippets into your terminal, a `.py` file, or a Jupyter notebook.
---

<CardGroup>

<Card
  title="Interactive demo"
  icon="gamepad-modern"
  iconType="solid"
  href="https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing"
>
  Try Open Interpreter without installing anything on your computer
</Card>

<Card
  title="Example voice interface"
  icon="circle"
  iconType="solid"
  href="https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK"
>
  An example implementation of Open Interpreter's streaming capabilities
</Card>

</CardGroup>

---

### Interactive Chat

To start an interactive chat in your terminal, either run `interpreter` from the command line:

```shell
interpreter
```

Or `interpreter.chat()` from a .py file:

```python
interpreter.chat()
```

---

### Programmatic Chat

For more precise control, you can pass messages directly to `.chat(message)` in Python:

```python
interpreter.chat("Add subtitles to all videos in /videos.")

# ... Displays output in your terminal, completes task ...

interpreter.chat("These look great but can you make the subtitles bigger?")

# ...
```

---

### Start a New Chat

In your terminal, Open Interpreter behaves like ChatGPT and will not remember previous conversations. Simply run `interpreter` to start a new chat:

```shell
interpreter
```

In Python, Open Interpreter remembers conversation history. If you want to start fresh, you can reset it:

```python
interpreter.messages = []
```

---

### Save and Restore Chats

In your terminal, Open Interpreter will save previous conversations to `<your application directory>/Open Interpreter/conversations/`.

You can resume any of them by running `--conversations`. Use your arrow keys to select one , then press `ENTER` to resume it.

```shell
interpreter --conversations
```

In Python, `interpreter.chat()` returns a List of messages, which can be used to resume a conversation with `interpreter.messages = messages`:

```python
# Save messages to 'messages'
messages = interpreter.chat("My name is Killian.")

# Reset interpreter ("Killian" will be forgotten)
interpreter.messages = []

# Resume chat from 'messages' ("Killian" will be remembered)
interpreter.messages = messages
```

---

### Configure Default Settings

We save default settings to a profile which can be edited by running the following command:

```shell
interpreter --profiles
```

You can use this to set your default language model, system message (custom instructions), max budget, etc.

<Info>
  **Note:** The Python library will also inherit settings from the default
  profile file. You can change it by running `interpreter --profiles` and
  editing `default.yaml`.
</Info>

---

### Customize System Message

In your terminal, modify the system message by [editing your configuration file as described here](#configure-default-settings).

In Python, you can inspect and configure Open Interpreter's system message to extend its functionality, modify permissions, or give it more context.

```python
interpreter.system_message += """
Run shell commands with -y so the user doesn't have to confirm them.
"""
print(interpreter.system_message)
```

---

### Change your Language Model

Open Interpreter uses [LiteLLM](https://docs.litellm.ai/docs/providers/) to connect to language models.

You can change the model by setting the model parameter:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

In Python, set the model on the object:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Find the appropriate "model" string for your language model here.](https://docs.litellm.ai/docs/providers/)

================
File: CONTRIBUTING.md
================
# ●

**Open Interpreter is large, open-source initiative to build a standard interface between language models and computers.**

There are many ways to contribute, from helping others on [Github](https://github.com/OpenInterpreter/open-interpreter/issues) or [Discord](https://discord.gg/6p3fD6rBVm), writing documentation, or improving code.

We depend on contributors like you. Let's build this.

## What should I work on?

First, please familiarize yourself with our [project scope](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md#whats-in-our-scope). Then, pick up a task from our [roadmap](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md) or work on solving an [issue](https://github.com/OpenInterpreter/open-interpreter/issues).

If you encounter a bug or have a feature in mind, don't hesitate to [open a new issue](https://github.com/OpenInterpreter/open-interpreter/issues/new/choose).

## Philosophy

This is a minimalist, **tightly scoped** project that places a premium on simplicity. We're skeptical of new extensions, integrations, and extra features. We would rather not extend the system if it adds nonessential complexity.

# Contribution Guidelines

1. Before taking on significant code changes, please discuss your ideas on [Discord](https://discord.gg/6p3fD6rBVm) to ensure they align with our vision. We want to keep the codebase simple and unintimidating for new users.
2. Fork the repository and create a new branch for your work.
3. Follow the [Running Your Local Fork](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#running-your-local-fork) guide below.
4. Make changes with clear code comments explaining your approach. Try to follow existing conventions in the code.
5. Follow the [Code Formatting and Linting](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#code-formatting-and-linting) guide below.
6. Open a PR into `main` linking any related issues. Provide detailed context on your changes.

We will review PRs when possible and work with you to integrate your contribution. Please be patient as reviews take time. Once approved, your code will be merged.

## Running Your Local Fork

**Note: for anyone testing the new `--local`, `--os`, and `--local --os` modes: When you run `poetry install` you aren't installing the optional dependencies and it'll throw errors. To test `--local` mode, run `poetry install -E local`. To test `--os` mode, run `poetry install -E os`. To test `--local --os` mode, run `poetry install -E local -E os`. You can edit the system messages for these modes in `interpreter/terminal_interface/profiles/defaults`.**

Once you've forked the code and created a new branch for your work, you can run the fork in CLI mode by following these steps:

1. CD into the project folder by running `cd open-interpreter`.
2. Install `poetry` [according to their documentation](https://python-poetry.org/docs/#installing-with-pipx), which will create a virtual environment for development + handle dependencies.
3. Install dependencies by running `poetry install`.
4. Run the program with `poetry run interpreter`. Run tests with `poetry run pytest -s -x`.

**Note**: This project uses [`black`](https://black.readthedocs.io/en/stable/index.html) and [`isort`](https://pypi.org/project/isort/) via a [`pre-commit`](https://pre-commit.com/) hook to ensure consistent code style. If you need to bypass it for some reason, you can `git commit` with the `--no-verify` flag.

### Installing New Dependencies

If you wish to install new dependencies into the project, please use `poetry add package-name`.

### Installing Developer Dependencies

If you need to install dependencies specific to development, like testing tools, formatting tools, etc. please use `poetry add package-name --group dev`.

### Known Issues

For some, `poetry install` might hang on some dependencies. As a first step, try to run the following command in your terminal:

`export PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring`

Then run `poetry install` again. If this doesn't work, please join our [Discord community](https://discord.gg/6p3fD6rBVm) for help.

## Code Formatting and Linting

Our project uses `black` for code formatting and `isort` for import sorting. To ensure consistency across contributions, please adhere to the following guidelines:

1. **Install Pre-commit Hooks**:

   If you want to automatically format your code every time you make a commit, install the pre-commit hooks.

   ```bash
   pip install pre-commit
   pre-commit install
   ```

   After installing, the hooks will automatically check and format your code every time you commit.

2. **Manual Formatting**:

   If you choose not to use the pre-commit hooks, you can manually format your code using:

   ```bash
   black .
   isort .
   ```

# Licensing

Contributions to Open Interpreter would be under the MIT license before version 0.2.0, or under AGPL for subsequent contributions.

# Questions?

Join our [Discord community](https://discord.gg/6p3fD6rBVm) and post in the #General channel to connect with contributors. We're happy to guide you through your first open source contribution to this project!

**Thank you for your dedication and understanding as we continue refining our processes. As we explore this extraordinary new technology, we sincerely appreciate your involvement.**

================
File: mint.json
================
{
  "name": "Open Interpreter",
  "logo": {
    "dark": "/assets/logo/circle-inverted.png",
    "light": "/assets/logo/circle.png"
  },
  "favicon": "/assets/favicon.png",
  "colors": {
    "primary": "#000000",
    "light": "#FFFFFF",
    "dark": "#000000",
    "background": {
      "light": "#FFFFFF",
      "dark": "#000000"
    },
    "anchors": {
      "from": "#000000",
      "to": "#000000"
    }
  },
  "topbarLinks": [
    {
      "name": "50K ★ GitHub",
      "url": "https://github.com/OpenInterpreter/open-interpreter"
    }
  ],
  "topbarCtaButton": {
    "name": "Join Discord",
    "url": "https://discord.gg/Hvz9Axh84z"
  },
  "navigation": [
    {
      "group": "Getting Started",
      "pages": [
        "getting-started/introduction",
        "getting-started/setup"
      ]
    },
    {
      "group": "Guides",
      "pages": [
        "guides/basic-usage",
        "guides/running-locally",
        "guides/profiles",
        "guides/streaming-response",
        "guides/advanced-terminal-usage",
        "guides/multiple-instances",
        "guides/os-mode"
      ]
    },
    {
      "group": "Settings",
      "pages": [
        "settings/all-settings"
      ]
    },
    {
      "group": "Language Models",
      "pages": [
        "language-models/introduction",
        {
          "group": "Hosted Providers",
          "pages": [
            "language-models/hosted-models/openai",
            "language-models/hosted-models/azure",
            "language-models/hosted-models/vertex-ai",
            "language-models/hosted-models/replicate",
            "language-models/hosted-models/togetherai",
            "language-models/hosted-models/mistral-api",
            "language-models/hosted-models/anthropic",
            "language-models/hosted-models/anyscale",
            "language-models/hosted-models/aws-sagemaker",
            "language-models/hosted-models/baseten",
            "language-models/hosted-models/cloudflare",
            "language-models/hosted-models/cohere",
            "language-models/hosted-models/ai21",
            "language-models/hosted-models/deepinfra",
            "language-models/hosted-models/huggingface",
            "language-models/hosted-models/nlp-cloud",
            "language-models/hosted-models/openrouter",
            "language-models/hosted-models/palm",
            "language-models/hosted-models/perplexity",
            "language-models/hosted-models/petals",
            "language-models/hosted-models/vllm"
          ]
        },
        {
          "group": "Local Providers",
          "pages": [
            "language-models/local-models/ollama",
            "language-models/local-models/llamafile",
            "language-models/local-models/janai",
            "language-models/local-models/lm-studio",
            "language-models/local-models/custom-endpoint",
            "language-models/local-models/best-practices"
          ]
        },
        "language-models/custom-models",
        "language-models/settings"
      ]
    },
    {
      "group": "Code Execution",
      "pages": [
        "code-execution/usage",
        "code-execution/computer-api",
        "code-execution/custom-languages",
        "code-execution/settings"
      ]
    },
    {
      "group": "Protocols",
      "pages": [
        "protocols/lmc-messages"
      ]
    },
    {
      "group": "Integrations",
      "pages": [
        "integrations/e2b",
        "integrations/docker"
      ]
    },
    {
      "group": "Safety",
      "pages": [
        "safety/introduction",
        "safety/isolation",
        "safety/safe-mode",
        "safety/best-practices"
      ]
    },
    {
      "group": "Troubleshooting",
      "pages": [
        "troubleshooting/faq"
      ]
    },
    {
      "group": "Telemetry",
      "pages": [
        "telemetry/telemetry"
      ]
    }
  ],
  "feedback": {
    "suggestEdit": true
  },
  "footerSocials": {
    "twitter": "https://x.com/OpenInterpreter",
    "youtube": "https://www.youtube.com/@OpenInterpreter",
    "linkedin": "https://www.linkedin.com/company/openinterpreter"
  }
}

================
File: NCU_MIGRATION_GUIDE.md
================
# `0.2.0` Migration Guide

Open Interpreter is [changing](https://changes.openinterpreter.com/log/the-new-computer-update). This guide will help you migrate your application to `0.2.0`, also called the _New Computer Update_ (NCU), the latest major version of Open Interpreter.

## A New Start

To start using Open Interpreter in Python, we now use a standard **class instantiation** format:

```python
# From the module `interpreter`, import the class `OpenInterpreter`
from interpreter import OpenInterpreter

# Create an instance of `OpenInterpreter` to use it
agent = OpenInterpreter()
agent.chat()
```

For convenience, we also provide an instance of `interpreter`, which you can import from the module (also called `interpreter`):

```python
 # From the module `interpreter`, import the included instance of `OpenInterpreter`
from interpreter import interpreter

interpreter.chat()
```

## New Parameters

All stateless LLM attributes have been moved to `interpreter.llm`:

- `interpreter.model` → `interpreter.llm.model`
- `interpreter.api_key` → `interpreter.llm.api_key`
- `interpreter.llm_supports_vision` → `interpreter.llm.supports_vision`
- `interpreter.supports_function_calling` → `interpreter.llm.supports_functions`
- `interpreter.max_tokens` → `interpreter.llm.max_tokens`
- `interpreter.context_window` → `interpreter.llm.context_window`
- `interpreter.temperature` → `interpreter.llm.temperature`
- `interpreter.api_version` → `interpreter.llm.api_version`
- `interpreter.api_base` → `interpreter.llm.api_base`

This is reflected **1)** in Python applications using Open Interpreter and **2)** in your profile for OI's terminal interface, which can be edited via `interpreter --profiles`.

## New Static Messages Structure

- The array of messages is now flat, making the architecture more modular, and easier to adapt to new kinds of media in the future.
- Each message holds only one kind of data. This yields more messages, but prevents large nested messages that can be difficult to parse.
- This allows you to pass the full `messages` list into Open Interpreter as `interpreter.messages = message_list`.
- Every message has a "role", which can be "assistant", "computer", or "user".
- Every message has a "type", specifying the type of data it contains.
- Every message has "content", which contains the data for the message.
- Some messages have a "format" key, to specify the format of the content, like "path" or "base64.png".
- The recipient of the message is specified by the "recipient" key, which can be "user" or "assistant". This is used to inform the LLM of who the message is intended for.

```python
[
  {"role": "user", "type": "message", "content": "Please create a plot from this data and display it as an image and then as HTML."}, # implied format: text (only one format for type message)
  {"role": "user", "type": "image", "format": "path", "content": "path/to/image.png"}
  {"role": "user", "type": "file", "content": "/path/to/file.pdf"} # implied format: path (only one format for type file)
  {"role": "assistant", "type": "message", "content": "Processing your request to generate a plot."} # implied format: text
  {"role": "assistant", "type": "code", "format": "python", "content": "plot = create_plot_from_data('data')\ndisplay_as_image(plot)\ndisplay_as_html(plot)"}
  {"role": "computer", "type": "image", "format": "base64.png", "content": "base64"}
  {"role": "computer", "type": "code", "format": "html", "content": "<html>Plot in HTML format</html>"}
  {"role": "computer", "type": "console", "format": "output", "content": "{HTML errors}"}
  {"role": "assistant", "type": "message", "content": "Plot generated successfully."} # implied format: text
]
```

## New Streaming Structure

- The streaming data structure closely matches the static messages structure, with only a few differences.
- Every streaming chunk has a "start" and "end" key, which are booleans that specify whether the chunk is the first or last chunk in the stream. This is what you should use to build messages from the streaming chunks.
- There is a "confirmation" chunk type, which is used to confirm with the user that the code should be run. The "content" key of this chunk is a dictionary with a `code` and a `language` key.
- Introducing more information per chunk is helpful in processing the streaming responses. Please take a look below for example code for processing streaming responses, in JavaScript.

```python
{"role": "assistant", "type": "message", "start": True}
{"role": "assistant", "type": "message", "content": "Pro"}
{"role": "assistant", "type": "message", "content": "cessing"}
{"role": "assistant", "type": "message", "content": "your request"}
{"role": "assistant", "type": "message", "content": "to generate a plot."}
{"role": "assistant", "type": "message", "end": True}

{"role": "assistant", "type": "code", "format": "python", "start": True}
{"role": "assistant", "type": "code", "format": "python", "content": "plot = create_plot_from_data"}
{"role": "assistant", "type": "code", "format": "python", "content": "('data')\ndisplay_as_image(plot)"}
{"role": "assistant", "type": "code", "format": "python", "content": "\ndisplay_as_html(plot)"}
{"role": "assistant", "type": "code", "format": "python", "end": True}

# The computer will emit a confirmation chunk *before* running the code. You can break here to cancel the execution.

{"role": "computer", "type": "confirmation", "format": "execution", "content": {
    "type": "code",
    "format": "python",
    "content": "plot = create_plot_from_data('data')\ndisplay_as_image(plot)\ndisplay_as_html(plot)",
}}

{"role": "computer", "type": "console", "start": True}
{"role": "computer", "type": "console", "format": "output", "content": "a printed statement"}
{"role": "computer", "type": "console", "format": "active_line", "content": "1"}
{"role": "computer", "type": "console", "format": "active_line", "content": "2"}
{"role": "computer", "type": "console", "format": "active_line", "content": "3"}
{"role": "computer", "type": "console", "format": "output", "content": "another printed statement"}
{"role": "computer", "type": "console", "end": True}
```

## Tips and Best Practices

- Adding an `id` and a `created_at` field to messages can be helpful to manipulate the messages later on.
- If you want your application to run the code instead of OI, then your app will act as the `computer`. This means breaking from the stream once OI emits a confirmation chunk (`{'role': 'computer', 'type': 'confirmation' ...}`) to prevent OI from running the code. When you run code, grab the message history via `messages = interpreter.messages`, then simply mimic the `computer` format above by appending new `{'role': 'computer' ...}` messages, then run `interpreter.chat(messages)`.
- Open Interpreter is designed to stop code execution when the stream is disconnected. Use this to your advantage to add a "Stop" button to the UI.
- Setting up your Python server to send errors and exceptions to the client can be helpful for debugging and generating error messages.

## Example Code

### Types

Python:

```python
class Message:
    role: Union["user", "assistant", "computer"]
    type: Union["message", "code", "image", "console", "file", "confirmation"]
    format: Union["output", "path", "base64.png", "base64.jpeg", "python", "javascript", "shell", "html", "active_line", "execution"]
    recipient: Union["user", "assistant"]
    content: Union[str, dict]  # dict should have 'code' and 'language' keys, this is only for confirmation messages

class StreamingChunk(Message):
    start: bool
    end: bool
```

TypeScript:

```typescript
interface Message {
  role: "user" | "assistant" | "computer";
  type: "message" | "code" | "image" | "console" | "file", | "confirmation";
  format: "output" | "path" | "base64.png" | "base64.jpeg" | "python" | "javascript" | "shell" | "html" | "active_line", | "execution";
  recipient: "user" | "assistant";
  content: string | { code: string; language: string };
}
```

```typescript
interface StreamingChunk extends Message {
  start: boolean;
  end: boolean;
}
```

### Handling streaming chunks

Here is a minimal example of how to handle streaming chunks in JavaScript. This example assumes that you are using a Python server to handle the streaming requests, and that you are using a JavaScript client to send the requests and handle the responses. See the main repository README for an example FastAPI server.

```javascript
//Javascript

let messages = []; //variable to hold all messages
let currentMessageIndex = 0; //variable to keep track of the current message index
let isGenerating = false; //variable to stop the stream

// Function to send a POST request to the OI
async function sendRequest() {
  // Temporary message to hold the message that is being processed
  try {
    // Define parameters for the POST request, add at least the full messages array, but you may also consider adding any other OI parameters here, like auto_run, local, etc.
    const params = {
      messages,
    };

    //Define a controller to allow for aborting the request
    const controller = new AbortController();
    const { signal } = controller;

    // Send the POST request to your Python server endpoint
    const interpreterCall = await fetch("https://YOUR_ENDPOINT/", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(params),
      signal,
    });

    // Throw an error if the request was not successful
    if (!interpreterCall.ok) {
      console.error("Interpreter didn't respond with 200 OK");
      return;
    }

    // Initialize a reader for the response body
    const reader = interpreterCall.body.getReader();

    isGenerating = true;
    while (true) {
      const { value, done } = await reader.read();

      // Break the loop if the stream is done
      if (done) {
        break;
      }
      // If isGenerating is set to false, cancel the reader and break the loop. This will halt the execution of the code run by OI as well
      if (!isGenerating) {
        await reader.cancel();
        controller.abort();
        break;
      }
      // Decode the stream and split it into lines
      const text = new TextDecoder().decode(value);
      const lines = text.split("\n");
      lines.pop(); // Remove last empty line

      // Process each line of the response
      for (const line of lines) {
        const chunk = JSON.parse(line);
        await processChunk(chunk);
      }
    }
    //Stream has completed here, so run any code that needs to be run after the stream has finished
    if (isGenerating) isGenerating = false;
  } catch (error) {
    console.error("An error occurred:", error);
  }
}

//Function to process each chunk of the stream, and create messages
function processChunk(chunk) {
  if (chunk.start) {
    const tempMessage = {};
    //add the new message's data to the tempMessage
    tempMessage.role = chunk.role;
    tempMessage.type = chunk.type;
    tempMessage.content = "";
    if (chunk.format) tempMessage.format = chunk.format;
    if (chunk.recipient) tempMessage.recipient = chunk.recipient;

    //add the new message to the messages array, and set the currentMessageIndex to the index of the new message
    messages.push(tempMessage);
    currentMessageIndex = messages.length - 1;
  }

  //Handle active lines for code blocks
  if (chunk.format === "active_line") {
    messages[currentMessageIndex].activeLine = chunk.content;
  } else if (chunk.end && chunk.type === "console") {
    messages[currentMessageIndex].activeLine = null;
  }

  //Add the content of the chunk to current message, avoiding adding the content of the active line
  if (chunk.content && chunk.format !== "active_line") {
    messages[currentMessageIndex].content += chunk.content;
  }
}
```

================
File: README_DE.md
================
<h1 align="center">● Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white">
    </a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Español-white.svg" alt="ES doc"/></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/ドキュメント-日本語-white.svg" alt="JA doc"></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/文档-中文版-white.svg" alt="ZH doc"></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/README_UK.md"><img src="https://img.shields.io/badge/Українська-white.svg" alt="UK doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br><br>
    <b>Lassen Sie Sprachmodelle Code auf Ihrem Computer ausführen.</b><br>
    Eine Open-Source, lokal laufende Implementierung von OpenAIs Code-Interpreter.<br>
    <br><a href="https://openinterpreter.com">Erhalten Sie frühen Zugang zur Desktop-Anwendung.</a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter** ermöglicht es LLMs (Language Models), Code (Python, Javascript, Shell und mehr) lokal auszuführen. Sie können mit Open Interpreter über eine ChatGPT-ähnliche Schnittstelle in Ihrem Terminal chatten, indem Sie $ interpreter nach der Installation ausführen.

Dies bietet eine natürliche Sprachschnittstelle zu den allgemeinen Fähigkeiten Ihres Computers:

- Erstellen und bearbeiten Sie Fotos, Videos, PDFs usw.
- Steuern Sie einen Chrome-Browser, um Forschungen durchzuführen
- Darstellen, bereinigen und analysieren Sie große Datensätze
- ...usw.

**⚠️ Hinweis: Sie werden aufgefordert, Code zu genehmigen, bevor er ausgeführt wird.**

<br>

## Demo

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Eine interaktive Demo ist auch auf Google Colab verfügbar:

[![In Colab öffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

## Schnellstart

```shell
pip install open-interpreter
```

### Terminal

Nach der Installation führen Sie einfach `interpreter` aus:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Stellen Sie AAPL und METAs normalisierte Aktienpreise dar") # Führt einen einzelnen Befehl aus
interpreter.chat() # Startet einen interaktiven Chat
```

## Vergleich zu ChatGPTs Code Interpreter

OpenAIs Veröffentlichung des [Code Interpreters](https://openai.com/blog/chatgpt-plugins#code-interpreter) mit GPT-4 bietet eine fantastische Möglichkeit, reale Aufgaben mit ChatGPT zu erledigen.

Allerdings ist OpenAIs Dienst gehostet, Closed-Source und stark eingeschränkt:

- Kein Internetzugang.
- [Begrenzte Anzahl vorinstallierter Pakete](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
- 100 MB maximale Uploadgröße, 120.0 Sekunden Laufzeitlimit.
- Der Zustand wird gelöscht (zusammen mit allen generierten Dateien oder Links), wenn die Umgebung abstirbt.

---

Open Interpreter überwindet diese Einschränkungen, indem es in Ihrer lokalen Umgebung läuft. Es hat vollen Zugang zum Internet, ist nicht durch Zeit oder Dateigröße eingeschränkt und kann jedes Paket oder jede Bibliothek nutzen.

Dies kombiniert die Kraft von GPT-4s Code Interpreter mit der Flexibilität Ihrer lokalen Maschine.

## Sicherheitshinweis

Da generierter Code in deiner lokalen Umgebung ausgeführt wird, kann er mit deinen Dateien und Systemeinstellungen interagieren, was potenziell zu unerwarteten Ergebnissen wie Datenverlust oder Sicherheitsrisiken führen kann.

**⚠️ Open Interpreter wird um Nutzerbestätigung bitten, bevor Code ausgeführt wird.**

Du kannst `interpreter -y` ausführen oder `interpreter.auto_run = True` setzen, um diese Bestätigung zu umgehen, in diesem Fall:

- Sei vorsichtig bei Befehlsanfragen, die Dateien oder Systemeinstellungen ändern.
- Beobachte Open Interpreter wie ein selbstfahrendes Auto und sei bereit, den Prozess zu beenden, indem du dein Terminal schließt.
- Betrachte die Ausführung von Open Interpreter in einer eingeschränkten Umgebung wie Google Colab oder Replit. Diese Umgebungen sind isolierter und reduzieren das Risiko der Ausführung willkürlichen Codes.

Es gibt **experimentelle** Unterstützung für einen [Sicherheitsmodus](docs/SAFE_MODE.md), um einige Risiken zu mindern.

## Wie funktioniert es?

Open Interpreter rüstet ein [funktionsaufrufendes Sprachmodell](https://platform.openai.com/docs/guides/gpt/function-calling) mit einer `exec()`-Funktion aus, die eine `language` (wie "Python" oder "JavaScript") und auszuführenden `code` akzeptiert.

Wir streamen dann die Nachrichten des Modells, Code und die Ausgaben deines Systems zum Terminal als Markdown.

# Mitwirken

Danke für dein Interesse an der Mitarbeit! Wir begrüßen die Beteiligung der Gemeinschaft.

Bitte sieh dir unsere [Richtlinien für Mitwirkende](docs/CONTRIBUTING.md) für weitere Details an, wie du dich einbringen kannst.

## Lizenz

Open Interpreter ist unter der MIT-Lizenz lizenziert. Du darfst die Software verwenden, kopieren, modifizieren, verteilen, unterlizenzieren und Kopien der Software verkaufen.

**Hinweis**: Diese Software ist nicht mit OpenAI affiliiert.

> Zugriff auf einen Junior-Programmierer zu haben, der mit der Geschwindigkeit deiner Fingerspitzen arbeitet ... kann neue Arbeitsabläufe mühelos und effizient machen sowie das Programmieren einem neuen Publikum öffnen.
>
> — _OpenAIs Code Interpreter Release_

<br>

================
File: README_ES.md
================
<h1 align="center">● Intérprete Abierto</h1>

<p align="center">
    <a href="https://discord.gg/Hvz9Axh84z">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/ドキュメント-日本語-white.svg" alt="JA doc"/></a>
    <a href="README_ZH.md"> <img src="https://img.shields.io/badge/文档-中文版-white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/Українська-white.svg" alt="UK doc"/></a>
    <a href="README_IN.md"> <img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <br><a href="https://0ggfznkwh4j.typeform.com/to/G21i9lJ2">Obtenga acceso temprano a la aplicación de escritorio</a>‎ ‎ |‎ ‎ <a href="https://docs.openinterpreter.com/">Documentación</a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>
<p align="center">
<strong>La Nueva Actualización del Computador</strong> presenta <strong><code>--os</code></strong> y una nueva <strong>API de Computadora</strong>. <a href="https://changes.openinterpreter.com/log/the-new-computer-update">Lea más →</a>
</p>
<br>

```shell
pip install open-interpreter
```

> ¿No funciona? Lea nuestra [guía de configuración](https://docs.openinterpreter.com/getting-started/setup).

```shell
interpreter
```

<br>

**Intérprete Abierto** permite a los LLMs ejecutar código (Python, JavaScript, Shell, etc.) localmente. Puede chatear con Intérprete Abierto a través de una interfaz de chat como ChatGPT en su terminal después de instalar.

Esto proporciona una interfaz de lenguaje natural para las capacidades generales de su computadora:

- Crear y editar fotos, videos, PDF, etc.
- Controlar un navegador de Chrome para realizar investigaciones
- Graficar, limpiar y analizar conjuntos de datos grandes
- ... etc.

**⚠️ Nota: Se le pedirá que apruebe el código antes de ejecutarlo.**

<br>

## Demo

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### También hay disponible una demo interactiva en Google Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

#### Además, hay un ejemplo de interfaz de voz inspirada en _Her_:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

## Inicio Rápido

```shell
pip install open-interpreter
```

### Terminal

Después de la instalación, simplemente ejecute `interpreter`:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Plot AAPL and META's normalized stock prices") # Ejecuta un comando sencillo
interpreter.chat() # Inicia una sesión de chat interactiva
```

### GitHub Codespaces

Presione la tecla `,` en la página de GitHub de este repositorio para crear un espacio de códigos. Después de un momento, recibirá un entorno de máquina virtual en la nube con Interprete Abierto pre-instalado. Puede entonces empezar a interactuar con él directamente y confirmar su ejecución de comandos del sistema sin preocuparse por dañar el sistema.

## Comparación con el Intérprete de Código de ChatGPT

El lanzamiento de [Intérprete de Código](https://openai.com/blog/chatgpt-plugins#code-interpreter) de OpenAI con GPT-4 presenta una oportunidad fantástica para realizar tareas del mundo real con ChatGPT.

Sin embargo, el servicio de OpenAI está alojado, su codigo es cerrado y está fuertemente restringido:

- No hay acceso a Internet.
- [Conjunto limitado de paquetes preinstalados](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
- Límite de 100 MB de carga, límite de tiempo de 120.0 segundos.
- El estado se elimina (junto con cualquier archivo generado o enlace) cuando el entorno se cierra.

---

Intérprete Abierto supera estas limitaciones al ejecutarse en su entorno local. Tiene acceso completo a Internet, no está restringido por tiempo o tamaño de archivo y puede utilizar cualquier paquete o libreria.

Esto combina el poder del Intérprete de Código de GPT-4 con la flexibilidad de su entorno de desarrollo local.

## Comandos

**Actualización:** La Actualización del Generador (0.1.5) introdujo streaming:

```python
message = "¿Qué sistema operativo estamos utilizando?"

for chunk in interpreter.chat(message, display=False, stream=True):
    print(chunk)
```

### Chat Interactivo

Para iniciar una sesión de chat interactiva en su terminal, puede ejecutar `interpreter` desde la línea de comandos:

```shell
interpreter
```

O `interpreter.chat()` desde un archivo `.py`:

```python
interpreter.chat()
```

**Puede también transmitir cada trozo:**

```python
message = "¿Qué sistema operativo estamos utilizando?"

for chunk in interpreter.chat(message, display=False, stream=True):
    print(chunk)
```

### Chat Programático

Para un control más preciso, puede pasar mensajes directamente a `.chat(message)`:

```python
interpreter.chat("Añade subtítulos a todos los videos en /videos.")

# ... Transmite salida a su terminal, completa tarea ...

interpreter.chat("Estos se ven bien, pero ¿pueden hacer los subtítulos más grandes?")

# ...
```

### Iniciar un nuevo chat

En Python, Intérprete Abierto recuerda el historial de conversación. Si desea empezar de nuevo, puede resetearlo:

```python
interpreter.messages = []
```

### Guardar y Restaurar Chats

`interpreter.chat()` devuelve una lista de mensajes, que puede utilizar para reanudar una conversación con `interpreter.messages = messages`:

```python
messages = interpreter.chat("Mi nombre es Killian.") # Guarda mensajes en 'messages'
interpreter.messages = [] # Resetear Intérprete ("Killian" será olvidado)

interpreter.messages = messages # Reanuda chat desde 'messages' ("Killian" será recordado)
```

### Personalizar el Mensaje del Sistema

Puede inspeccionar y configurar el mensaje del sistema de Intérprete Abierto para extender su funcionalidad, modificar permisos o darle más contexto.

```python
interpreter.system_message += """
Ejecute comandos de shell con -y para que el usuario no tenga que confirmarlos.
"""
print(interpreter.system_message)
```

### Cambiar el Modelo de Lenguaje

Intérprete Abierto utiliza [LiteLLM](https://docs.litellm.ai/docs/providers/) para conectarse a modelos de lenguaje hospedados.

Puede cambiar el modelo estableciendo el parámetro de modelo:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

En Python, establezca el modelo en el objeto:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Encuentre la cadena adecuada para su modelo de lenguaje aquí.](https://docs.litellm.ai/docs/providers/)

### Ejecutar Intérprete Abierto localmente

#### Terminal

Intérprete Abierto puede utilizar un servidor de OpenAI compatible para ejecutar modelos localmente. (LM Studio, jan.ai, ollama, etc.)

Simplemente ejecute `interpreter` con la URL de base de API de su servidor de inferencia (por defecto, `http://localhost:1234/v1` para LM Studio):

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

O puede utilizar Llamafile sin instalar software adicional simplemente ejecutando:

```shell
interpreter --local
```

Para una guía mas detallada, consulte [este video de Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

**Cómo ejecutar LM Studio en segundo plano.**

1. Descargue [https://lmstudio.ai/](https://lmstudio.ai/) luego ejecutelo.
2. Seleccione un modelo, luego haga clic **↓ Descargar**.
3. Haga clic en el botón **↔️** en la izquierda (debajo de 💬).
4. Seleccione su modelo en la parte superior, luego haga clic **Iniciar Servidor**.

Una vez que el servidor esté funcionando, puede empezar su conversación con Intérprete Abierto.

> **Nota:** El modo local establece su `context_window` en 3000 y su `max_tokens` en 1000. Si su modelo tiene requisitos diferentes, ajuste estos parámetros manualmente (ver a continuación).

#### Python

Nuestro paquete de Python le da más control sobre cada ajuste. Para replicar y conectarse a LM Studio, utilice estos ajustes:

```python
from interpreter import interpreter

interpreter.offline = True # Desactiva las características en línea como Procedimientos Abiertos
interpreter.llm.model = "openai/x" # Indica a OI que envíe mensajes en el formato de OpenAI
interpreter.llm.api_key = "fake_key" # LiteLLM, que utilizamos para hablar con LM Studio, requiere esto
interpreter.llm.api_base = "http://localhost:1234/v1" # Apunta esto a cualquier servidor compatible con OpenAI

interpreter.chat()
```

#### Ventana de Contexto, Tokens Máximos

Puede modificar los `max_tokens` y `context_window` (en tokens) de los modelos locales.

Para el modo local, ventanas de contexto más cortas utilizarán menos RAM, así que recomendamos intentar una ventana mucho más corta (~1000) si falla o si es lenta. Asegúrese de que `max_tokens` sea menor que `context_window`.

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

### Modo Detallado

Para ayudarle a inspeccionar Intérprete Abierto, tenemos un modo `--verbose` para depuración.

Puede activar el modo detallado utilizando el parámetro (`interpreter --verbose`), o en plena sesión:

```shell
$ interpreter
...
> %verbose true <- Activa el modo detallado

> %verbose false <- Desactiva el modo verbose
```

### Comandos de Modo Interactivo

En el modo interactivo, puede utilizar los siguientes comandos para mejorar su experiencia. Aquí hay una lista de comandos disponibles:

**Comandos Disponibles:**

- `%verbose [true/false]`: Activa o desactiva el modo detallado. Sin parámetros o con `true` entra en modo detallado.
  Con `false` sale del modo verbose.
- `%reset`: Reinicia la sesión actual de conversación.
- `%undo`: Elimina el mensaje de usuario previo y la respuesta del AI del historial de mensajes.
- `%tokens [prompt]`: (_Experimental_) Calcula los tokens que se enviarán con el próximo prompt como contexto y estima su costo. Opcionalmente, calcule los tokens y el costo estimado de un `prompt` si se proporciona. Depende de [LiteLLM's `cost_per_token()` method](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token) para costos estimados.
- `%help`: Muestra el mensaje de ayuda.

### Configuración / Perfiles

Intérprete Abierto permite establecer comportamientos predeterminados utilizando archivos `yaml`.

Esto proporciona una forma flexible de configurar el intérprete sin cambiar los argumentos de línea de comandos cada vez.

Ejecutar el siguiente comando para abrir el directorio de perfiles:

```
interpreter --profiles
```

Puede agregar archivos `yaml` allí. El perfil predeterminado se llama `default.yaml`.

#### Perfiles Múltiples

Intérprete Abierto admite múltiples archivos `yaml`, lo que permite cambiar fácilmente entre configuraciones:

```
interpreter --profile my_profile.yaml
```

## Servidor de FastAPI de ejemplo

El generador actualiza permite controlar Intérprete Abierto a través de puntos de conexión HTTP REST:

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

Puede iniciar un servidor idéntico al anterior simplemente ejecutando `interpreter.server()`.

## Android

La guía paso a paso para instalar Intérprete Abierto en su dispositivo Android se encuentra en el [repo de open-interpreter-termux](https://github.com/MikeBirdTech/open-interpreter-termux).

## Aviso de Seguridad

Ya que el código generado se ejecuta en su entorno local, puede interactuar con sus archivos y configuraciones del sistema, lo que puede llevar a resultados inesperados como pérdida de datos o riesgos de seguridad.

**⚠️ Intérprete Abierto le pedirá que apruebe el código antes de ejecutarlo.**

Puede ejecutar `interpreter -y` o establecer `interpreter.auto_run = True` para evitar esta confirmación, en cuyo caso:

- Sea cuidadoso al solicitar comandos que modifican archivos o configuraciones del sistema.
- Vigile Intérprete Abierto como si fuera un coche autónomo y esté preparado para terminar el proceso cerrando su terminal.
- Considere ejecutar Intérprete Abierto en un entorno restringido como Google Colab o Replit. Estos entornos son más aislados, reduciendo los riesgos de ejecutar código arbitrario.

Hay soporte **experimental** para un [modo seguro](docs/SAFE_MODE.md) para ayudar a mitigar algunos riesgos.

## ¿Cómo Funciona?

Intérprete Abierto equipa un [modelo de lenguaje de llamada a funciones](https://platform.openai.com/docs/guides/gpt/function-calling) con una función `exec()`, que acepta un `lenguaje` (como "Python" o "JavaScript") y `código` para ejecutar.

Luego, transmite los mensajes del modelo, el código y las salidas del sistema a la terminal como Markdown.

# Acceso a la Documentación Offline

La documentación completa está disponible en línea sin necesidad de conexión a Internet.

[Node](https://nodejs.org/en) es un requisito previo:

- Versión 18.17.0 o cualquier versión posterior 18.x.x.
- Versión 20.3.0 o cualquier versión posterior 20.x.x.
- Cualquier versión a partir de 21.0.0 sin límite superior especificado.

Instale [Mintlify](https://mintlify.com/):

```bash
npm i -g mintlify@latest
```

Cambia a la carpeta de documentos y ejecuta el comando apropiado:

```bash
# Suponiendo que estás en la carpeta raíz del proyecto
cd ./docs

# Ejecute el servidor de documentación
mintlify dev
```

Una nueva ventana del navegador debería abrirse. La documentación estará disponible en [http://localhost:3000](http://localhost:3000) mientras el servidor de documentación esté funcionando.

# Contribuyendo

¡Gracias por su interés en contribuir! Damos la bienvenida a la implicación de la comunidad.

Por favor, consulte nuestras [directrices de contribución](docs/CONTRIBUTING.md) para obtener más detalles sobre cómo involucrarse.

# Roadmap

Visite [nuestro roadmap](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md) para ver el futuro de Intérprete Abierto.

**Nota:** Este software no está afiliado con OpenAI.

![thumbnail-ncu](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/1b19a5db-b486-41fd-a7a1-fe2028031686)

> Tener acceso a un programador junior trabajando a la velocidad de su dedos... puede hacer que los nuevos flujos de trabajo sean sencillos y eficientes, además de abrir los beneficios de la programación a nuevas audiencias.
>
> — _Lanzamiento del intérprete de código de OpenAI_

<br>

================
File: README_IN.md
================
<h1 align="center">● Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/>
    </a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Español-white.svg" alt="ES doc"/></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/ドキュメント-日本語-white.svg" alt="JA doc"/></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/文档-中文版-white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/Українська-white.svg" alt="UK doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br><br>
    <b>अपने कंप्यूटर पर कोड चलाने के लिए भाषा मॉडल को चलाएं।</b><br>
    ओपनएआई कोड इंटरप्रेटर का एक ओपन-सोर्स, स्थानीय चलने वाला अमल।<br>
    <br><a href="https://openinterpreter.com">डेस्कटॉप एप्लिकेशन को पहले से ही उपयोग करने के लिए एरली एक्सेस प्राप्त करें।</a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**ओपन इंटरप्रेटर** एलएलएम कोड (पायथन, जावास्क्रिप्ट, शेल, और अधिक) को स्थानीय रूप से चलाने की अनुमति देता है। आप इंस्टॉल करने के बाद अपने टर्मिनल में `$ interpreter` चलाकर ओपन इंटरप्रेटर के साथ एक चैटजीपीटी-जैसे इंटरफ़ेस के माध्यम से चैट कर सकते हैं।

यह आपके कंप्यूटर की सामान्य-उद्देश्य क्षमताओं के लिए एक प्राकृतिक भाषा इंटरफ़ेस प्रदान करता है:

- फ़ोटो, वीडियो, पीडीएफ़ आदि बनाएँ और संपादित करें।
- अनुसंधान करने के लिए एक क्रोम ब्राउज़र को नियंत्रित करें।
- बड़े डेटासेट को प्लॉट करें, साफ करें और विश्लेषण करें।
- ...आदि।

**⚠️ ध्यान दें: कोड को चलाने से पहले आपसे मंज़ूरी मांगी जाएगी।**

<br>

## डेमो

[![कोलैब में खोलें](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

## त्वरित प्रारंभ

```shell
pip install open-interpreter
```

### टर्मिनल

इंस्टॉलेशन के बाद, सीधे `interpreter` चलाएं:

```shell
interpreter
```

### पायथन

```python
from interpreter import interpreter

interpreter.chat("AAPL और META के मानकीकृत स्टॉक मूल्यों का चित्रित करें") # एकल कमांड को निष्पादित करता है
interpreter.chat() # एक इंटरैक्टिव चैट शुरू करता है
```

## ChatGPT के कोड इंटरप्रेटर के साथ तुलना

ओपनएआई द्वारा [कोड इंटरप्रेटर](https://openai.com/blog/chatgpt-plugins#code-interpreter) का विमोचन। GPT-4 के साथ यह एक शानदार अवसर प्रस्तुत करता है जिससे ChatGPT के साथ वास्तविक दुनिया के कार्यों को पूरा करने का संभावना होती है।

हालांकि, ओपनएआई की सेवा होस्ट की जाती है, क्लोज़-स्रोत है और गहरी प्रतिबंधित है।

यहां दिए गए नियमों के अनुसार, चैटजीपीटी कोड इंटरप्रेटर के लिए निर्धारित नियमों को हिंदी में अनुवाद किया जा सकता है:

- कोई इंटरनेट पहुंच नहीं होती।
- [प्रतिष्ठित सेट की सीमित संख्या के पहले स्थापित पैकेज](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/) होते हैं।
- 100 एमबी तक की अधिकतम अपलोड सीमा होती है।
- 120.0 सेकंड की रनटाइम सीमा होती है।
- जब एनवायरनमेंट समाप्त होता है, तो स्थिति साफ हो जाती है (साथ ही उत्पन्न किए गए फ़ाइल या लिंक भी)।

---

ओपन इंटरप्रेटर इन सीमाओं को पार करता है जो आपके स्थानीय वातावरण पर चलता है। इसके पास इंटरनेट का पूरा उपयोग होता है, समय या फ़ाइल का आकार पर प्रतिबंध नहीं होता है, और किसी भी पैकेज या लाइब्रेरी का उपयोग कर सकता है।

यह GPT-4 के कोड इंटरप्रेटर की शक्ति को आपके स्थानीय विकास वातावरण की लचीलापन के साथ मिलाता है।

## Commands

### Interactive Chat

To start an interactive chat in your terminal, either run `interpreter` from the command line:

```shell
interpreter
```

Or `interpreter.chat()` from a .py file:

```python
interpreter.chat()
```

## कमांड

### इंटरैक्टिव चैट

अपने टर्मिनल में इंटरैक्टिव चैट शुरू करने के लिए, या तो कमांड लाइन से `interpreter` चलाएँ:

```shell
interpreter
```

या एक .py फ़ाइल से `interpreter.chat()` चलाएँ:

````python
interpreter.chat()

### प्रोग्रामेटिक चैट

और सटीक नियंत्रण के लिए, आप सीधे `.chat(message)` को संदेश पास कर सकते हैं:

```python
interpreter.chat("सभी वीडियो में उपशीर्षक जोड़ें /videos में।")

# ... आपके टर्मिनल में आउटपुट स्ट्रीम करता है, कार्य पूरा करता है ...

interpreter.chat("ये बड़े दिख रहे हैं लेकिन क्या आप उपशीर्षक को और बड़ा कर सकते हैं?")

# ...
````

### नया चैट शुरू करें

Python में, ओपन इंटरप्रेटर संवाद इतिहास को याद रखता है। यदि आप एक नया आरंभ करना चाहते हैं, तो आप इसे रीसेट कर सकते हैं:

```python
interpreter.messages = []
```

### चैट सहेजें और पुनर्स्थापित करें

```python
messages = interpreter.chat("मेरा नाम किलियन है।") # संदेशों को 'messages' में सहेजें

interpreter.messages = messages # 'messages' से चैट को फिर से शुरू करें ("किलियन" याद रखा जाएगा)
```

### सिस्टम संदेश कस्टमाइज़ करें

आप ओपन इंटरप्रेटर के सिस्टम संदेश की जांच और कॉन्फ़िगर कर सकते हैं ताकि इसकी क्षमता को विस्तारित किया जा सके, अनुमतियों को संशोधित किया जा सके, या इसे अधिक संदर्भ दिया जा सके।

```python
interpreter.system_message += """
यूज़र को पुष्टि करने की आवश्यकता न हो, -y के साथ शेल कमांड चलाएँ।
"""
print(interpreter.system_message)
```

### मॉडल बदलें

`gpt-3.5-turbo` के लिए तेज़ मोड का उपयोग करें:

```shell
interpreter --fast
```

Python में, आपको मॉडल को मैन्युअली सेट करने की आवश्यकता होगी:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

### ओपन इंटरप्रेटर को स्थानीय रूप से चलाना

```shell
interpreter --local
```

#### स्थानीय मॉडल पैरामीटर

आप स्थानीय रूप से चल रहे मॉडल की `max_tokens` और `context_window` (टोकन में) आसानी से संशोधित कर सकते हैं।

छोटे संदर्भ विंडो का उपयोग करने से कम RAM का उपयोग होगा, इसलिए यदि GPU असफल हो रहा है तो हम एक छोटी विंडो की कोशिश करने की सलाह देते हैं।

```shell
interpreter --max_tokens 2000 --context_window 16000
```

### डीबग मोड

सहयोगियों को ओपन इंटरप्रेटर की जांच करने में मदद करने के लिए, `--verbose` मोड अत्यधिक वर्बोस होता है।

आप डीबग मोड को उसके फ़्लैग (`interpreter --verbose`) का उपयोग करके या चैट के बीच में सक्षम कर सकते हैं:

```shell
$ interpreter
...
> %verbose true <- डीबग मोड चालू करता है

> %verbose false <- डीबग मोड बंद करता है
```

### इंटरैक्टिव मोड कमांड्स

इंटरैक्टिव मोड में, आप निम्नलिखित कमांडों का उपयोग करके अपने अनुभव को बेहतर बना सकते हैं। यहां उपलब्ध कमांडों की सूची है:

**उपलब्ध कमांड:**
• `%verbose [true/false]`: डीबग मोड को टॉगल करें। कोई तर्क नहीं या 'true' के साथ, यह डीबग मोड में प्रवेश करता है। 'false' के साथ, यह डीबग मोड से बाहर निकलता है।
• `%reset`: वर्तमान सत्र को रीसेट करता है।
• `%undo`: पिछले संदेश और उसके जवाब को संदेश इतिहास से हटा देता है।
• `%save_message [पथ]`: संदेशों को एक निर्दिष्ट JSON पथ पर सहेजता है। यदि कोई पथ निर्दिष्ट नहीं किया गया है, तो यह डिफ़ॉल्ट रूप से 'messages.json' पर जाता है।
• `%load_message [पथ]`: एक निर्दिष्ट JSON पथ से संदेश लोड करता है। यदि कोई पथ निर्दिष्ट नहीं किया गया है, तो यह डिफ़ॉल्ट रूप से 'messages.json' पर जाता है।
• `%help`: मदद संदेश दिखाएं।

इन कमांडों का प्रयोग करके अपनी प्रतिक्रिया दें और हमें अपनी प्रतिक्रिया दें!

## सुरक्षा सूचना

क्योंकि उत्पन्न कोड आपके स्थानीय वातावरण में निष्पादित किया जाता है, इसलिए यह आपके फ़ाइलों और सिस्टम सेटिंग्स के साथ संवाद कर सकता है, जिससे अप्रत्याशित परिणाम जैसे डेटा हानि या सुरक्षा जोखिम हो सकता है।

**⚠️ Open Interpreter कोड को निष्पादित करने से पहले उपयोगकर्ता की पुष्टि के लिए पूछेगा।**

आप `interpreter -y` चला सकते हैं या ... ... `interpreter.auto_run = True` सेट कर सकते हैं ताकि इस पुष्टि को छोड़ दें, जिसके बाद:

- फ़ाइलों या सिस्टम सेटिंग्स को संशोधित करने वाले कमांडों के लिए सतर्क रहें।
- ओपन इंटरप्रेटर को एक स्व-चालित कार की तरह देखें और अपने टर्मिनल को बंद करके प्रक्रिया को समाप्त करने के लिए तत्पर रहें।
- Google Colab या Replit जैसे प्रतिबंधित वातावरण में ओपन इंटरप्रेटर को चलाने का विचार करें। ये वातावरण अधिक संगठित होते हैं और अनियंत्रित कोड के साथ जुड़े जोखिमों को कम करते हैं।

## यह कार्य कैसे करता है?

Open Interpreter एक [फ़ंक्शन-कॉलिंग भाषा मॉडल](https://platform.openai.com/docs/guides/gpt/function-calling) को एक `exec()` फ़ंक्शन के साथ लैस करता है, जो एक `language` (जैसे "Python" या "JavaScript") और `code` को चलाने के लिए स्वीकार करता है।

फिर हम मॉडल के संदेश, कोड और आपके सिस्टम के आउटपुट को टर्मिनल में मार्कडाउन के रूप में स्ट्रीम करते हैं।

# योगदान

योगदान करने के लिए आपकी रुचि के लिए धन्यवाद! हम समुदाय से सहभागिता का स्वागत करते हैं।

अधिक जानकारी के लिए कृपया हमारे [योगदान दिशानिर्देश](CONTRIBUTING.md) देखें।

## लाइसेंस

Open Interpreter MIT लाइसेंस के तहत लाइसेंस है। आपको सॉफ़्टवेयर की प्रतिलिपि का उपयोग, प्रतिलिपि, संशोधन, वितरण, सबलाइसेंस और बेचने की अनुमति है।

**ध्यान दें**: यह सॉफ़्टवेयर OpenAI से संबद्ध नहीं है।

> अपनी उंगलियों की गति से काम करने वाले एक जूनियर प्रोग्रामर तक पहुंच ... नए वर्कफ़्लो को सरल और कुशल बना सकता है, साथ ही ... प्रोग्रामिंग के लाभों को नए दरबारों तक पहुंचा सकता है।
>
> — _OpenAI's Code Interpreter Release_

<br>

================
File: README_JA.md
================
<h1 align="center">● Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Español-white.svg" alt="ES doc"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/文档-中文版-white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/Українська-white.svg" alt="UK doc"/></a>
    <a href="README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <b>自然言語で指示するだけでコードを書いて実行までしてくれる。</b><br>
    ローカルに実装したOpenAI Code Interpreterのオープンソース版。<br>
    <br><a href="https://openinterpreter.com">デスクトップアプリへの早期アクセス</a>‎ ‎ |‎ ‎ <a href="https://docs.openinterpreter.com/">ドキュメント</a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

**Update:** ● 0.1.12 アップデートで `interpreter --vision` 機能が導入されました。([ドキュメント](https://docs.openinterpreter.com/usage/terminal/vision))

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter**は、言語モデルに指示し、コード（Python、Javascript、Shell など）をローカル環境で実行できるようにします。インストール後、`$ interpreter` を実行するとターミナル経由で ChatGPT のようなインターフェースを介し、Open Interpreter とチャットができます。

これにより、自然言語のインターフェースを通して、パソコンの一般的な機能が操作できます。

- 写真、動画、PDF などの作成や編集
- Chrome ブラウザの制御とリサーチ作業
- 大規模なデータセットのプロット、クリーニング、分析
- 等々

**⚠️ 注意: 実行する前にコードを承認するよう求められます。**

<br>

## デモ

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Google Colab でも対話形式のデモを利用できます:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

#### 音声インターフェースの実装例 (_Her_ からインスピレーションを得たもの):

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

## クイックスタート

```shell
pip install open-interpreter
```

### ターミナル

インストール後、`interpreter` を実行するだけです:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("AAPLとMETAの株価グラフを描いてください") # コマンドを実行
interpreter.chat() # 対話形式のチャットを開始
```

## ChatGPT の Code Interpreter との違い

GPT-4 で実装された OpenAI の [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter) は、実世界のタスクを ChatGPT で操作できる素晴らしい機会を提供しています。

しかし、OpenAI のサービスはホスティングされていてるクローズドな環境で、かなり制限がされています:

- インターネットに接続できない。
- [プリインストールされているパッケージが限られている](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/)。
- 最大アップロードは 100MB で、120 秒という実行時間の制限も。
- 生成されたファイルやリンクとともに状態がリセットされる。

---

Open Interpreter は、ローカル環境で操作することで、これらの制限を克服しています。インターネットにフルアクセスでき、時間やファイルサイズの制限を受けず、どんなパッケージやライブラリも利用できます。

Open Interpter は、GPT-4 Code Interpreter のパワーとローカル開発環境の柔軟性を組み合わせたものです。

## コマンド

**更新:** アップデート(0.1.5)でストリーミング機能が導入されました:

```python
message = "どのオペレーティングシステムを使用していますか？"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### 対話型チャット

ターミナルで対話形式のチャットを開始するには、コマンドラインから `interpreter` を実行します。

```shell
interpreter
```

または、.py ファイルから `interpreter.chat()` も利用できます。

```python
interpreter.chat()
```

**ストリーミングすることで chunk 毎に処理することも可能です:**

```python
message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### プログラム的なチャット

より精確な制御のために、メッセージを直接`.chat(message)`に渡すことができます。

```python
interpreter.chat("/videos フォルダにあるすべての動画に字幕を追加する。")

# ... ターミナルに出力をストリームし、タスクを完了 ...

interpreter.chat("ついでに、字幕を大きくできますか？")

# ...
```

### 新しいチャットを開始

プログラム的チャットで Open Interpreter は、会話の履歴を記憶しています。新しくやり直したい場合は、リセットすることができます:

```python
interpreter.messages = []
```

### チャットの保存と復元

`interpreter.chat()` はメッセージのリストを返し, `interpreter.messages = messages` のように使用することで会話を再開することが可能です:

```python
messages = interpreter.chat("私の名前は田中です。") # 'messages'にメッセージを保存
interpreter.messages = [] # インタープリタをリセット（"田中"は忘れられる）

interpreter.messages = messages # 'messages'からチャットを再開（"田中"は記憶される）
```

### システムメッセージのカスタマイズ

Open Interpreter のシステムメッセージを確認し、設定することで、機能を拡張したり、権限を変更したり、またはより多くのコンテキストを与えたりすることができます。

```python
interpreter.system_message += """
シェルコマンドを '-y' フラグ付きで実行し、ユーザーが確認する必要がないようにする。
"""
print(interpreter.system_message)
```

### モデルの変更

Open Interpreter は、ホストされた言語モデルへの接続に [LiteLLM](https://docs.litellm.ai/docs/providers/) を使用しています。

model パラメータを設定することで、モデルを変更することが可能です:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

Python では、オブジェクト上でモデルを設定します:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[適切な "model" の値はこちらから検索してください。](https://docs.litellm.ai/docs/providers/)

### ローカルのモデルを実行する

Open Interpreter は、OpenAI 互換サーバーを使用してモデルをローカルで実行できます。 (LM Studio、jan.ai、ollam など)

推論サーバーの api_base URL を指定して「interpreter」を実行するだけです (LM Studio の場合、デフォルトでは「http://localhost:1234/v1」です)。

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

あるいは、サードパーティのソフトウェアをインストールせずに、単に実行するだけで Llamafile を使用することもできます。

```shell
interpreter --local
```

より詳細なガイドについては、[Mike Bird によるこのビデオ](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H) をご覧ください。

**LM Studio をバックグラウンドで使用する方法**

1. [https://lmstudio.ai/](https://lmstudio.ai/)からダウンロードして起動します。
2. モデルを選択し、**↓ ダウンロード** をクリックします。
3. 左側の **↔️** ボタン（💬 の下）をクリックします。
4. 上部でモデルを選択し、**サーバーを起動** をクリックします。

サーバーが稼働を開始したら、Open Interpreter との会話を開始できます。

> **注意:** ローカルモードでは、`context_window` を 3000 に、`max_tokens` を 1000 に設定します。モデルによって異なる要件がある場合、これらのパラメータを手動で設定してください（下記参照）。

#### コンテキストウィンドウ、最大トークン数

ローカルで実行しているモデルの `max_tokens` と `context_window`（トークン単位）を変更することができます。

ローカルモードでは、小さいコンテキストウィンドウは RAM を少なく使用するので、失敗する場合や遅い場合は、より短いウィンドウ（〜1000）を試すことをお勧めします。`max_tokens` が `context_window` より小さいことを確認してください。

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

### デバッグモード

コントリビューターが Open Interpreter を調査するのを助けるために、`--verbose` モードは非常に便利です。

デバッグモードは、フラグ（`interpreter --verbose`）を使用するか、またはチャットの中から有効にできます:

```shell
$ interpreter
...
> %verbose true # <- デバッグモードを有効にする

> %verbose false # <- デバッグモードを無効にする
```

### 対話モードのコマンド

対話モードでは、以下のコマンドを使用して操作を便利にすることができます。利用可能なコマンドのリストは以下の通りです:

**利用可能なコマンド:**

- `%verbose [true/false]`: デバッグモードを切り替えます。引数なしまたは `true` でデバッグモードに入ります。`false` でデバッグモードを終了します。
- `%reset`: 現在のセッションの会話をリセットします。
- `%undo`: メッセージ履歴から前のユーザーメッセージと AI の応答を削除します。
- `%save_message [path]`: メッセージを指定した JSON パスに保存します。パスが指定されていない場合、デフォルトは `messages.json` になります。
- `%load_message [path]`: 指定した JSON パスからメッセージを読み込みます。パスが指定されていない場合、デフォルトは `messages.json` になります。
- `%tokens [prompt]`: (_実験的_) 次のプロンプトのコンテキストとして送信されるトークンを計算し、そのコストを見積もります。オプションで、`prompt` が提供された場合のトークンと見積もりコストを計算します。見積もりコストは [LiteLLM の `cost_per_token()` メソッド](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token)に依存します。
- `%help`: ヘルプメッセージを表示します。

### 設定

Open Interpreter では、`config.yaml` ファイルを使用してデフォルトの動作を設定することができます。

これにより、毎回コマンドライン引数を変更することなく柔軟に設定することができます。

以下のコマンドを実行して設定ファイルを開きます:

```
interpreter --config
```

#### 設定ファイルの複数利用

Open Interpreter は複数の `config.yaml` ファイルをサポートしており、`--config_file` 引数を通じて簡単に設定を切り替えることができます。

**注意**: `--config_file` はファイル名またはファイルパスを受け入れます。ファイル名はデフォルトの設定ディレクトリを使用し、ファイルパスは指定されたパスを使用します。

新しい設定を作成または編集するには、次のコマンドを実行します:

```
interpreter --config --config_file $config_path
```

特定の設定ファイルをロードして Open Interpreter を実行するには、次のコマンドを実行します:

```
interpreter --config_file $config_path
```

**注意**: `$config_path` をあなたの設定ファイルの名前またはパスに置き換えてください。

##### 対話モードでの使用例

1. 新しい `config.turbo.yaml` ファイルを作成します
   ```
   interpreter --config --config_file config.turbo.yaml
   ```
2. `config.turbo.yaml` ファイルを編集して、`model` を `gpt-3.5-turbo` に設定します
3. `config.turbo.yaml` 設定で、Open Interpreter を実行します
   ```
   interpreter --config_file config.turbo.yaml
   ```

##### Python での使用例

Python のスクリプトから Open Interpreter を呼び出すときにも設定ファイルをロードできます:

```python
import os
from interpreter import interpreter

currentPath = os.path.dirname(os.path.abspath(__file__))
config_path=os.path.join(currentPath, './config.test.yaml')

interpreter.extend_config(config_path=config_path)

message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

## FastAPI サーバーのサンプル

アップデートにより Open Interpreter は、HTTP REST エンドポイントを介して制御できるようになりました:

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

## 安全に関する注意

生成されたコードはローカル環境で実行されるため、ファイルやシステム設定と相互作用する可能性があり、データ損失やセキュリティリスクなど予期せぬ結果につながる可能性があります。

**⚠️ Open Interpreter はコードを実行する前にユーザーの確認を求めます。**

この確認を回避するには、`interpreter -y` を実行するか、`interpreter.auto_run = True` を設定します。その場合:

- ファイルやシステム設定を変更するコマンドを要求するときは注意してください。
- Open Interpreter を自動運転車のように監視し、ターミナルを閉じてプロセスを終了できるように準備しておいてください。
- Google Colab や Replit のような制限された環境で Open Interpreter を実行することを検討してください。これらの環境はより隔離されており、任意のコードの実行に関連するリスクを軽減します。

一部のリスクを軽減するための[セーフモード](docs/SAFE_MODE.md)と呼ばれる **実験的な** サポートがあります。

## Open Interpreter はどのように機能するのか？

Open Interpreter は、[関数が呼び出せる言語モデル](https://platform.openai.com/docs/guides/gpt/function-calling)に `exec()` 関数を装備し、実行する言語（"python"や"javascript"など）とコードが渡せるようになっています。

そして、モデルからのメッセージ、コード、システムの出力を Markdown としてターミナルにストリーミングします。

# 貢献

貢献に興味を持っていただき、ありがとうございます！コミュニティからの参加を歓迎しています。

詳しくは、[貢献ガイドライン](CONTRIBUTING.md)を参照してください。

# ロードマップ

Open Interpreter の未来を一足先に見るために、[私たちのロードマップ](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md)をご覧ください。

**注意**: このソフトウェアは OpenAI とは関連していません。

> あなたの指先のスピードで作業するジュニアプログラマーにアクセスすることで、… 新しいワークフローを楽で効率的なものにし、プログラミングの利点を新しいオーディエンスに開放することができます。
>
> — _OpenAI Code Interpreter リリース_

<br>

================
File: README_UK.md
================
<h1 align="center">● Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/Hvz9Axh84z">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/ドキュメント-日本語-white.svg" alt="JA doc"/></a>
    <a href="README_ZH.md"><img src="https://img.shields.io/badge/文档-中文版-white.svg" alt="ZH doc"/></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Español-white.svg" alt="ES doc"/></a>
    <a href="README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <br><a href="https://0ggfznkwh4j.typeform.com/to/G21i9lJ2">Отримайте ранній доступ до десктопної програми</a>‎ ‎ |‎ ‎ <a href="https://docs.openinterpreter.com/">Документація</a><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>
<p align="center">
<strong>Нове комп'ютерне оновлення</strong> представило <strong><code>--os</code></strong> та новий <strong>Computer API</strong>. <a href="https://changes.openinterpreter.com/log/the-new-computer-update">Читати далі →</a>
</p>
<br>

```shell
pip install open-interpreter
```

> Не працює? Прочитайте наш [посібник з налаштування](https://docs.openinterpreter.com/getting-started/setup).

```shell
interpreter
```

<br>

**Open Interpreter** дозволяє LLM локально запускати код (Python, Javascript, Shell тощо). Ви можете спілкуватися з Open Interpreter через інтерфейс, схожий на ChatGPT, у вашому терміналі, запустивши `$ interpreter` після встановлення.

Це забезпечує інтерфейс природною мовою для загального використання можливостей вашого комп’ютера:

- Створювати та редагувати фотографії, відео, PDF-файли тощо.
- Керувати браузером Chrome для проведення досліджень
- Створювати, очищати та аналізувати великі набори даних
- ...і т.д.

**⚠️ Увага: Вам буде запропоновано підтвердити код перед його запуском.**

<br>

## Demo

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Інтерактивна демонстрація також доступна на Google Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

#### Разом із прикладом голосового інтерфейсу, натхненного _Her_:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

## Швидкий старт

```shell
pip install open-interpreter
```

### Термінал

Після встановлення просто запустіть `interpreter`:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Plot AAPL and META's normalized stock prices") # Виконує одну команду
interpreter.chat() # Починає інтерактивний чат
```

### GitHub Codespaces

Натисніть клавішу `,` на сторінці GitHub цього репозиторію, щоб створити Codespace. Через деякий час ви отримаєте хмарне середовище віртуальної машини, попередньо встановлене з відкритим інтерпретатором. Потім ви можете почати взаємодіяти з ним безпосередньо та вільно підтверджувати виконання ним системних команд, не турбуючись про пошкодження системи.

## Порівняння з інтерпретатором коду ChatGPT

Випуск OpenAI [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter) з GPT-4 надає фантастичну можливість виконувати реальні завдання за допомогою ChatGPT.

Однак служба OpenAI є хмарною, з закритим вихідним кодом і суворо обмежена:

- Немає доступу до Інтернету.
- [Обмежений набір попередньо встановлених пакетів](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
- Максимальний розмір завантаження - 100 МБ, обмеження часу виконання - 120,0 секунд.
- Стан очищається (разом із будь-якими згенерованими файлами чи посиланнями), коли середовище зупиняється.

---

Open Interpreter долає ці обмеження, запускаючись у вашому локальному середовищі. Він має повний доступ до Інтернету, не обмежений часом або розміром файлу, і може використовувати будь-який пакет або бібліотеку.

Це поєднує потужність інтерпретатора коду GPT-4 із гнучкістю вашого локального середовища розробки.

## Команди

**Оновлення:** Оновлення Generator (0.1.5) представило потокове передавання:

```python
message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### Інтерактивний чат

Щоб почати інтерактивний чат у вашому терміналі, запустіть `interpreter` з командного рядка:

```shell
interpreter
```

Або `interpreter.chat()` з файлу .py:

```python
interpreter.chat()
```

**Ви також можете транслювати кожен фрагмент:**

```python
message = "На якій операційній системі ми працюємо?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### Програмований чат

Для більш точного керування ви можете передавати повідомлення безпосередньо до `.chat(message)`:

```python
interpreter.chat("Додайте субтитри до всіх відео в /videos.")

# ... Потік виведення на ваш термінал, виконання завдання ...

interpreter.chat("Виглядає чудово, але чи можеш ти збільшити субтитри?")

# ...
```

### Почати новий чат

В Python, Open Interpreter запам’ятовує історію розмов. Якщо ви хочете почати заново, ви можете скинути її:

```python
interpreter.messages = []
```

### Зберегти та відновити чати

`interpreter.chat()` повертає список повідомлень, який можна використовувати для відновлення розмови за допомогою `interpreter.messages = messages`:

```python
messages = interpreter.chat("Мене звати Степан.") # Зберегти повідомлення в "messages"
interpreter.messages = [] # Скинути інтерпретатор ("Степан" буде забутий)

interpreter.messages = messages # Відновити чат із "messages" ("Степан" запам’ятається)
```

### Кастомізувати системне повідомлення

Ви можете перевірити та налаштувати системне повідомлення Open Interpreter, щоб розширити його функціональність, змінити дозволи або надати йому більше контексту.

```python
interpreter.system_message += """
Виконуй команди оболонки з -y, щоб користувачеві не потрібно було їх підтверджувати.
"""
print(interpreter.system_message)
```

### Змініть свою мовну модель

Open Interpreter використовує [LiteLLM](https://docs.litellm.ai/docs/providers/) для підключення до розміщених мовних моделей.

Ви можете змінити модель, встановивши параметр моделі:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

В Pythonб встановити модель на об’єкт:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Знайдіть відповідний рядок «model» для вашої мовної моделі тут.](https://docs.litellm.ai/docs/providers/)

### Запуск Open Interpreter локально

#### Термінал

Open Interpreter може використовувати OpenAI-сумісний сервер для запуску моделей локально. (LM Studio, jan.ai, ollama тощо)

Просто запустіть `interpreter` з URL-адресою api_base вашого сервера interference (для LM Studio це `http://localhost:1234/v1` за замовчуванням):

```shell
interpreter --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

Крім того, ви можете використовувати Llamafile без встановлення стороннього програмного забезпечення, просто запустивши його

```shell
interpreter --local
```

for a more detailed guide check out [this video by Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

**Як запустити LM Studio у фоновому режимі.**

1. Завантажте [https://lmstudio.ai/](https://lmstudio.ai/), після чого запустіть його.
2. Виберіть модель і натисніть **↓ Завантажити**.
3. Натисніть кнопку **↔️** ліворуч (нижче 💬).
4. Виберіть свою модель угорі, а потім натисніть **Запустити сервер**.

Коли сервер запущено, ви можете почати розмову за допомогою Open Interpreter.

> **Примітка.** Локальний режим встановлює ваше `context_window` на 3000, а `max_tokens` на 1000. Якщо ваша модель має інші вимоги, установіть ці параметри вручну (див. нижче).

#### Python

Наш пакет Python дає вам більше контролю над кожним параметром. Для реплікації та підключення до LM Studio використовуйте ці налаштування:

```python
from interpreter import interpreter

interpreter.offline = True # Вимикає такі онлайн-функції, як Open Procedures
interpreter.llm.model = "openai/x" # Каже AI надсилати повідомлення у форматі OpenAI
interpreter.llm.api_key = "fake_key" # LiteLLM, який ми використовуємо для спілкування з LM Studio, вимагає api-ключ
interpreter.llm.api_base = "http://localhost:1234/v1" # Познчате це на будь-якому сервері, сумісному з OpenAI

interpreter.chat()
```

#### Контекстне вікно, максимальна кількість токенів

Ви можете змінити `max_tokens` і `context_window` (у токенах) локально запущених моделей.

У локальному режимі менші контекстні вікна використовуватимуть менше оперативної пам’яті, тому ми рекомендуємо спробувати набагато коротше вікно (~1000), якщо воно не вдається або працює повільно. Переконайтеся, що `max_tokens` менший за `context_window`.

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

### Режим "verbose"

Щоб допомогти вам перевірити Open Interpreter, у нас є режим `--verbose` для налагодження.

Ви можете активувати режим "verbose", використовуючи його прапорець (`interpreter --verbose`) або в середині чату:

```shell
$ interpreter
...
> %verbose true <- Вмикає режим verbose

> %verbose false <- Вимикає режим verbose
```

### Команди інтерактивного режиму

В інтерактивному режимі ви можете використовувати наведені нижче команди, щоб покращити свій досвід. Ось список доступних команд:
**Доступні команди:**

- `%verbose [true/false]`: увімкнути режим verbose. Без аргументів або з `true`.
  переходить у багатослівний режим. З `false` він виходить із багатослівного режиму.
- `%reset`: скидає розмову поточного сеансу.
- `% undo`: видаляє попереднє повідомлення користувача та відповідь ШІ з історії повідомлень.
- `%tokens [prompt]`: (_Експериментально_) Розрахувати токени, які будуть надіслані з наступним запитом як контекст, і оцінити їх вартість. Додатково обчисліть токени та приблизну вартість «підказки», якщо вона надається. Покладається на [метод `cost_per_token()` LiteLLM](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token) для оцінки витрат.
- `%help`: Показати повідомлення довідки.

### Конфігурація / Профілі

Open Interpreter дозволяє встановлювати поведінку за замовчуванням за допомогою файлів `yaml`.

Це забезпечує гнучкий спосіб налаштування інтерпретатора, не змінюючи щоразу аргументи командного рядка.

Виконайте цю команду, щоб відкрити каталог профілів:

```
interpreter --profiles
```

Ви можете додати файли `yaml`. Профіль за замовчуванням має назву `default.yaml`.

#### Кілька профілів

Open Interpreter підтримує декілька файлів `yaml`, що дозволяє легко перемикатися між конфігураціями:

```
interpreter --profile my_profile.yaml
```

## Зразок сервера FastAPI

Оновлення генератора дозволяє керувати Open Interpreter через кінцеві точки HTTP REST:

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

Ви також можете запустити сервер, ідентичний наведеному вище, просто запустивши `interpreter.server()`.

## Android

Покроковий посібник із встановлення Open Interpreter на вашому пристрої Android можна знайти в [репозиторії open-interpreter-termux](https://github.com/MikeBirdTech/open-interpreter-termux).

## Повідомлення про безпеку

Оскільки згенерований код виконується у вашому локальному середовищі, він може взаємодіяти з вашими файлами та налаштуваннями системи, потенційно призводячи до неочікуваних результатів, як-от втрати даних або ризиків для безпеки.

**⚠️ Open Interpreter попросить підтвердження користувача перед виконанням коду.**

Ви можете запустити `interpreter -y` або встановити `interpreter.auto_run = True`, щоб обійти це підтвердження, у такому випадку:

- Будьте обережні, запитуючи команди, які змінюють файли або налаштування системи.
- Дивіться на Open Interpreter як на самокерований автомобіль і будьте готові завершити процес, закривши термінал.
- Спробуйте запустити Open Interpreter у обмеженому середовищі, наприклад Google Colab або Replit. Ці середовища більш ізольовані, що зменшує ризики виконання довільного коду.

Існує **експериментальна** підтримка [безпечного режиму](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/SAFE_MODE.md), щоб зменшити деякі ризики.

## Як це працює?

Open Interpreter оснащує [модель мови виклику функцій](https://platform.openai.com/docs/guides/gpt/function-calling) функцією `exec()`, яка приймає `мову` (як "Python" або "JavaScript") і `code` для запуску.

Потім ми передаємо повідомлення моделі, код і результати вашої системи на термінал як Markdown.

# Доступ до документації в автономному режимі

Повна [документація](https://docs.openinterpreter.com/) доступна в дорозі без підключення до Інтернету.

[Node](https://nodejs.org/en) є необхідною умовою:

- Версія 18.17.0 або будь-яка пізніша версія 18.x.x.
- Версія 20.3.0 або будь-яка пізніша версія 20.x.x.
- Будь-яка версія, починаючи з 21.0.0 і далі, без вказівки верхньої межі.

Встановіть [Mintlify](https://mintlify.com/):

```bash
npm i -g mintlify@latest
```

Перейдіть у каталог документів і виконайте відповідну команду:

```bash
# Якщо ви перебуваєте в кореневому каталозі проекту
cd ./docs

# Запустіть сервер документації
mintlify dev
```

Має відкритися нове вікно браузера. Документація буде доступна за адресою [http://localhost:3000](http://localhost:3000), поки працює сервер документації.

# Вклади

Дякуємо за ваш інтерес до участі! Ми вітаємо участь спільноти.

Щоб дізнатися більше про те, як взяти участь, ознайомтеся з нашими [інструкціями щодо створення внеску](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md).

# Дорожня карта

Відвідайте [нашу дорожню карту](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md), щоб переглянути майбутнє Open Interpreter.

**Примітка**: це програмне забезпечення не пов’язане з OpenAI.

![thumbnail-ncu](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/1b19a5db-b486-41fd-a7a1-fe2028031686)

> Маючи доступ до джуніора, який працює зі швидкістю ваших пальців ... ви можете зробити нові робочі процеси легкими та ефективними, а також відкрити переваги програмування новій аудиторії.
>
> — _OpenAI's Code Interpreter Release_

<br>

================
File: README_VN.md
================
<h1 align="center">● Open Interpreter</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Español-white.svg" alt="ES doc"/></a>
    <a href="docs/README_JA.md"><img src="https://img.shields.io/badge/ドキュメント-日本語-white.svg" alt="JA doc"/></a>
    <a href="docs/README_ZH.md"><img src="https://img.shields.io/badge/文档-中文版-white.svg" alt="ZH doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/Українська-white.svg" alt="UK doc"/></a>
    <a href="docs/README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <b>chạy mô hình ngôn ngữ trí tuệ nhân tạo trên máy tính của bạn.</b><br>
    Mã nguồn mở và ứng dụng phát triển dựa trên code của OpenAI.<br>
    <br><a href="https://openinterpreter.com">Quyền truy cập sớm dành cho máy tính cá nhân</a>‎ ‎ |‎ ‎ <b><a href="https://docs.openinterpreter.com/">Tài liệu đọc tham khảo</a></b><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter** Chạy LLMs trên máy tính cục bộ (Có thể sử dụng ngôn ngữ Python, Javascript, Shell, và nhiều hơn thế). Bạn có thể nói chuyện với Open Interpreter thông qua giao diện giống với ChatGPT ngay trên terminal của bạn bằng cách chạy lệnh `$ interpreter` sau khi tải thành công.

Các tính năng chung giao diện ngôn ngữ mang llại

- Tạo và chỉnh sửa ảnh, videos, PDF, vân vân...
- Điều khiển trình duyệt Chrome để tiến hành nghiên cứu
- Vẽ, làm sạch và phân tích các tập dữ liệu lớn (large datasets)
- ...vân vân.

**⚠️ Lưu ý: Bạn sẽ được yêu cầu phê duyệt mã trước khi chạy.**

<br>

## Thử nghiệm

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Bản thử nghiệm có sẵn trên Google Colab:

[![Mở trong Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

#### Đi kèm với ứng dụng mẫu qua tương tác giọng nói (Lấy cảm hứng từ _Cô ấy_ (Giọng nữ)):

[![Mở trong Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK)

## Hướng dẫn khởi dộng ngắn

```shell
pip install open-interpreter
```

### Terminal

Sau khi cài đặt, chạy dòng lệnh `interpreter`:

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Vẽ giá cổ phiếu đã bình hoá của AAPL và META ") # Chạy trên 1 dòng lệnh
interpreter.chat() # Khởi động chat có khả năng tương tác
```

## So sánh Code Interpreter của ChatGPT

Bản phát hành của OpenAI [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter) sử dụng GPT-4 tăng khả năng hoàn thiện vấn đề thực tiễn với ChatGPT.

Tuy nhiên, dịch vụ của OpenAI được lưu trữ, mã nguồn đóng, và rất hạn chế:

- Không có truy cập Internet.
- [Số lượng gói cài đặt hỗ trỡ có sẵn giới hạn](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/).
- tốc độ tải tối đa 100 MB , thời gian chạy giới hạn 120.0 giây .
- Trạng thái tin nhắn bị xoá kèm với các tệp và liên kết được tạo trước đó khi đóng môi trường lại.

---

Open Interpreter khắc phục những hạn chế này bằng cách chạy cục bộ trobộ môi trường máy tính của bạn. Nó có toàn quyền truy cập vào Internet, không bị hạn chế về thời gian hoặc kích thước tệp và có thể sử dụng bất kỳ gói hoặc thư viện nào.

Đây là sự kết hợp sức mạnh của mã nguồn của GPT-4 với tính linh hoạt của môi trường phát triển cục bộ của bạn.

## Dòng lệnh

**Update:** Cập nhật trình tạo lệnh (0.1.5) giới thiệu tính năng trực tuyến:

```python
message = "Chúng ta đang ở trên hệ điều hành nào?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### Trò chuyện tương tác

Để tạo một cuộc trò chuyện tương tác từ terminal của bạn, chạy `interpreter` bằng dòng lệnh:

```shell
interpreter
```

hoặc `interpreter.chat()` từ file có đuôi .py :

```python
interpreter.chat()
```

**Bạn cũng có thể phát trực tuyến từng đoạn:**

```python
message = "Chúng ta đang chạy trên hệ điều hành nào?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

### Trò chuyện lập trình được

Để kiểm soát tốt hơn, bạn chuyển tin nhắn qua `.chat(message)`:

```python
interpreter.chat("Truyền phụ đề tới tất cả videos vào /videos.")

# ... Truyền đầu ra đến thiết bị đầu cuối của bạn (terminal) hoàn thành tác vụ ...

interpreter.chat("Nhìn đẹp đấy nhưng bạn có thể làm cho phụ đề lớn hơn được không?")

# ...
```

### Tạo một cuộc trò chuyện mới:

Trong Python, Open Interpreter ghi nhớ lịch sử hội thoại, nếu muốn bắt đầu lại từ đầu, bạn có thể cài thứ:

```python
interpreter.messages = []
```

### Lưu và khôi phục cuộc trò chuyện

`interpreter.chat()` trả về danh sách tin nhắn, có thể được sử dụng để tiếp tục cuộc trò chuyện với `interpreter.messages = messages`:

```python
messages = interpreter.chat("Tên của tôi là Killian.") # Lưu tin nhắn tới 'messages'
interpreter.messages = [] # Khởi động lại trình phiên dịch ("Killian" sẽ bị lãng quên)

interpreter.messages = messages # Tiếp tục cuộc trò chuyện từ 'messages' ("Killian" sẽ được ghi nhớ)
```

### Cá nhân hoá tin nhắn từ hệ thống

Bạn có thể kiếm tra và điều chỉnh tin nhắn hệ thống từ Optừ Interpreter để mở rộng chức năng của nó, thay đổi quyền, hoặc đưa cho nó nhiều ngữ cảnh hơn.

```python
interpreter.system_message += """
Chạy shell commands với -y để người dùng không phải xác nhận chúng.
"""
print(interpreter.system_message)
```

### Thay đổi mô hình ngôn ngữ

Open Interpreter sử dụng mô hình [LiteLLM](https://docs.litellm.ai/docs/providers/) để kết nối tới các mô hình ngôn ngữ được lưu trữ trước đó.

Bạn có thể thay đổi mô hình ngôn ngữ bằng cách thay đổi tham số mô hình:

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

Ở trong Python, đổi model bằng cách thay đổi đối tượng:

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

[Tìm tên chuỗi "mô hình" phù hợp cho mô hình ngôn ngữ của bạn ở đây.](https://docs.litellm.ai/docs/providers/)

### Chạy Open Interpreter trên máy cục bộ

Open Interpreter có thể sử dụng máy chủ tương thích với OpenAI để chạy các mô hình cục bộ. (LM Studio, jan.ai, ollama, v.v.)

Chỉ cần chạy `interpreter` với URL api_base của máy chủ suy luận của bạn (đối với LM studio, nó là `http://localhost:1234/v1` theo mặc định):

```vỏ
trình thông dịch --api_base "http://localhost:1234/v1" --api_key "fake_key"
```

Ngoài ra, bạn có thể sử dụng Llamafile mà không cần cài đặt bất kỳ phần mềm bên thứ ba nào chỉ bằng cách chạy

```vỏ
thông dịch viên --local
```

để biết hướng dẫn chi tiết hơn, hãy xem [video này của Mike Bird](https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H)

**Để chạy LM Studio ở chế độ nền.**

1. Tải [https://lmstudio.ai/](https://lmstudio.ai/) và khởi động.
2. Chọn một mô hình rồi nhấn **↓ Download**.
3. Nhấn vào nút **↔️** ở bên trái (dưới 💬).
4. Chọn mô hình của bạn ở phía trên, rồi nhấn chạy **Start Server**.

Một khi server chạy, bạn có thể bắt đầu trò chuyện với Open Interpreter.

> **Lưu ý:** Chế độ cục bộ chỉnh `context_window` của bạn tới 3000, và `max_tokens` của bạn tới 600. Nếu mô hình của bạn có các yêu cầu khác, thì hãy chỉnh các tham số thủ công (xem bên dưới).

#### Cửa sổ ngữ cảnh (Context Window), (Max Tokens)

Bạn có thể thay đổi `max_tokens` và `context_window` (ở trong các) of locally running models.

Ở chế độ cục bộ, các cửa sổ ngữ cảnh sẽ tiêu ít RAM hơn, vậy nên chúng tôi khuyến khích dùng cửa sổ nhỏ hơn (~1000) nếu như nó chạy không ổn định / hoặc nếu nó chậm. Đảm bảo rằng `max_tokens` ít hơn `context_window`.

```shell
interpreter --local --max_tokens 1000 --context_window 3000
```

### Chế độ sửa lỗi

Để giúp đóng góp kiểm tra Open Interpreter, thì chế độ `--verbose` hơi dài dòng.

Bạn có thể khởi động chế độ sửa lỗi bằng cách sử dụng cờ (`interpreter --verbose`), hoặc mid-chat:

```shell
$ interpreter
...
> %verbose true <- Khởi động chế độ gỡ lỗi

> %verbose false <- Tắt chế độ gỡ lỗi
```

### Lệnh chế độ tương tác

Trong chế độ tương tác, bạn có thể sử dụng những dòng lệnh sau để cải thiện trải nghiệm của mình. Đây là danh sách các dòng lệnh có sẵn:

**Các lệnh có sẵn:**

- `%verbose [true/false]`: Bật chế độ gỡ lỗi. Có hay không có `true` đều khởi động chế độ gỡ lỗi. Với `false` thì nó tắt chế độ gỡ lỗi.
- `%reset`: Khởi động lại toàn bộ phiên trò chuyện hiện tại.
- `%undo`: Xóa tin nhắn của người dùng trước đó và phản hồi của AI khỏi lịch sử tin nhắn.
- `%save_message [path]`: Lưu tin nhắn vào một đường dẫn JSON được xác định từ trước. Nếu không có đường dẫn nào được cung cấp, nó sẽ mặc định là `messages.json`.
- `%load_message [path]`: Tải tin nhắn từ một đường dẫn JSON được chỉ định. Nếu không có đường dẫn nào được cung cấp, nó sẽ mặc định là `messages.json`.
- `%tokens [prompt]`: (_Experimental_) Tính toán các token sẽ được gửi cùng với lời nhắc tiếp theo dưới dạng ngữ cảnh và hao tổn. Tùy chọn tính toán token và hao tổn ước tính của một `prompt` nếu được cung cấp. Dựa vào [hàm `cost_per_token()` của mô hình LiteLLM](https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token) để tính toán hao tổn.
- `%help`: Hiện lên trợ giúp cho cuộc trò chuyện.

### Cấu hình cài

Open Interpreter cho phép bạn thiết lập các tác vụ mặc định bằng cách sử dụng file `config.yaml`.

Điều này cung cấp một cách linh hoạt để định cấu hình trình thông dịch mà không cần thay đổi đối số dòng lệnh mỗi lần

Chạy lệnh sau để mở tệp cấu hình:

```
interpreter --config
```

#### Cấu hình cho nhiều tệp

Open Interpreter hỗ trợ nhiều file `config.yaml`, cho phép bạn dễ dàng chuyển đổi giữa các cấu hình thông qua lệnh `--config_file`.

**Chú ý**: `--config_file` chấp nhận tên tệp hoặc đường dẫn tệp. Tên tệp sẽ sử dụng thư mục cấu hình mặc định, trong khi đường dẫn tệp sẽ sử dụng đường dẫn đã chỉ định.

Để tạo hoặc chỉnh sửa cấu hình mới, hãy chạy:

```
interpreter --config --config_file $config_path
```

Để yêu cầu Open Interpreter chạy một tệp cấu hình cụ thể, hãy chạy:

```
interpreter --config_file $config_path
```

**Chú ý**: Thay đổi `$config_path` với tên hoặc đường dẫn đến tệp cấu hình của bạn.

##### Ví dụ CLI

1. Tạo mới một file `config.turbo.yaml`
   ```
   interpreter --config --config_file config.turbo.yaml
   ```
2. Chạy file `config.turbo.yaml`để đặt lại `model` thành `gpt-3.5-turbo`
3. Chạy Open Interpreter với cấu hình `config.turbo.yaml
   ```
   interpreter --config_file config.turbo.yaml
   ```

##### Ví dụ Python

Bạn cũng có thể tải các tệp cấu hình khi gọi Open Interpreter từ tập lệnh Python:

```python
import os
from interpreter import interpreter

currentPath = os.path.dirname(os.path.abspath(__file__))
config_path=os.path.join(currentPath, './config.test.yaml')

interpreter.extend_config(config_path=config_path)

message = "What operating system are we on?"

for chunk in interpreter.chat(message, display=False, stream=True):
  print(chunk)
```

## Máy chủ FastAPI mẫu

Bản cập nhật trình tạo cho phép điều khiển Trình thông dịch mở thông qua các điểm cuối HTTP REST:

```python
# server.py

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from interpreter import interpreter

app = FastAPI()

@app.get("/chat")
def chat_endpoint(message: str):
    def event_stream():
        for result in interpreter.chat(message, stream=True):
            yield f"data: {result}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.get("/history")
def history_endpoint():
    return interpreter.messages
```

```shell
pip install fastapi uvicorn
uvicorn server:app --reload
```

## Hướng dẫn an toàn

Vì mã được tạo được thực thi trong môi trường cục bộ của bạn nên nó có thể tương tác với các tệp và cài đặt hệ thống của bạn, có khả năng dẫn đến các kết quả không mong muốn như mất dữ liệu hoặc rủi ro bảo mật.

**⚠️ Open Interpreter sẽ yêu cầu xác nhận của người dùng trước khi chạy code.**

Bạn có thể chạy `interpreter -y` hoặc đặt `interpreter.auto_run = True` để bỏ qua xác nhận này, trong trường hợp đó:

- Hãy thận trọng khi yêu cầu các lệnh sửa đổi tệp hoặc cài đặt hệ thống.
- Theo dõi Open Interpreter giống như một chiếc ô tô tự lái và sẵn sàng kết thúc quá trình bằng cách đóng terminal của bạn.
- Cân nhắc việc chạy Open Interpreter trong môi trường bị hạn chế như Google Colab hoặc Replit. Những môi trường này biệt lập hơn, giảm thiểu rủi ro khi chạy code tùy ý.

Đây là hỗ trợ **thử nghiệm** cho [chế độ an toàn](docs/SAFE_MODE.md) giúp giảm thiểu rủi ro.

## Nó hoạt động thế nào?

Open Interpreter trang bị [mô hình ngôn ngữ gọi hàm](https://platform.openai.com/docs/guides/gpt/function-calling) với một hàm `exec()`, chấp nhận một `language` (như "Python" hoặc "JavaScript") và `code` để chạy.

Sau đó, chúng tôi truyền trực tuyến thông báo, mã của mô hình và kết quả đầu ra của hệ thống của bạn đến terminal dưới dạng Markdown.

# Đóng góp

Cảm ơn bạn đã quan tâm đóng góp! Chúng tôi hoan nghênh sự tham gia của cộng đồng.

Vui lòng xem [Hướng dẫn đóng góp](CONTRIBUTING.md) để biết thêm chi tiết cách tham gia.

## Giấy phép

Open Interpreter được cấp phép theo Giấy phép MIT. Bạn được phép sử dụng, sao chép, sửa đổi, phân phối, cấp phép lại và bán các bản sao của phần mềm.

**Lưu ý**: Phần mềm này không liên kết với OpenAI.

> Có quyền truy cập vào một lập trình viên cấp dưới làm việc nhanh chóng trong tầm tay bạn ... có thể khiến quy trình làm việc mới trở nên dễ dàng và hiệu quả, cũng như mở ra những lợi ích của việc lập trình cho người mới.
>
> — _Phát hành trình thông dịch mã của OpenAI_

<br>

================
File: README_ZH.md
================
<h1 align="center">● Open Interpreter（开放解释器）</h1>

<p align="center">
    <a href="https://discord.gg/6p3fD6rBVm"><img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"></a>
    <a href="README_JA.md"><img src="https://img.shields.io/badge/ドキュメント-日本語-white.svg" alt="JA doc"></a>
    <a href="README_ES.md"> <img src="https://img.shields.io/badge/Español-white.svg" alt="ES doc"/></a>
    <a href="README_UK.md"><img src="https://img.shields.io/badge/Українська-white.svg" alt="UK doc"/></a>
    <a href="README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="../README.md"><img src="https://img.shields.io/badge/english-document-white.svg" alt="EN doc"></a>
    <a href="../LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
    <br>
    <br>
    <b>让语言模型在您的计算机上运行代码。</b><br>
    在本地实现的开源OpenAI的代码解释器。<br>
    <br><a href="https://openinterpreter.com">登记以提前获取Open Interpreter（开放解释器）桌面应用程序</a>‎ ‎ |‎ ‎ <b><a href="https://docs.openinterpreter.com/">阅读新文档</a></b><br>
</p>

<br>

![poster](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56)

<br>

```shell
pip install open-interpreter
```

```shell
interpreter
```

<br>

**Open Interpreter（开放解释器）** 可以让大语言模型（LLMs）在本地运行代码（比如 Python、JavaScript、Shell 等）。安装后，在终端上运行 `$ interpreter` 即可通过类似 ChatGPT 的界面与 Open Interpreter 聊天。

本软件为计算机的通用功能提供了一个自然语言界面，比如：

- 创建和编辑照片、视频、PDF 等
- 控制 Chrome 浏览器进行搜索
- 绘制、清理和分析大型数据集
- ...

**⚠️ 注意：在代码运行前都会要求您批准执行代码。**

<br>

## 演示

https://github.com/OpenInterpreter/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60

#### Google Colab 上也提供了交互式演示：

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing)

## 快速开始

```shell
pip install open-interpreter
```

### 终端

安装后，运行 `interpreter`：

```shell
interpreter
```

### Python

```python
from interpreter import interpreter

interpreter.chat("Plot AAPL and META's normalized stock prices") # 执行单一命令
interpreter.chat() # 开始交互式聊天
```

## 与 ChatGPT 的代码解释器比较

OpenAI 发布的 [Code Interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter) 和 GPT-4 提供了一个与 ChatGPT 完成实际任务的绝佳机会。

但是，OpenAI 的服务是托管的，闭源的，并且受到严格限制：

- 无法访问互联网。
- [预装软件包数量有限](https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/)。
- 允许的最大上传为 100 MB，且最大运行时间限制为 120.0 秒
- 当运行环境中途结束时，之前的状态会被清除（包括任何生成的文件或链接）。

---

Open Interpreter（开放解释器）通过在本地环境中运行克服了这些限制。它可以完全访问互联网，不受运行时间或是文件大小的限制，也可以使用任何软件包或库。

它将 GPT-4 代码解释器的强大功能与本地开发环境的灵活性相结合。

## 命令

### 交互式聊天

要在终端中开始交互式聊天，从命令行运行 `interpreter`：

```shell
interpreter
```

或者从.py 文件中运行 `interpreter.chat()`：

```python
interpreter.chat()
```

### 程序化聊天

为了更精确的控制，您可以通过 `.chat(message)` 直接传递消息 ：

```python
interpreter.chat("Add subtitles to all videos in /videos.")

# ... Streams output to your terminal, completes task ...

interpreter.chat("These look great but can you make the subtitles bigger?")

# ...
```

### 开始新的聊天

在 Python 中，Open Interpreter 会记录历史对话。如果你想从头开始，可以进行重置：

```python
interpreter.messages = []
```

### 保存和恢复聊天

```python
messages = interpreter.chat("My name is Killian.") # 保存消息到 'messages'
interpreter.messages = [] # 重置解释器 ("Killian" 将被遗忘)

interpreter.messages = messages # 从 'messages' 恢复聊天 ("Killian" 将被记住)
```

### 自定义系统消息

你可以检查和配置 Open Interpreter 的系统信息，以扩展其功能、修改权限或赋予其更多上下文。

```python
interpreter.system_message += """
使用 -y 运行 shell 命令，这样用户就不必确认它们。
"""
print(interpreter.system_message)
```

### 更改模型

Open Interpreter 使用[LiteLLM](https://docs.litellm.ai/docs/providers/)连接到语言模型。

您可以通过设置模型参数来更改模型：

```shell
interpreter --model gpt-3.5-turbo
interpreter --model claude-2
interpreter --model command-nightly
```

在 Python 环境下，您需要手动设置模型：

```python
interpreter.llm.model = "gpt-3.5-turbo"
```

### 在本地运行 Open Interpreter（开放解释器）

```shell
interpreter --local
```

### 调试模式

为了帮助贡献者检查和调试 Open Interpreter，`--verbose` 模式提供了详细的日志。

您可以使用 `interpreter --verbose` 来激活调试模式，或者直接在终端输入：

```shell
$ interpreter
...
> %verbose true <- 开启调试模式

> %verbose false <- 关闭调试模式
```

## 安全提示

由于生成的代码是在本地环境中运行的，因此会与文件和系统设置发生交互，从而可能导致本地数据丢失或安全风险等意想不到的结果。

**⚠️ 所以在执行任何代码之前，Open Interpreter 都会询问用户是否运行。**

您可以运行 `interpreter -y` 或设置 `interpreter.auto_run = True` 来绕过此确认，此时：

- 在运行请求修改本地文件或系统设置的命令时要谨慎。
- 请像驾驶自动驾驶汽车一直握着方向盘一样留意 Open Interpreter，并随时做好通过关闭终端来结束进程的准备。
- 考虑在 Google Colab 或 Replit 等受限环境中运行 Open Interpreter 的主要原因是这些环境更加独立，从而降低执行任意代码导致出现问题的风险。

## 它是如何工作的？

Open Interpreter 为[函数调用语言模型](https://platform.openai.com/docs/guides/gpt/function-calling)配备了 `exec()` 函数，该函数接受 `编程语言`（如 "Python "或 "JavaScript"）和要运行的 `代码`。

然后，它会将模型的信息、代码和系统的输出以 Markdown 的形式流式传输到终端。

# 作出贡献

感谢您对本项目参与的贡献！我们欢迎所有人贡献到本项目里面。

请参阅我们的 [贡献准则](CONTRIBUTING.md)，了解如何参与贡献的更多详情。

## 规划图

若要预览 Open Interpreter 的未来，请查看[我们的路线图](https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/ROADMAP.md) 。

**请注意**：此软件与 OpenAI 无关。

![thumbnail-ncu](https://github.com/OpenInterpreter/open-interpreter/assets/63927363/1b19a5db-b486-41fd-a7a1-fe2028031686)

================
File: ROADMAP.md
================
# Roadmap

## Documentation
- [ ] Work with Mintlify to translate docs. How does Mintlify let us translate our documentation automatically? I know there's a way.
- [ ] Better comments throughout the package (they're like docs for contributors)
- [ ] Show how to replace interpreter.llm so you can use a custom llm

## New features
- [ ] Figure out how to get OI to answer to user input requests like python's `input()`. Do we somehow detect a delay in the output..? Is there some universal flag that TUIs emit when they expect user input? Should we do this semantically with embeddings, then ask OI to review it and respond..?
- [ ] Placeholder text that gives a compelling example OI request. Probably use `textual`
- [ ] Everything else `textual` offers, like could we make it easier to select text? Copy paste in and out? Code editing interface?
- [x] Let people turn off the active line highlighting
- [ ] Add a --plain flag which doesn't use rich, just prints stuff in plain text
- [ ] Use iPython stuff to track the active line, instead of inserting print statements, which makes debugging weird (From ChatGPT: For deeper insights into what's happening behind the scenes, including which line of code is being executed, you can increase the logging level of the IPython kernel. You can configure the kernel's logger to a more verbose setting, which logs each execution request. However, this requires modifying the kernel's startup settings, which might involve changing logging configurations in the IPython kernel source or when launching the kernel.)
- [ ] Let people edit the code OI writes. Could just open it in the user's preferred editor. Simple. [Full description of how to implement this here.](https://github.com/OpenInterpreter/open-interpreter/pull/830#issuecomment-1854989795)
- [ ] Display images in the terminal interface
- [ ] There should be a function that just renders messages to the terminal, so we can revive conversation navigator, and let people look at their conversations
- [ ] ^ This function should also render the last like 5 messages once input() is about to be run, so we don't get those weird stuttering `rich` artifacts
- [ ] Let OI use OI, add `interpreter.chat(async=True)` bool. OI can use this to open OI on a new thread
  - [ ] Also add `interpreter.await()` which waits for `interpreter.running` (?) to = False, and `interpreter.result()` which returns the last assistant messages content.
- [ ] Allow for limited functions (`interpreter.functions`) using regex
  - [ ] If `interpreter.functions != []`:
    - [ ] set `interpreter.computer.languages` to only use Python
    - [ ] Use regex to ensure the output of code blocks conforms to just using those functions + other python basics
- [ ] (Maybe) Allow for a custom embedding function (`interpreter.embed` or `computer.ai.embed`) which will let us do semantic search
- [ ] (Maybe) if a git is detected, switch to a mode that's good for developers, like showing nested file structure in dynamic system message, searching for relevant functions (use computer.files.search)
- [x] Allow for integrations somehow (you can replace interpreter.llm.completions with a wrapped completions endpoint for any kind of logging. need to document this tho)
  - [ ] Document this^
- [ ] Expand "safe mode" to have proper, simple Docker support, or maybe Cosmopolitan LibC
- [ ] Make it so core can be run elsewhere from terminal package — perhaps split over HTTP (this would make docker easier too)
- [ ] For OS mode, experiment with screenshot just returning active window, experiment with it just showing the changes, or showing changes in addition to the whole thing, etc. GAIA should be your guide

## Future-proofing

- [ ] Really good tests / optimization framework, to be run less frequently than Github actions tests
  - [x] Figure out how to run us on [GAIA](https://huggingface.co/gaia-benchmark)
    - [x] How do we just get the questions out of this thing?
    - [x] How do we assess whether or not OI has solved the task?
  - [ ] Loop over GAIA, use a different language model every time (use Replicate, then ask LiteLLM how they made their "mega key" to many different LLM providers)
  - [ ] Loop over that ↑ using a different prompt each time. Which prompt is best across all LLMs?
  - [ ] (For the NCU) might be good to use a Google VM with a display
  - [ ] (Future future) Use GPT-4 to assess each result, explaining each failure. Summarize. Send it all to GPT-4 + our prompt. Let it redesign the prompt, given the failures, rinse and repeat
- [ ] Stateless (as in, doesn't use the application directory) core python package. All `appdir` or `platformdirs` stuff should be only for the TUI
  - [ ] `interpreter.__dict__` = a dict derived from config is how the python package should be set, and this should be from the TUI. `interpreter` should not know about the config
  - [ ] Move conversation storage out of the core and into the TUI. When we exit or error, save messages same as core currently does
- [ ] Further split TUI from core (some utils still reach across)
- [ ] Better storage of different model keys in TUI / config file. All keys, to multiple providers, should be stored in there. Easy switching
  - [ ] Automatically migrate users from old config to new config, display a message of this
- [ ] On update, check for new system message and ask user to overwrite theirs, or only let users pass in "custom instructions" which adds to our system message
  - [ ] I think we could have a config that's like... system_message_version. If system_message_version is below the current version, ask the user if we can overwrite it with the default config system message of that version. (This somewhat exists now but needs to be robust)

# What's in our scope?

Open Interpreter contains two projects which support each other, whose scopes are as follows:

1. `core`, which is dedicated to figuring out how to get LLMs to safely control a computer. Right now, this means creating a real-time code execution environment that language models can operate.
2. `terminal_interface`, a text-only way for users to direct the code-running LLM running inside `core`. This includes functions for connecting the `core` to various local and hosted LLMs (which the `core` itself should not know about).

# What's not in our scope?

Our guiding philosophy is minimalism, so we have also decided to explicitly consider the following as **out of scope**:

1. Additional functions in `core` beyond running code.
2. More complex interactions with the LLM in `terminal_interface` beyond text (but file paths to more complex inputs, like images or video, can be included in that text).

---

This roadmap gets pretty rough from here. More like working notes.

# Working Notes

## \* Roughly, how to build `computer.browser`:

First I think we should have a part, like `computer.browser.ask(query)` which just hits up [perplexity](https://www.perplexity.ai/) for fast answers to questions.

Then we want these sorts of things:

- `browser.open(url)`
- `browser.screenshot()`
- `browser.click()`

It should actually be based closely on Selenium. Copy their API so the LLM knows it.

Other than that, basically should be = to the computer module itself, at least the IO / keyboard and mouse parts.

However, for non vision models, `browser.screenshot()` can return the accessibility tree, not an image. And for `browser.click(some text)` we can use the HTML to find that text.

**Here's how GPT suggests we implement the first steps of this:**

Creating a Python script that automates the opening of Chrome with the necessary flags and then interacts with it to navigate to a URL and retrieve the accessibility tree involves a few steps. Here's a comprehensive approach:

1. **Script to Launch Chrome with Remote Debugging**:

   - This script will start Chrome with the `--remote-debugging-port=9222` flag.
   - It will handle different platforms (Windows, macOS, Linux).

2. **Python Script for Automation**:
   - This script uses `pychrome` to connect to the Chrome instance, navigate to a URL, and retrieve the accessibility tree.

### Step 1: Launching Chrome with Remote Debugging

You'll need a script to launch Chrome. This script varies based on the operating system. Below is an example for Windows. You can adapt it for macOS or Linux by changing the path and command to start Chrome.

```python
import subprocess
import sys
import os

def launch_chrome():
    chrome_path = "C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe"  # Update this path for your system
    url = "http://localhost:9222/json/version"
    subprocess.Popen([chrome_path, '--remote-debugging-port=9222'], shell=True)
    print("Chrome launched with remote debugging on port 9222.")

if __name__ == "__main__":
    launch_chrome()
```

### Step 2: Python Script to Navigate and Retrieve Accessibility Tree

Next, you'll use `pychrome` to connect to this Chrome instance. Ensure you've installed `pychrome`:

```bash
pip install pychrome
```

Here's the Python script:

```python
import pychrome
import time

def get_accessibility_tree(tab):
    # Enable the Accessibility domain
    tab.call_method("Accessibility.enable")

    # Get the accessibility tree
    tree = tab.call_method("Accessibility.getFullAXTree")
    return tree

def main():
    # Create a browser instance
    browser = pychrome.Browser(url="http://127.0.0.1:9222")

    # Create a new tab
    tab = browser.new_tab()

    # Start the tab
    tab.start()

    # Navigate to a URL
    tab.set_url("https://www.example.com")
    time.sleep(3)  # Wait for page to load

    # Retrieve the accessibility tree
    accessibility_tree = get_accessibility_tree(tab)
    print(accessibility_tree)

    # Stop the tab (closes it)
    tab.stop()

    # Close the browser
    browser.close()

if __name__ == "__main__":
    main()
```

This script will launch Chrome, connect to it, navigate to "https://www.example.com", and then print the accessibility tree to the console.

**Note**: The script to launch Chrome assumes a typical installation path on Windows. You will need to modify this path according to your Chrome installation location and operating system. Additionally, handling different operating systems requires conditional checks and respective commands for each OS.

================
File: SAFE_MODE.md
================
# Safe Mode

**⚠️ Safe mode is experimental and does not provide any guarantees of safety or security.**

Open Interpreter is working on providing an experimental safety toolkit to help you feel more confident running the code generated by Open Interpreter.

Install Open Interpreter with the safety toolkit dependencies as part of the bundle:

```shell
pip install open-interpreter[safe]
```

Alternatively, you can install the safety toolkit dependencies separately in your virtual environment:

```shell
pip install semgrep
```

## Features

- **No Auto Run**: Safe mode disables the ability to automatically execute code
- **Code Scanning**: Scan generated code for vulnerabilities with [`semgrep`](https://semgrep.dev/)

## Enabling Safe Mode

You can enable safe mode by passing the `--safe` flag when invoking `interpreter` or by configuring `safe_mode` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration).

The safe mode setting has three options:

- `off`: disables the safety toolkit (_default_)
- `ask`: prompts you to confirm that you want to scan code
- `auto`: automatically scans code

### Example Config:

```yaml
model: gpt-4
temperature: 0
verbose: false
safe_mode: ask
```

## Roadmap

Some upcoming features that enable even more safety:

- [Execute code in containers](https://github.com/OpenInterpreter/open-interpreter/pull/459)

## Tips & Tricks

You can adjust the `system_message` in your [config file](https://github.com/OpenInterpreter/open-interpreter#configuration) to include instructions for the model to scan packages with [`guarddog`]() before installing them.

```yaml
model: gpt-4
verbose: false
safe_mode: ask
system_message: |
  # normal system message here
  BEFORE INSTALLING ANY PACKAGES WITH pip OR npm YOU MUST SCAN THEM WITH `guarddog` FIRST. Run `guarddog pypi scan $package` for pip packages and `guarddog npm scan $package` for npm packages. `guarddog` only accepts one package name at a time.
```

================
File: SECURITY.md
================
# Open Interpreter Security Policy

We take security seriously. Responsible reporting and disclosure of security
vulnerabilities is important for the protection and privacy of our users. If you
discover any security vulnerabilities, please follow these guidelines.

Published security advisories are available on our [GitHub Security Advisories]
page.

To report a vulnerability, please draft a [new security advisory on GitHub]. Any
fields that you are unsure of or don't understand can be left at their default
values. The important part is that the vulnerability is reported. Once the
security advisory draft has been created, we will validate the vulnerability and
coordinate with you to fix it, release a patch, and responsibly disclose the
vulnerability to the public. Read GitHub's documentation on [privately reporting
a security vulnerability] for details.

Please do not report undisclosed vulnerabilities on public sites or forums,
including GitHub issues and pull requests. Reporting vulnerabilities to the
public could allow attackers to exploit vulnerable applications before we have
been able to release a patch and before applications have had time to install
the patch. Once we have released a patch and sufficient time has passed for
applications to install the patch, we will disclose the vulnerability to the
public, at which time you will be free to publish details of the vulnerability
on public sites and forums.

If you have a fix for a security vulnerability, please do not submit a GitHub
pull request. Instead, report the vulnerability as described in this policy.
Once we have verified the vulnerability, we can create a [temporary private
fork] to collaborate on a patch.

We appreciate your cooperation in helping keep our users safe by following this
policy.

[github security advisories]: https://github.com/OpenInterpreter/open-interpreter/security/advisories
[new security advisory on github]: https://github.com/OpenInterpreter/open-interpreter/security/advisories/new
[privately reporting a security vulnerability]: https://docs.github.com/en/code-security/security-advisories/guidance-on-reporting-and-writing/privately-reporting-a-security-vulnerability
[temporary private fork]: https://docs.github.com/en/code-security/security-advisories/repository-security-advisories/collaborating-in-a-temporary-private-fork-to-resolve-a-repository-security-vulnerability

================
File: style.css
================
.rounded-lg {
    border-radius: 0;
}

/*

.rounded-sm, .rounded-md, .rounded-lg, .rounded-xl, .rounded-2xl, .rounded-3xl {
    border-radius: 0.125rem;
}

.rounded-full {
    border-radius: 0.125rem;
}

*/

.font-extrabold {
    font-weight: 600;
}

.h1, .h2, .h3, .h4, .h5, .h6 {
    font-weight: 600;
}

.body {
    font-weight: normal;
}
